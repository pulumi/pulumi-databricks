// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.TypeShape;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.GetAccountFederationPolicyArgs;
import com.pulumi.databricks.inputs.GetAccountFederationPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAccountNetworkPolicyArgs;
import com.pulumi.databricks.inputs.GetAccountNetworkPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAccountSettingV2Args;
import com.pulumi.databricks.inputs.GetAccountSettingV2PlainArgs;
import com.pulumi.databricks.inputs.GetAlertV2Args;
import com.pulumi.databricks.inputs.GetAlertV2PlainArgs;
import com.pulumi.databricks.inputs.GetAlertsV2Args;
import com.pulumi.databricks.inputs.GetAlertsV2PlainArgs;
import com.pulumi.databricks.inputs.GetAppArgs;
import com.pulumi.databricks.inputs.GetAppPlainArgs;
import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatePlainArgs;
import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesPlainArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
import com.pulumi.databricks.inputs.GetBudgetPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogArgs;
import com.pulumi.databricks.inputs.GetCatalogPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogsArgs;
import com.pulumi.databricks.inputs.GetCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetRevisionsCleanRoomAssetArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetRevisionsCleanRoomAssetPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetRevisionsCleanRoomAssetsArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetRevisionsCleanRoomAssetsPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetsArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAssetsPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAutoApprovalRuleArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAutoApprovalRulePlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAutoApprovalRulesArgs;
import com.pulumi.databricks.inputs.GetCleanRoomAutoApprovalRulesPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomPlainArgs;
import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomsArgs;
import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomsPlainArgs;
import com.pulumi.databricks.inputs.GetClusterArgs;
import com.pulumi.databricks.inputs.GetClusterPlainArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetClustersArgs;
import com.pulumi.databricks.inputs.GetClustersPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetDashboardsArgs;
import com.pulumi.databricks.inputs.GetDashboardsPlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogPlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogsArgs;
import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstancePlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstancesPlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTablePlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTablesArgs;
import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTablesPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFileArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePlainArgs;
import com.pulumi.databricks.inputs.GetDirectoryArgs;
import com.pulumi.databricks.inputs.GetDirectoryPlainArgs;
import com.pulumi.databricks.inputs.GetEntityTagAssignmentArgs;
import com.pulumi.databricks.inputs.GetEntityTagAssignmentPlainArgs;
import com.pulumi.databricks.inputs.GetEntityTagAssignmentsArgs;
import com.pulumi.databricks.inputs.GetEntityTagAssignmentsPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationArgs;
import com.pulumi.databricks.inputs.GetExternalLocationPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsPlainArgs;
import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
import com.pulumi.databricks.inputs.GetExternalMetadataPlainArgs;
import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
import com.pulumi.databricks.inputs.GetExternalMetadatasPlainArgs;
import com.pulumi.databricks.inputs.GetFunctionsArgs;
import com.pulumi.databricks.inputs.GetFunctionsPlainArgs;
import com.pulumi.databricks.inputs.GetGroupArgs;
import com.pulumi.databricks.inputs.GetGroupPlainArgs;
import com.pulumi.databricks.inputs.GetInstancePoolArgs;
import com.pulumi.databricks.inputs.GetInstancePoolPlainArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesPlainArgs;
import com.pulumi.databricks.inputs.GetJobArgs;
import com.pulumi.databricks.inputs.GetJobPlainArgs;
import com.pulumi.databricks.inputs.GetJobsArgs;
import com.pulumi.databricks.inputs.GetJobsPlainArgs;
import com.pulumi.databricks.inputs.GetMaterializedFeaturesFeatureTagArgs;
import com.pulumi.databricks.inputs.GetMaterializedFeaturesFeatureTagPlainArgs;
import com.pulumi.databricks.inputs.GetMaterializedFeaturesFeatureTagsArgs;
import com.pulumi.databricks.inputs.GetMaterializedFeaturesFeatureTagsPlainArgs;
import com.pulumi.databricks.inputs.GetMetastoreArgs;
import com.pulumi.databricks.inputs.GetMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetMetastoresArgs;
import com.pulumi.databricks.inputs.GetMetastoresPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowModelArgs;
import com.pulumi.databricks.inputs.GetMlflowModelPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
import com.pulumi.databricks.inputs.GetMlflowModelsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigPlainArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsPlainArgs;
import com.pulumi.databricks.inputs.GetNodeTypeArgs;
import com.pulumi.databricks.inputs.GetNodeTypePlainArgs;
import com.pulumi.databricks.inputs.GetNotebookArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsPlainArgs;
import com.pulumi.databricks.inputs.GetNotebookPlainArgs;
import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
import com.pulumi.databricks.inputs.GetNotificationDestinationsPlainArgs;
import com.pulumi.databricks.inputs.GetOnlineStoreArgs;
import com.pulumi.databricks.inputs.GetOnlineStorePlainArgs;
import com.pulumi.databricks.inputs.GetOnlineStoresArgs;
import com.pulumi.databricks.inputs.GetOnlineStoresPlainArgs;
import com.pulumi.databricks.inputs.GetPipelinesArgs;
import com.pulumi.databricks.inputs.GetPipelinesPlainArgs;
import com.pulumi.databricks.inputs.GetPolicyInfoArgs;
import com.pulumi.databricks.inputs.GetPolicyInfoPlainArgs;
import com.pulumi.databricks.inputs.GetPolicyInfosArgs;
import com.pulumi.databricks.inputs.GetPolicyInfosPlainArgs;
import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
import com.pulumi.databricks.inputs.GetQualityMonitorV2PlainArgs;
import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
import com.pulumi.databricks.inputs.GetQualityMonitorsV2PlainArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPoliciesArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPoliciesPlainArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPolicyArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelPlainArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelVersionsPlainArgs;
import com.pulumi.databricks.inputs.GetSchemaArgs;
import com.pulumi.databricks.inputs.GetSchemaPlainArgs;
import com.pulumi.databricks.inputs.GetSchemasArgs;
import com.pulumi.databricks.inputs.GetSchemasPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalFederationPolicyArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalFederationPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsPlainArgs;
import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
import com.pulumi.databricks.inputs.GetServingEndpointsPlainArgs;
import com.pulumi.databricks.inputs.GetShareArgs;
import com.pulumi.databricks.inputs.GetSharePlainArgs;
import com.pulumi.databricks.inputs.GetSharesArgs;
import com.pulumi.databricks.inputs.GetSharesPlainArgs;
import com.pulumi.databricks.inputs.GetSparkVersionArgs;
import com.pulumi.databricks.inputs.GetSparkVersionPlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousePlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetTableArgs;
import com.pulumi.databricks.inputs.GetTablePlainArgs;
import com.pulumi.databricks.inputs.GetTablesArgs;
import com.pulumi.databricks.inputs.GetTablesPlainArgs;
import com.pulumi.databricks.inputs.GetTagPoliciesArgs;
import com.pulumi.databricks.inputs.GetTagPoliciesPlainArgs;
import com.pulumi.databricks.inputs.GetTagPolicyArgs;
import com.pulumi.databricks.inputs.GetTagPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetUserArgs;
import com.pulumi.databricks.inputs.GetUserPlainArgs;
import com.pulumi.databricks.inputs.GetViewsArgs;
import com.pulumi.databricks.inputs.GetViewsPlainArgs;
import com.pulumi.databricks.inputs.GetVolumeArgs;
import com.pulumi.databricks.inputs.GetVolumePlainArgs;
import com.pulumi.databricks.inputs.GetVolumesArgs;
import com.pulumi.databricks.inputs.GetVolumesPlainArgs;
import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionPlainArgs;
import com.pulumi.databricks.inputs.GetWorkspaceSettingV2Args;
import com.pulumi.databricks.inputs.GetWorkspaceSettingV2PlainArgs;
import com.pulumi.databricks.inputs.GetZonesArgs;
import com.pulumi.databricks.inputs.GetZonesPlainArgs;
import com.pulumi.databricks.outputs.GetAccountFederationPoliciesResult;
import com.pulumi.databricks.outputs.GetAccountFederationPolicyResult;
import com.pulumi.databricks.outputs.GetAccountNetworkPoliciesResult;
import com.pulumi.databricks.outputs.GetAccountNetworkPolicyResult;
import com.pulumi.databricks.outputs.GetAccountSettingV2Result;
import com.pulumi.databricks.outputs.GetAlertV2Result;
import com.pulumi.databricks.outputs.GetAlertsV2InvokeResult;
import com.pulumi.databricks.outputs.GetAppResult;
import com.pulumi.databricks.outputs.GetAppsResult;
import com.pulumi.databricks.outputs.GetAppsSettingsCustomTemplateResult;
import com.pulumi.databricks.outputs.GetAppsSettingsCustomTemplatesResult;
import com.pulumi.databricks.outputs.GetAwsAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsBucketPolicyResult;
import com.pulumi.databricks.outputs.GetAwsCrossAccountPolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogPolicyResult;
import com.pulumi.databricks.outputs.GetBudgetPoliciesResult;
import com.pulumi.databricks.outputs.GetBudgetPolicyResult;
import com.pulumi.databricks.outputs.GetCatalogResult;
import com.pulumi.databricks.outputs.GetCatalogsResult;
import com.pulumi.databricks.outputs.GetCleanRoomAssetResult;
import com.pulumi.databricks.outputs.GetCleanRoomAssetRevisionsCleanRoomAssetResult;
import com.pulumi.databricks.outputs.GetCleanRoomAssetRevisionsCleanRoomAssetsResult;
import com.pulumi.databricks.outputs.GetCleanRoomAssetsResult;
import com.pulumi.databricks.outputs.GetCleanRoomAutoApprovalRuleResult;
import com.pulumi.databricks.outputs.GetCleanRoomAutoApprovalRulesResult;
import com.pulumi.databricks.outputs.GetCleanRoomsCleanRoomResult;
import com.pulumi.databricks.outputs.GetCleanRoomsCleanRoomsResult;
import com.pulumi.databricks.outputs.GetClusterPolicyResult;
import com.pulumi.databricks.outputs.GetClusterResult;
import com.pulumi.databricks.outputs.GetClustersResult;
import com.pulumi.databricks.outputs.GetCurrentConfigResult;
import com.pulumi.databricks.outputs.GetCurrentMetastoreResult;
import com.pulumi.databricks.outputs.GetCurrentUserResult;
import com.pulumi.databricks.outputs.GetDashboardsResult;
import com.pulumi.databricks.outputs.GetDatabaseDatabaseCatalogResult;
import com.pulumi.databricks.outputs.GetDatabaseDatabaseCatalogsResult;
import com.pulumi.databricks.outputs.GetDatabaseInstanceResult;
import com.pulumi.databricks.outputs.GetDatabaseInstancesResult;
import com.pulumi.databricks.outputs.GetDatabaseSyncedDatabaseTableResult;
import com.pulumi.databricks.outputs.GetDatabaseSyncedDatabaseTablesResult;
import com.pulumi.databricks.outputs.GetDbfsFilePathsResult;
import com.pulumi.databricks.outputs.GetDbfsFileResult;
import com.pulumi.databricks.outputs.GetDirectoryResult;
import com.pulumi.databricks.outputs.GetEntityTagAssignmentResult;
import com.pulumi.databricks.outputs.GetEntityTagAssignmentsResult;
import com.pulumi.databricks.outputs.GetExternalLocationResult;
import com.pulumi.databricks.outputs.GetExternalLocationsResult;
import com.pulumi.databricks.outputs.GetExternalMetadataResult;
import com.pulumi.databricks.outputs.GetExternalMetadatasResult;
import com.pulumi.databricks.outputs.GetFunctionsResult;
import com.pulumi.databricks.outputs.GetGroupResult;
import com.pulumi.databricks.outputs.GetInstancePoolResult;
import com.pulumi.databricks.outputs.GetInstanceProfilesResult;
import com.pulumi.databricks.outputs.GetJobResult;
import com.pulumi.databricks.outputs.GetJobsResult;
import com.pulumi.databricks.outputs.GetMaterializedFeaturesFeatureTagResult;
import com.pulumi.databricks.outputs.GetMaterializedFeaturesFeatureTagsResult;
import com.pulumi.databricks.outputs.GetMetastoreResult;
import com.pulumi.databricks.outputs.GetMetastoresResult;
import com.pulumi.databricks.outputs.GetMlflowExperimentResult;
import com.pulumi.databricks.outputs.GetMlflowModelResult;
import com.pulumi.databricks.outputs.GetMlflowModelsResult;
import com.pulumi.databricks.outputs.GetMwsCredentialsResult;
import com.pulumi.databricks.outputs.GetMwsNetworkConnectivityConfigResult;
import com.pulumi.databricks.outputs.GetMwsNetworkConnectivityConfigsResult;
import com.pulumi.databricks.outputs.GetMwsWorkspacesResult;
import com.pulumi.databricks.outputs.GetNodeTypeResult;
import com.pulumi.databricks.outputs.GetNotebookPathsResult;
import com.pulumi.databricks.outputs.GetNotebookResult;
import com.pulumi.databricks.outputs.GetNotificationDestinationsResult;
import com.pulumi.databricks.outputs.GetOnlineStoreResult;
import com.pulumi.databricks.outputs.GetOnlineStoresResult;
import com.pulumi.databricks.outputs.GetPipelinesResult;
import com.pulumi.databricks.outputs.GetPolicyInfoResult;
import com.pulumi.databricks.outputs.GetPolicyInfosResult;
import com.pulumi.databricks.outputs.GetQualityMonitorV2Result;
import com.pulumi.databricks.outputs.GetQualityMonitorsV2Result;
import com.pulumi.databricks.outputs.GetRecipientFederationPoliciesResult;
import com.pulumi.databricks.outputs.GetRecipientFederationPolicyResult;
import com.pulumi.databricks.outputs.GetRegisteredModelResult;
import com.pulumi.databricks.outputs.GetRegisteredModelVersionsResult;
import com.pulumi.databricks.outputs.GetSchemaResult;
import com.pulumi.databricks.outputs.GetSchemasResult;
import com.pulumi.databricks.outputs.GetServicePrincipalFederationPoliciesResult;
import com.pulumi.databricks.outputs.GetServicePrincipalFederationPolicyResult;
import com.pulumi.databricks.outputs.GetServicePrincipalResult;
import com.pulumi.databricks.outputs.GetServicePrincipalsResult;
import com.pulumi.databricks.outputs.GetServingEndpointsResult;
import com.pulumi.databricks.outputs.GetShareResult;
import com.pulumi.databricks.outputs.GetSharesResult;
import com.pulumi.databricks.outputs.GetSparkVersionResult;
import com.pulumi.databricks.outputs.GetSqlWarehouseResult;
import com.pulumi.databricks.outputs.GetSqlWarehousesResult;
import com.pulumi.databricks.outputs.GetStorageCredentialResult;
import com.pulumi.databricks.outputs.GetStorageCredentialsResult;
import com.pulumi.databricks.outputs.GetTableResult;
import com.pulumi.databricks.outputs.GetTablesResult;
import com.pulumi.databricks.outputs.GetTagPoliciesResult;
import com.pulumi.databricks.outputs.GetTagPolicyResult;
import com.pulumi.databricks.outputs.GetUserResult;
import com.pulumi.databricks.outputs.GetViewsResult;
import com.pulumi.databricks.outputs.GetVolumeResult;
import com.pulumi.databricks.outputs.GetVolumesResult;
import com.pulumi.databricks.outputs.GetWorkspaceNetworkOptionResult;
import com.pulumi.databricks.outputs.GetWorkspaceSettingV2Result;
import com.pulumi.databricks.outputs.GetZonesResult;
import com.pulumi.deployment.Deployment;
import com.pulumi.deployment.InvokeOptions;
import com.pulumi.deployment.InvokeOutputOptions;
import com.pulumi.resources.InvokeArgs;
import java.util.concurrent.CompletableFuture;

public final class DatabricksFunctions {
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountFederationPoliciesResult> getAccountFederationPolicies() {
        return getAccountFederationPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountFederationPoliciesResult> getAccountFederationPoliciesPlain() {
        return getAccountFederationPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountFederationPoliciesResult> getAccountFederationPolicies(InvokeArgs args) {
        return getAccountFederationPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountFederationPoliciesResult> getAccountFederationPoliciesPlain(InvokeArgs args) {
        return getAccountFederationPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountFederationPoliciesResult> getAccountFederationPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountFederationPolicies:getAccountFederationPolicies", TypeShape.of(GetAccountFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountFederationPoliciesResult> getAccountFederationPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountFederationPolicies:getAccountFederationPolicies", TypeShape.of(GetAccountFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of account federation policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all account federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountFederationPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountFederationPoliciesResult> getAccountFederationPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountFederationPolicies:getAccountFederationPolicies", TypeShape.of(GetAccountFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static Output<GetAccountFederationPolicyResult> getAccountFederationPolicy() {
        return getAccountFederationPolicy(GetAccountFederationPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static CompletableFuture<GetAccountFederationPolicyResult> getAccountFederationPolicyPlain() {
        return getAccountFederationPolicyPlain(GetAccountFederationPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static Output<GetAccountFederationPolicyResult> getAccountFederationPolicy(GetAccountFederationPolicyArgs args) {
        return getAccountFederationPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static CompletableFuture<GetAccountFederationPolicyResult> getAccountFederationPolicyPlain(GetAccountFederationPolicyPlainArgs args) {
        return getAccountFederationPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static Output<GetAccountFederationPolicyResult> getAccountFederationPolicy(GetAccountFederationPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountFederationPolicy:getAccountFederationPolicy", TypeShape.of(GetAccountFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static Output<GetAccountFederationPolicyResult> getAccountFederationPolicy(GetAccountFederationPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountFederationPolicy:getAccountFederationPolicy", TypeShape.of(GetAccountFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single account federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an account federation policy by id:
     * 
     */
    public static CompletableFuture<GetAccountFederationPolicyResult> getAccountFederationPolicyPlain(GetAccountFederationPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountFederationPolicy:getAccountFederationPolicy", TypeShape.of(GetAccountFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies() {
        return getAccountNetworkPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain() {
        return getAccountNetworkPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args) {
        return getAccountNetworkPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain(InvokeArgs args) {
        return getAccountNetworkPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of network policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all network policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAccountNetworkPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy() {
        return getAccountNetworkPolicy(GetAccountNetworkPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain() {
        return getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args) {
        return getAccountNetworkPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs args) {
        return getAccountNetworkPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single network policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     */
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetAccountSettingV2Result> getAccountSettingV2() {
        return getAccountSettingV2(GetAccountSettingV2Args.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountSettingV2Result> getAccountSettingV2Plain() {
        return getAccountSettingV2Plain(GetAccountSettingV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetAccountSettingV2Result> getAccountSettingV2(GetAccountSettingV2Args args) {
        return getAccountSettingV2(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountSettingV2Result> getAccountSettingV2Plain(GetAccountSettingV2PlainArgs args) {
        return getAccountSettingV2Plain(args, InvokeOptions.Empty);
    }
    public static Output<GetAccountSettingV2Result> getAccountSettingV2(GetAccountSettingV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountSettingV2:getAccountSettingV2", TypeShape.of(GetAccountSettingV2Result.class), args, Utilities.withVersion(options));
    }
    public static Output<GetAccountSettingV2Result> getAccountSettingV2(GetAccountSettingV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountSettingV2:getAccountSettingV2", TypeShape.of(GetAccountSettingV2Result.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetAccountSettingV2Result> getAccountSettingV2Plain(GetAccountSettingV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountSettingV2:getAccountSettingV2", TypeShape.of(GetAccountSettingV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2() {
        return getAlertV2(GetAlertV2Args.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain() {
        return getAlertV2Plain(GetAlertV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args) {
        return getAlertV2(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain(GetAlertV2PlainArgs args) {
        return getAlertV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain(GetAlertV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2() {
        return getAlertsV2(GetAlertsV2Args.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain() {
        return getAlertsV2Plain(GetAlertsV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(GetAlertsV2Args args) {
        return getAlertsV2(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain(GetAlertsV2PlainArgs args) {
        return getAlertsV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(GetAlertsV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(GetAlertsV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain(GetAlertsV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args) {
        return getApp(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppResult> getAppPlain(GetAppPlainArgs args) {
        return getAppPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppResult> getAppPlain(GetAppPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps() {
        return getApps(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain() {
        return getAppsPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args) {
        return getApps(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain(InvokeArgs args) {
        return getAppsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customers Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Custom Template.
     * 
     * ## Example Usage
     * 
     * Referring to a Custom Template by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var myTemplate = DatabricksFunctions.getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs.builder()
     *             .name("my-custom-template")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplateResult> getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs args) {
        return getAppsSettingsCustomTemplate(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Custom Template.
     * 
     * ## Example Usage
     * 
     * Referring to a Custom Template by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var myTemplate = DatabricksFunctions.getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs.builder()
     *             .name("my-custom-template")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAppsSettingsCustomTemplateResult> getAppsSettingsCustomTemplatePlain(GetAppsSettingsCustomTemplatePlainArgs args) {
        return getAppsSettingsCustomTemplatePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Custom Template.
     * 
     * ## Example Usage
     * 
     * Referring to a Custom Template by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var myTemplate = DatabricksFunctions.getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs.builder()
     *             .name("my-custom-template")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplateResult> getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAppsSettingsCustomTemplate:getAppsSettingsCustomTemplate", TypeShape.of(GetAppsSettingsCustomTemplateResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Custom Template.
     * 
     * ## Example Usage
     * 
     * Referring to a Custom Template by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var myTemplate = DatabricksFunctions.getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs.builder()
     *             .name("my-custom-template")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplateResult> getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAppsSettingsCustomTemplate:getAppsSettingsCustomTemplate", TypeShape.of(GetAppsSettingsCustomTemplateResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Custom Template.
     * 
     * ## Example Usage
     * 
     * Referring to a Custom Template by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplateArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var myTemplate = DatabricksFunctions.getAppsSettingsCustomTemplate(GetAppsSettingsCustomTemplateArgs.builder()
     *             .name("my-custom-template")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAppsSettingsCustomTemplateResult> getAppsSettingsCustomTemplatePlain(GetAppsSettingsCustomTemplatePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAppsSettingsCustomTemplate:getAppsSettingsCustomTemplate", TypeShape.of(GetAppsSettingsCustomTemplateResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplates() {
        return getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplatesPlain() {
        return getAppsSettingsCustomTemplatesPlain(GetAppsSettingsCustomTemplatesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs args) {
        return getAppsSettingsCustomTemplates(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplatesPlain(GetAppsSettingsCustomTemplatesPlainArgs args) {
        return getAppsSettingsCustomTemplatesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAppsSettingsCustomTemplates:getAppsSettingsCustomTemplates", TypeShape.of(GetAppsSettingsCustomTemplatesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAppsSettingsCustomTemplates:getAppsSettingsCustomTemplates", TypeShape.of(GetAppsSettingsCustomTemplatesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Custom Templates within the workspace.
     * The list can then be accessed via the data object&#39;s `templates` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Custom Templates:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppsSettingsCustomTemplatesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAppsSettingsCustomTemplates(GetAppsSettingsCustomTemplatesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCustomTemplates", all.templates());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAppsSettingsCustomTemplatesResult> getAppsSettingsCustomTemplatesPlain(GetAppsSettingsCustomTemplatesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAppsSettingsCustomTemplates:getAppsSettingsCustomTemplates", TypeShape.of(GetAppsSettingsCustomTemplatesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args) {
        return getAwsAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args) {
        return getAwsAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args) {
        return getAwsBucketPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args) {
        return getAwsBucketPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy() {
        return getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain() {
        return getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args) {
        return getAwsCrossAccountPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args) {
        return getAwsCrossAccountPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricksAwsS3Mount pages.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args) {
        return getAwsUnityCatalogAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args) {
        return getAwsUnityCatalogAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args) {
        return getAwsUnityCatalogPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args) {
        return getAwsUnityCatalogPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies() {
        return getBudgetPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain() {
        return getBudgetPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args) {
        return getBudgetPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain(InvokeArgs args) {
        return getBudgetPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy() {
        return getBudgetPolicy(GetBudgetPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain() {
        return getBudgetPolicyPlain(GetBudgetPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args) {
        return getBudgetPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain(GetBudgetPolicyPlainArgs args) {
        return getBudgetPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getBudgetPolicy(GetBudgetPolicyArgs.builder()
     *             .policyId("test")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain(GetBudgetPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args) {
        return getCatalog(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args) {
        return getCatalogPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs() {
        return getCatalogs(GetCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain() {
        return getCatalogsPlain(GetCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args) {
        return getCatalogs(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args) {
        return getCatalogsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room asset.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Asset Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import com.pulumi.databricks.cleanRoomsAssetArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new CleanRoomsAsset("this", CleanRoomsAssetArgs.builder()
     *             .name("example-cleanroom-asset")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetResult> getCleanRoomAsset(GetCleanRoomAssetArgs args) {
        return getCleanRoomAsset(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room asset.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Asset Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import com.pulumi.databricks.cleanRoomsAssetArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new CleanRoomsAsset("this", CleanRoomsAssetArgs.builder()
     *             .name("example-cleanroom-asset")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomAssetResult> getCleanRoomAssetPlain(GetCleanRoomAssetPlainArgs args) {
        return getCleanRoomAssetPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room asset.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Asset Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import com.pulumi.databricks.cleanRoomsAssetArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new CleanRoomsAsset("this", CleanRoomsAssetArgs.builder()
     *             .name("example-cleanroom-asset")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetResult> getCleanRoomAsset(GetCleanRoomAssetArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAsset:getCleanRoomAsset", TypeShape.of(GetCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room asset.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Asset Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import com.pulumi.databricks.cleanRoomsAssetArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new CleanRoomsAsset("this", CleanRoomsAssetArgs.builder()
     *             .name("example-cleanroom-asset")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetResult> getCleanRoomAsset(GetCleanRoomAssetArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAsset:getCleanRoomAsset", TypeShape.of(GetCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room asset.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Asset Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import com.pulumi.databricks.cleanRoomsAssetArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new CleanRoomsAsset("this", CleanRoomsAssetArgs.builder()
     *             .name("example-cleanroom-asset")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomAssetResult> getCleanRoomAssetPlain(GetCleanRoomAssetPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAsset:getCleanRoomAsset", TypeShape.of(GetCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetResult> getCleanRoomAssetRevisionsCleanRoomAsset(GetCleanRoomAssetRevisionsCleanRoomAssetArgs args) {
        return getCleanRoomAssetRevisionsCleanRoomAsset(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAssetRevisionsCleanRoomAssetResult> getCleanRoomAssetRevisionsCleanRoomAssetPlain(GetCleanRoomAssetRevisionsCleanRoomAssetPlainArgs args) {
        return getCleanRoomAssetRevisionsCleanRoomAssetPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetResult> getCleanRoomAssetRevisionsCleanRoomAsset(GetCleanRoomAssetRevisionsCleanRoomAssetArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssetRevisionsCleanRoomAsset:getCleanRoomAssetRevisionsCleanRoomAsset", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetResult> getCleanRoomAssetRevisionsCleanRoomAsset(GetCleanRoomAssetRevisionsCleanRoomAssetArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssetRevisionsCleanRoomAsset:getCleanRoomAssetRevisionsCleanRoomAsset", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetCleanRoomAssetRevisionsCleanRoomAssetResult> getCleanRoomAssetRevisionsCleanRoomAssetPlain(GetCleanRoomAssetRevisionsCleanRoomAssetPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAssetRevisionsCleanRoomAsset:getCleanRoomAssetRevisionsCleanRoomAsset", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetsResult> getCleanRoomAssetRevisionsCleanRoomAssets(GetCleanRoomAssetRevisionsCleanRoomAssetsArgs args) {
        return getCleanRoomAssetRevisionsCleanRoomAssets(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAssetRevisionsCleanRoomAssetsResult> getCleanRoomAssetRevisionsCleanRoomAssetsPlain(GetCleanRoomAssetRevisionsCleanRoomAssetsPlainArgs args) {
        return getCleanRoomAssetRevisionsCleanRoomAssetsPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetsResult> getCleanRoomAssetRevisionsCleanRoomAssets(GetCleanRoomAssetRevisionsCleanRoomAssetsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssetRevisionsCleanRoomAssets:getCleanRoomAssetRevisionsCleanRoomAssets", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAssetRevisionsCleanRoomAssetsResult> getCleanRoomAssetRevisionsCleanRoomAssets(GetCleanRoomAssetRevisionsCleanRoomAssetsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssetRevisionsCleanRoomAssets:getCleanRoomAssetRevisionsCleanRoomAssets", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetCleanRoomAssetRevisionsCleanRoomAssetsResult> getCleanRoomAssetRevisionsCleanRoomAssetsPlain(GetCleanRoomAssetRevisionsCleanRoomAssetsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAssetRevisionsCleanRoomAssets:getCleanRoomAssetRevisionsCleanRoomAssets", TypeShape.of(GetCleanRoomAssetRevisionsCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean room assets.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Assets Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var all = new CleanRoomsAsset("all");
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetsResult> getCleanRoomAssets(GetCleanRoomAssetsArgs args) {
        return getCleanRoomAssets(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean room assets.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Assets Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var all = new CleanRoomsAsset("all");
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomAssetsResult> getCleanRoomAssetsPlain(GetCleanRoomAssetsPlainArgs args) {
        return getCleanRoomAssetsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean room assets.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Assets Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var all = new CleanRoomsAsset("all");
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetsResult> getCleanRoomAssets(GetCleanRoomAssetsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssets:getCleanRoomAssets", TypeShape.of(GetCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean room assets.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Assets Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var all = new CleanRoomsAsset("all");
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomAssetsResult> getCleanRoomAssets(GetCleanRoomAssetsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAssets:getCleanRoomAssets", TypeShape.of(GetCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean room assets.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Assets Datasource
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.cleanRoomsAsset;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var all = new CleanRoomsAsset("all");
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomAssetsResult> getCleanRoomAssetsPlain(GetCleanRoomAssetsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAssets:getCleanRoomAssets", TypeShape.of(GetCleanRoomAssetsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRule() {
        return getCleanRoomAutoApprovalRule(GetCleanRoomAutoApprovalRuleArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRulePlain() {
        return getCleanRoomAutoApprovalRulePlain(GetCleanRoomAutoApprovalRulePlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRule(GetCleanRoomAutoApprovalRuleArgs args) {
        return getCleanRoomAutoApprovalRule(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRulePlain(GetCleanRoomAutoApprovalRulePlainArgs args) {
        return getCleanRoomAutoApprovalRulePlain(args, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRule(GetCleanRoomAutoApprovalRuleArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAutoApprovalRule:getCleanRoomAutoApprovalRule", TypeShape.of(GetCleanRoomAutoApprovalRuleResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRule(GetCleanRoomAutoApprovalRuleArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAutoApprovalRule:getCleanRoomAutoApprovalRule", TypeShape.of(GetCleanRoomAutoApprovalRuleResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRuleResult> getCleanRoomAutoApprovalRulePlain(GetCleanRoomAutoApprovalRulePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAutoApprovalRule:getCleanRoomAutoApprovalRule", TypeShape.of(GetCleanRoomAutoApprovalRuleResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRules() {
        return getCleanRoomAutoApprovalRules(GetCleanRoomAutoApprovalRulesArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRulesPlain() {
        return getCleanRoomAutoApprovalRulesPlain(GetCleanRoomAutoApprovalRulesPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRules(GetCleanRoomAutoApprovalRulesArgs args) {
        return getCleanRoomAutoApprovalRules(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRulesPlain(GetCleanRoomAutoApprovalRulesPlainArgs args) {
        return getCleanRoomAutoApprovalRulesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRules(GetCleanRoomAutoApprovalRulesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAutoApprovalRules:getCleanRoomAutoApprovalRules", TypeShape.of(GetCleanRoomAutoApprovalRulesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRules(GetCleanRoomAutoApprovalRulesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomAutoApprovalRules:getCleanRoomAutoApprovalRules", TypeShape.of(GetCleanRoomAutoApprovalRulesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetCleanRoomAutoApprovalRulesResult> getCleanRoomAutoApprovalRulesPlain(GetCleanRoomAutoApprovalRulesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomAutoApprovalRules:getCleanRoomAutoApprovalRules", TypeShape.of(GetCleanRoomAutoApprovalRulesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoom() {
        return getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoomPlain() {
        return getCleanRoomsCleanRoomPlain(GetCleanRoomsCleanRoomPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs args) {
        return getCleanRoomsCleanRoom(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoomPlain(GetCleanRoomsCleanRoomPlainArgs args) {
        return getCleanRoomsCleanRoomPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomsCleanRoom:getCleanRoomsCleanRoom", TypeShape.of(GetCleanRoomsCleanRoomResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomsCleanRoom:getCleanRoomsCleanRoom", TypeShape.of(GetCleanRoomsCleanRoomResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single clean room.
     * 
     * ## Example Usage
     * 
     * # Example: Datasource (Singular) Artifact
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var example = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .name("example-clean-room")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomResult> getCleanRoomsCleanRoomPlain(GetCleanRoomsCleanRoomPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomsCleanRoom:getCleanRoomsCleanRoom", TypeShape.of(GetCleanRoomsCleanRoomResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRooms() {
        return getCleanRoomsCleanRooms(GetCleanRoomsCleanRoomsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRoomsPlain() {
        return getCleanRoomsCleanRoomsPlain(GetCleanRoomsCleanRoomsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRooms(GetCleanRoomsCleanRoomsArgs args) {
        return getCleanRoomsCleanRooms(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRoomsPlain(GetCleanRoomsCleanRoomsPlainArgs args) {
        return getCleanRoomsCleanRoomsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRooms(GetCleanRoomsCleanRoomsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomsCleanRooms:getCleanRoomsCleanRooms", TypeShape.of(GetCleanRoomsCleanRoomsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRooms(GetCleanRoomsCleanRoomsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCleanRoomsCleanRooms:getCleanRoomsCleanRooms", TypeShape.of(GetCleanRoomsCleanRoomsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of clean rooms.
     * 
     * ## Example Usage
     * 
     * # Example: Clean Room Datasource (Plural)
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCleanRoomsCleanRoomArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCleanRoomsCleanRoom(GetCleanRoomsCleanRoomArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetCleanRoomsCleanRoomsResult> getCleanRoomsCleanRoomsPlain(GetCleanRoomsCleanRoomsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCleanRoomsCleanRooms:getCleanRoomsCleanRooms", TypeShape.of(GetCleanRoomsCleanRoomsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static Output<GetClusterResult> getCluster() {
        return getCluster(GetClusterArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain() {
        return getClusterPlain(GetClusterPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args) {
        return getCluster(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args) {
        return getClusterPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy() {
        return getClusterPolicy(GetClusterPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain() {
        return getClusterPolicyPlain(GetClusterPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args) {
        return getClusterPolicy(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args) {
        return getClusterPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static Output<GetClustersResult> getClusters() {
        return getClusters(GetClustersArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain() {
        return getClustersPlain(GetClustersPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args) {
        return getClusters(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args) {
        return getClustersPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig() {
        return getCurrentConfig(GetCurrentConfigArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain() {
        return getCurrentConfigPlain(GetCurrentConfigPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args) {
        return getCurrentConfig(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args) {
        return getCurrentConfigPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `isAccount` - Whether the provider is configured at account-level
     * * `accountId` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloudType` - Cloud type specified in the provider
     * * `authType` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore() {
        return getCurrentMetastore(GetCurrentMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain() {
        return getCurrentMetastorePlain(GetCurrentMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args) {
        return getCurrentMetastore(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args) {
        return getCurrentMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser() {
        return getCurrentUser(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain() {
        return getCurrentUserPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args) {
        return getCurrentUser(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args) {
        return getCurrentUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static Output<GetDashboardsResult> getDashboards() {
        return getDashboards(GetDashboardsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain() {
        return getDashboardsPlain(GetDashboardsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args) {
        return getDashboards(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain(GetDashboardsPlainArgs args) {
        return getDashboardsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain(GetDashboardsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Catalog.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Catalog by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs.builder()
     *             .name("my-database-catalog")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseDatabaseCatalogResult> getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs args) {
        return getDatabaseDatabaseCatalog(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Catalog.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Catalog by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs.builder()
     *             .name("my-database-catalog")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseDatabaseCatalogResult> getDatabaseDatabaseCatalogPlain(GetDatabaseDatabaseCatalogPlainArgs args) {
        return getDatabaseDatabaseCatalogPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Catalog.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Catalog by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs.builder()
     *             .name("my-database-catalog")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseDatabaseCatalogResult> getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseDatabaseCatalog:getDatabaseDatabaseCatalog", TypeShape.of(GetDatabaseDatabaseCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Catalog.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Catalog by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs.builder()
     *             .name("my-database-catalog")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseDatabaseCatalogResult> getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseDatabaseCatalog:getDatabaseDatabaseCatalog", TypeShape.of(GetDatabaseDatabaseCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Catalog.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Catalog by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseDatabaseCatalogArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseDatabaseCatalog(GetDatabaseDatabaseCatalogArgs.builder()
     *             .name("my-database-catalog")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseDatabaseCatalogResult> getDatabaseDatabaseCatalogPlain(GetDatabaseDatabaseCatalogPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseDatabaseCatalog:getDatabaseDatabaseCatalog", TypeShape.of(GetDatabaseDatabaseCatalogResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogs() {
        return getDatabaseDatabaseCatalogs(GetDatabaseDatabaseCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogsPlain() {
        return getDatabaseDatabaseCatalogsPlain(GetDatabaseDatabaseCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogs(GetDatabaseDatabaseCatalogsArgs args) {
        return getDatabaseDatabaseCatalogs(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogsPlain(GetDatabaseDatabaseCatalogsPlainArgs args) {
        return getDatabaseDatabaseCatalogsPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogs(GetDatabaseDatabaseCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseDatabaseCatalogs:getDatabaseDatabaseCatalogs", TypeShape.of(GetDatabaseDatabaseCatalogsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogs(GetDatabaseDatabaseCatalogsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseDatabaseCatalogs:getDatabaseDatabaseCatalogs", TypeShape.of(GetDatabaseDatabaseCatalogsResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetDatabaseDatabaseCatalogsResult> getDatabaseDatabaseCatalogsPlain(GetDatabaseDatabaseCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseDatabaseCatalogs:getDatabaseDatabaseCatalogs", TypeShape.of(GetDatabaseDatabaseCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args) {
        return getDatabaseInstance(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseInstanceResult> getDatabaseInstancePlain(GetDatabaseInstancePlainArgs args) {
        return getDatabaseInstancePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseInstanceResult> getDatabaseInstancePlain(GetDatabaseInstancePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances() {
        return getDatabaseInstances(GetDatabaseInstancesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain() {
        return getDatabaseInstancesPlain(GetDatabaseInstancesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(GetDatabaseInstancesArgs args) {
        return getDatabaseInstances(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain(GetDatabaseInstancesPlainArgs args) {
        return getDatabaseInstancesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(GetDatabaseInstancesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(GetDatabaseInstancesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `databaseInstances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstancesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(GetDatabaseInstancesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain(GetDatabaseInstancesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Synced Database Table.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs.builder()
     *             .name("my_database_catalog.public.synced_table")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseSyncedDatabaseTableResult> getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs args) {
        return getDatabaseSyncedDatabaseTable(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Synced Database Table.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs.builder()
     *             .name("my_database_catalog.public.synced_table")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseSyncedDatabaseTableResult> getDatabaseSyncedDatabaseTablePlain(GetDatabaseSyncedDatabaseTablePlainArgs args) {
        return getDatabaseSyncedDatabaseTablePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Synced Database Table.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs.builder()
     *             .name("my_database_catalog.public.synced_table")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseSyncedDatabaseTableResult> getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseSyncedDatabaseTable:getDatabaseSyncedDatabaseTable", TypeShape.of(GetDatabaseSyncedDatabaseTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Synced Database Table.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs.builder()
     *             .name("my_database_catalog.public.synced_table")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDatabaseSyncedDatabaseTableResult> getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseSyncedDatabaseTable:getDatabaseSyncedDatabaseTable", TypeShape.of(GetDatabaseSyncedDatabaseTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Synced Database Table.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseSyncedDatabaseTableArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseSyncedDatabaseTable(GetDatabaseSyncedDatabaseTableArgs.builder()
     *             .name("my_database_catalog.public.synced_table")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDatabaseSyncedDatabaseTableResult> getDatabaseSyncedDatabaseTablePlain(GetDatabaseSyncedDatabaseTablePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseSyncedDatabaseTable:getDatabaseSyncedDatabaseTable", TypeShape.of(GetDatabaseSyncedDatabaseTableResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTables() {
        return getDatabaseSyncedDatabaseTables(GetDatabaseSyncedDatabaseTablesArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTablesPlain() {
        return getDatabaseSyncedDatabaseTablesPlain(GetDatabaseSyncedDatabaseTablesPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTables(GetDatabaseSyncedDatabaseTablesArgs args) {
        return getDatabaseSyncedDatabaseTables(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTablesPlain(GetDatabaseSyncedDatabaseTablesPlainArgs args) {
        return getDatabaseSyncedDatabaseTablesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTables(GetDatabaseSyncedDatabaseTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseSyncedDatabaseTables:getDatabaseSyncedDatabaseTables", TypeShape.of(GetDatabaseSyncedDatabaseTablesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTables(GetDatabaseSyncedDatabaseTablesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseSyncedDatabaseTables:getDatabaseSyncedDatabaseTables", TypeShape.of(GetDatabaseSyncedDatabaseTablesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetDatabaseSyncedDatabaseTablesResult> getDatabaseSyncedDatabaseTablesPlain(GetDatabaseSyncedDatabaseTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseSyncedDatabaseTables:getDatabaseSyncedDatabaseTables", TypeShape.of(GetDatabaseSyncedDatabaseTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args) {
        return getDbfsFile(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args) {
        return getDbfsFilePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args) {
        return getDbfsFilePaths(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args) {
        return getDbfsFilePathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args) {
        return getDirectory(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args) {
        return getDirectoryPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetEntityTagAssignmentResult> getEntityTagAssignment(GetEntityTagAssignmentArgs args) {
        return getEntityTagAssignment(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetEntityTagAssignmentResult> getEntityTagAssignmentPlain(GetEntityTagAssignmentPlainArgs args) {
        return getEntityTagAssignmentPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetEntityTagAssignmentResult> getEntityTagAssignment(GetEntityTagAssignmentArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getEntityTagAssignment:getEntityTagAssignment", TypeShape.of(GetEntityTagAssignmentResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetEntityTagAssignmentResult> getEntityTagAssignment(GetEntityTagAssignmentArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getEntityTagAssignment:getEntityTagAssignment", TypeShape.of(GetEntityTagAssignmentResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetEntityTagAssignmentResult> getEntityTagAssignmentPlain(GetEntityTagAssignmentPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getEntityTagAssignment:getEntityTagAssignment", TypeShape.of(GetEntityTagAssignmentResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetEntityTagAssignmentsResult> getEntityTagAssignments(GetEntityTagAssignmentsArgs args) {
        return getEntityTagAssignments(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetEntityTagAssignmentsResult> getEntityTagAssignmentsPlain(GetEntityTagAssignmentsPlainArgs args) {
        return getEntityTagAssignmentsPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetEntityTagAssignmentsResult> getEntityTagAssignments(GetEntityTagAssignmentsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getEntityTagAssignments:getEntityTagAssignments", TypeShape.of(GetEntityTagAssignmentsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetEntityTagAssignmentsResult> getEntityTagAssignments(GetEntityTagAssignmentsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getEntityTagAssignments:getEntityTagAssignments", TypeShape.of(GetEntityTagAssignmentsResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetEntityTagAssignmentsResult> getEntityTagAssignmentsPlain(GetEntityTagAssignmentsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getEntityTagAssignments:getEntityTagAssignments", TypeShape.of(GetEntityTagAssignmentsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args) {
        return getExternalLocation(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args) {
        return getExternalLocationPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations() {
        return getExternalLocations(GetExternalLocationsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain() {
        return getExternalLocationsPlain(GetExternalLocationsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args) {
        return getExternalLocations(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args) {
        return getExternalLocationsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single external metadata object.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an external metadata object by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalMetadata(GetExternalMetadataArgs.builder()
     *             .name("security_events_stream")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadataResult> getExternalMetadata(GetExternalMetadataArgs args) {
        return getExternalMetadata(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single external metadata object.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an external metadata object by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalMetadata(GetExternalMetadataArgs.builder()
     *             .name("security_events_stream")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetExternalMetadataResult> getExternalMetadataPlain(GetExternalMetadataPlainArgs args) {
        return getExternalMetadataPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single external metadata object.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an external metadata object by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalMetadata(GetExternalMetadataArgs.builder()
     *             .name("security_events_stream")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadataResult> getExternalMetadata(GetExternalMetadataArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalMetadata:getExternalMetadata", TypeShape.of(GetExternalMetadataResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single external metadata object.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an external metadata object by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalMetadata(GetExternalMetadataArgs.builder()
     *             .name("security_events_stream")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadataResult> getExternalMetadata(GetExternalMetadataArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalMetadata:getExternalMetadata", TypeShape.of(GetExternalMetadataResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single external metadata object.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an external metadata object by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadataArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalMetadata(GetExternalMetadataArgs.builder()
     *             .name("security_events_stream")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetExternalMetadataResult> getExternalMetadataPlain(GetExternalMetadataPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalMetadata:getExternalMetadata", TypeShape.of(GetExternalMetadataResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadatasResult> getExternalMetadatas() {
        return getExternalMetadatas(GetExternalMetadatasArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetExternalMetadatasResult> getExternalMetadatasPlain() {
        return getExternalMetadatasPlain(GetExternalMetadatasPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadatasResult> getExternalMetadatas(GetExternalMetadatasArgs args) {
        return getExternalMetadatas(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetExternalMetadatasResult> getExternalMetadatasPlain(GetExternalMetadatasPlainArgs args) {
        return getExternalMetadatasPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadatasResult> getExternalMetadatas(GetExternalMetadatasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalMetadatas:getExternalMetadatas", TypeShape.of(GetExternalMetadatasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetExternalMetadatasResult> getExternalMetadatas(GetExternalMetadatasArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalMetadatas:getExternalMetadatas", TypeShape.of(GetExternalMetadatasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of external metadata objects.
     * 
     * &gt; **Note** This resource can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all external metadata objects:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalMetadatasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalMetadatas(GetExternalMetadatasArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetExternalMetadatasResult> getExternalMetadatasPlain(GetExternalMetadatasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalMetadatas:getExternalMetadatas", TypeShape.of(GetExternalMetadatasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args) {
        return getFunctions(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static CompletableFuture<GetFunctionsResult> getFunctionsPlain(GetFunctionsPlainArgs args) {
        return getFunctionsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static CompletableFuture<GetFunctionsResult> getFunctionsPlain(GetFunctionsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args) {
        return getGroup(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args) {
        return getGroupPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args) {
        return getInstancePool(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args) {
        return getInstancePoolPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles() {
        return getInstanceProfiles(GetInstanceProfilesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain() {
        return getInstanceProfilesPlain(GetInstanceProfilesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args) {
        return getInstanceProfiles(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args) {
        return getInstanceProfilesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob() {
        return getJob(GetJobArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain() {
        return getJobPlain(GetJobPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args) {
        return getJob(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args) {
        return getJobPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs() {
        return getJobs(GetJobsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain() {
        return getJobsPlain(GetJobsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args) {
        return getJobs(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args) {
        return getJobsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetMaterializedFeaturesFeatureTagResult> getMaterializedFeaturesFeatureTag(GetMaterializedFeaturesFeatureTagArgs args) {
        return getMaterializedFeaturesFeatureTag(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetMaterializedFeaturesFeatureTagResult> getMaterializedFeaturesFeatureTagPlain(GetMaterializedFeaturesFeatureTagPlainArgs args) {
        return getMaterializedFeaturesFeatureTagPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetMaterializedFeaturesFeatureTagResult> getMaterializedFeaturesFeatureTag(GetMaterializedFeaturesFeatureTagArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMaterializedFeaturesFeatureTag:getMaterializedFeaturesFeatureTag", TypeShape.of(GetMaterializedFeaturesFeatureTagResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetMaterializedFeaturesFeatureTagResult> getMaterializedFeaturesFeatureTag(GetMaterializedFeaturesFeatureTagArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMaterializedFeaturesFeatureTag:getMaterializedFeaturesFeatureTag", TypeShape.of(GetMaterializedFeaturesFeatureTagResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetMaterializedFeaturesFeatureTagResult> getMaterializedFeaturesFeatureTagPlain(GetMaterializedFeaturesFeatureTagPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMaterializedFeaturesFeatureTag:getMaterializedFeaturesFeatureTag", TypeShape.of(GetMaterializedFeaturesFeatureTagResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTags() {
        return getMaterializedFeaturesFeatureTags(GetMaterializedFeaturesFeatureTagsArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTagsPlain() {
        return getMaterializedFeaturesFeatureTagsPlain(GetMaterializedFeaturesFeatureTagsPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTags(GetMaterializedFeaturesFeatureTagsArgs args) {
        return getMaterializedFeaturesFeatureTags(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTagsPlain(GetMaterializedFeaturesFeatureTagsPlainArgs args) {
        return getMaterializedFeaturesFeatureTagsPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTags(GetMaterializedFeaturesFeatureTagsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMaterializedFeaturesFeatureTags:getMaterializedFeaturesFeatureTags", TypeShape.of(GetMaterializedFeaturesFeatureTagsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTags(GetMaterializedFeaturesFeatureTagsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMaterializedFeaturesFeatureTags:getMaterializedFeaturesFeatureTags", TypeShape.of(GetMaterializedFeaturesFeatureTagsResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetMaterializedFeaturesFeatureTagsResult> getMaterializedFeaturesFeatureTagsPlain(GetMaterializedFeaturesFeatureTagsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMaterializedFeaturesFeatureTags:getMaterializedFeaturesFeatureTags", TypeShape.of(GetMaterializedFeaturesFeatureTagsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore() {
        return getMetastore(GetMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain() {
        return getMetastorePlain(GetMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args) {
        return getMetastore(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args) {
        return getMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores() {
        return getMetastores(GetMetastoresArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain() {
        return getMetastoresPlain(GetMetastoresPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args) {
        return getMetastores(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args) {
        return getMetastoresPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment() {
        return getMlflowExperiment(GetMlflowExperimentArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain() {
        return getMlflowExperimentPlain(GetMlflowExperimentPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args) {
        return getMlflowExperiment(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args) {
        return getMlflowExperimentPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args) {
        return getMlflowModel(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args) {
        return getMlflowModelPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels() {
        return getMlflowModels(GetMlflowModelsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain() {
        return getMlflowModelsPlain(GetMlflowModelsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args) {
        return getMlflowModels(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain(GetMlflowModelsPlainArgs args) {
        return getMlflowModelsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain(GetMlflowModelsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials() {
        return getMwsCredentials(GetMwsCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain() {
        return getMwsCredentialsPlain(GetMwsCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args) {
        return getMwsCredentials(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args) {
        return getMwsCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args) {
        return getMwsNetworkConnectivityConfig(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfigPlain(GetMwsNetworkConnectivityConfigPlainArgs args) {
        return getMwsNetworkConnectivityConfigPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfigPlain(GetMwsNetworkConnectivityConfigPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs() {
        return getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain() {
        return getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args) {
        return getMwsNetworkConnectivityConfigs(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs args) {
        return getMwsNetworkConnectivityConfigsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces() {
        return getMwsWorkspaces(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain() {
        return getMwsWorkspacesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args) {
        return getMwsWorkspaces(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(InvokeArgs args) {
        return getMwsWorkspacesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurermDatabricksWorkspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType() {
        return getNodeType(GetNodeTypeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain() {
        return getNodeTypePlain(GetNodeTypePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args) {
        return getNodeType(args, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args) {
        return getNodeTypePlain(args, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `minGpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args) {
        return getNotebook(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args) {
        return getNotebookPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args) {
        return getNotebookPaths(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args) {
        return getNotebookPathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations() {
        return getNotificationDestinations(GetNotificationDestinationsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain() {
        return getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args) {
        return getNotificationDestinations(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs args) {
        return getNotificationDestinationsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args) {
        return getOnlineStore(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoreResult> getOnlineStorePlain(GetOnlineStorePlainArgs args) {
        return getOnlineStorePlain(args, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetOnlineStoreResult> getOnlineStorePlain(GetOnlineStorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoresResult> getOnlineStores() {
        return getOnlineStores(GetOnlineStoresArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain() {
        return getOnlineStoresPlain(GetOnlineStoresPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(GetOnlineStoresArgs args) {
        return getOnlineStores(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain(GetOnlineStoresPlainArgs args) {
        return getOnlineStoresPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(GetOnlineStoresArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(GetOnlineStoresArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain(GetOnlineStoresPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines() {
        return getPipelines(GetPipelinesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain() {
        return getPipelinesPlain(GetPipelinesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args) {
        return getPipelines(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args) {
        return getPipelinesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Lakeflow Declarative Pipelines:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (exact match):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * Filter Lakeflow Declarative Pipelines by name (wildcard search):
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetPolicyInfoResult> getPolicyInfo(GetPolicyInfoArgs args) {
        return getPolicyInfo(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetPolicyInfoResult> getPolicyInfoPlain(GetPolicyInfoPlainArgs args) {
        return getPolicyInfoPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetPolicyInfoResult> getPolicyInfo(GetPolicyInfoArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPolicyInfo:getPolicyInfo", TypeShape.of(GetPolicyInfoResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetPolicyInfoResult> getPolicyInfo(GetPolicyInfoArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPolicyInfo:getPolicyInfo", TypeShape.of(GetPolicyInfoResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetPolicyInfoResult> getPolicyInfoPlain(GetPolicyInfoPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPolicyInfo:getPolicyInfo", TypeShape.of(GetPolicyInfoResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetPolicyInfosResult> getPolicyInfos(GetPolicyInfosArgs args) {
        return getPolicyInfos(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetPolicyInfosResult> getPolicyInfosPlain(GetPolicyInfosPlainArgs args) {
        return getPolicyInfosPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetPolicyInfosResult> getPolicyInfos(GetPolicyInfosArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPolicyInfos:getPolicyInfos", TypeShape.of(GetPolicyInfosResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetPolicyInfosResult> getPolicyInfos(GetPolicyInfosArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPolicyInfos:getPolicyInfos", TypeShape.of(GetPolicyInfosResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetPolicyInfosResult> getPolicyInfosPlain(GetPolicyInfosPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPolicyInfos:getPolicyInfos", TypeShape.of(GetPolicyInfosResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args) {
        return getQualityMonitorV2(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetQualityMonitorV2Result> getQualityMonitorV2Plain(GetQualityMonitorV2PlainArgs args) {
        return getQualityMonitorV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetQualityMonitorV2Result> getQualityMonitorV2Plain(GetQualityMonitorV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2() {
        return getQualityMonitorsV2(GetQualityMonitorsV2Args.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain() {
        return getQualityMonitorsV2Plain(GetQualityMonitorsV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(GetQualityMonitorsV2Args args) {
        return getQualityMonitorsV2(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain(GetQualityMonitorsV2PlainArgs args) {
        return getQualityMonitorsV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(GetQualityMonitorsV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(GetQualityMonitorsV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetQualityMonitorsV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(GetQualityMonitorsV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain(GetQualityMonitorsV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies() {
        return getRecipientFederationPolicies(GetRecipientFederationPoliciesArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain() {
        return getRecipientFederationPoliciesPlain(GetRecipientFederationPoliciesPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(GetRecipientFederationPoliciesArgs args) {
        return getRecipientFederationPolicies(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain(GetRecipientFederationPoliciesPlainArgs args) {
        return getRecipientFederationPoliciesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(GetRecipientFederationPoliciesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(GetRecipientFederationPoliciesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain(GetRecipientFederationPoliciesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy() {
        return getRecipientFederationPolicy(GetRecipientFederationPolicyArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain() {
        return getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args) {
        return getRecipientFederationPolicy(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs args) {
        return getRecipientFederationPolicyPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args) {
        return getRegisteredModel(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelResult> getRegisteredModelPlain(GetRegisteredModelPlainArgs args) {
        return getRegisteredModelPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelResult> getRegisteredModelPlain(GetRegisteredModelPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args) {
        return getRegisteredModelVersions(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelVersionsResult> getRegisteredModelVersionsPlain(GetRegisteredModelVersionsPlainArgs args) {
        return getRegisteredModelVersionsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelVersionsResult> getRegisteredModelVersionsPlain(GetRegisteredModelVersionsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalogName`.`schemaName`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args) {
        return getSchema(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalogName`.`schemaName`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args) {
        return getSchemaPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalogName`.`schemaName`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalogName`.`schemaName`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalogName`.`schemaName`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args) {
        return getSchemas(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args) {
        return getSchemasPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal() {
        return getServicePrincipal(GetServicePrincipalArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain() {
        return getServicePrincipalPlain(GetServicePrincipalPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args) {
        return getServicePrincipal(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args) {
        return getServicePrincipalPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricksService principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of federation policies for a service principal.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all service principal federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs.builder()
     *             .servicePrincipalId(1234)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetServicePrincipalFederationPoliciesResult> getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs args) {
        return getServicePrincipalFederationPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of federation policies for a service principal.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all service principal federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs.builder()
     *             .servicePrincipalId(1234)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetServicePrincipalFederationPoliciesResult> getServicePrincipalFederationPoliciesPlain(GetServicePrincipalFederationPoliciesPlainArgs args) {
        return getServicePrincipalFederationPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of federation policies for a service principal.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all service principal federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs.builder()
     *             .servicePrincipalId(1234)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetServicePrincipalFederationPoliciesResult> getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipalFederationPolicies:getServicePrincipalFederationPolicies", TypeShape.of(GetServicePrincipalFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of federation policies for a service principal.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all service principal federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs.builder()
     *             .servicePrincipalId(1234)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetServicePrincipalFederationPoliciesResult> getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipalFederationPolicies:getServicePrincipalFederationPolicies", TypeShape.of(GetServicePrincipalFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of federation policies for a service principal.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all service principal federation policies:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServicePrincipalFederationPoliciesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServicePrincipalFederationPolicies(GetServicePrincipalFederationPoliciesArgs.builder()
     *             .servicePrincipalId(1234)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetServicePrincipalFederationPoliciesResult> getServicePrincipalFederationPoliciesPlain(GetServicePrincipalFederationPoliciesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipalFederationPolicies:getServicePrincipalFederationPolicies", TypeShape.of(GetServicePrincipalFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static Output<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicy() {
        return getServicePrincipalFederationPolicy(GetServicePrincipalFederationPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static CompletableFuture<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicyPlain() {
        return getServicePrincipalFederationPolicyPlain(GetServicePrincipalFederationPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static Output<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicy(GetServicePrincipalFederationPolicyArgs args) {
        return getServicePrincipalFederationPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static CompletableFuture<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicyPlain(GetServicePrincipalFederationPolicyPlainArgs args) {
        return getServicePrincipalFederationPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static Output<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicy(GetServicePrincipalFederationPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipalFederationPolicy:getServicePrincipalFederationPolicy", TypeShape.of(GetServicePrincipalFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static Output<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicy(GetServicePrincipalFederationPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipalFederationPolicy:getServicePrincipalFederationPolicy", TypeShape.of(GetServicePrincipalFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single service principal federation policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a service principal federation policy by id:
     * 
     */
    public static CompletableFuture<GetServicePrincipalFederationPolicyResult> getServicePrincipalFederationPolicyPlain(GetServicePrincipalFederationPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipalFederationPolicy:getServicePrincipalFederationPolicy", TypeShape.of(GetServicePrincipalFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals() {
        return getServicePrincipals(GetServicePrincipalsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain() {
        return getServicePrincipalsPlain(GetServicePrincipalsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args) {
        return getServicePrincipals(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args) {
        return getServicePrincipalsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `applicationIds` of all databricks.ServicePrincipal based on their `displayName`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints() {
        return getServingEndpoints(GetServingEndpointsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain() {
        return getServingEndpointsPlain(GetServingEndpointsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args) {
        return getServingEndpoints(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain(GetServingEndpointsPlainArgs args) {
        return getServingEndpointsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain(GetServingEndpointsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare() {
        return getShare(GetShareArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain() {
        return getSharePlain(GetSharePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args) {
        return getShare(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args) {
        return getSharePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares() {
        return getShares(GetSharesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain() {
        return getSharesPlain(GetSharesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args) {
        return getShares(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args) {
        return getSharesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion() {
        return getSparkVersion(GetSparkVersionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain() {
        return getSparkVersionPlain(GetSparkVersionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args) {
        return getSparkVersion(args, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args) {
        return getSparkVersionPlain(args, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `sparkVersion` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse() {
        return getSqlWarehouse(GetSqlWarehouseArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain() {
        return getSqlWarehousePlain(GetSqlWarehousePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args) {
        return getSqlWarehouse(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args) {
        return getSqlWarehousePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses() {
        return getSqlWarehouses(GetSqlWarehousesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain() {
        return getSqlWarehousesPlain(GetSqlWarehousesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args) {
        return getSqlWarehouses(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args) {
        return getSqlWarehousesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.Grants to manage data access in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args) {
        return getStorageCredential(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args) {
        return getStorageCredentialPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials() {
        return getStorageCredentials(GetStorageCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain() {
        return getStorageCredentialsPlain(GetStorageCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args) {
        return getStorageCredentials(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args) {
        return getStorageCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args) {
        return getTable(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args) {
        return getTablePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args) {
        return getTables(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args) {
        return getTablesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPoliciesResult> getTagPolicies() {
        return getTagPolicies(GetTagPoliciesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static CompletableFuture<GetTagPoliciesResult> getTagPoliciesPlain() {
        return getTagPoliciesPlain(GetTagPoliciesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPoliciesResult> getTagPolicies(GetTagPoliciesArgs args) {
        return getTagPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static CompletableFuture<GetTagPoliciesResult> getTagPoliciesPlain(GetTagPoliciesPlainArgs args) {
        return getTagPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPoliciesResult> getTagPolicies(GetTagPoliciesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTagPolicies:getTagPolicies", TypeShape.of(GetTagPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPoliciesResult> getTagPolicies(GetTagPoliciesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTagPolicies:getTagPolicies", TypeShape.of(GetTagPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to list all tag policies in the account.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static CompletableFuture<GetTagPoliciesResult> getTagPoliciesPlain(GetTagPoliciesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTagPolicies:getTagPolicies", TypeShape.of(GetTagPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single tag policy by its tag key.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPolicyResult> getTagPolicy(GetTagPolicyArgs args) {
        return getTagPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single tag policy by its tag key.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static CompletableFuture<GetTagPolicyResult> getTagPolicyPlain(GetTagPolicyPlainArgs args) {
        return getTagPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single tag policy by its tag key.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPolicyResult> getTagPolicy(GetTagPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTagPolicy:getTagPolicy", TypeShape.of(GetTagPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single tag policy by its tag key.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static Output<GetTagPolicyResult> getTagPolicy(GetTagPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTagPolicy:getTagPolicy", TypeShape.of(GetTagPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single tag policy by its tag key.
     * 
     * &gt; **Note** This resource can only be used with an account-level provider!
     * 
     */
    public static CompletableFuture<GetTagPolicyResult> getTagPolicyPlain(GetTagPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTagPolicy:getTagPolicy", TypeShape.of(GetTagPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser() {
        return getUser(GetUserArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain() {
        return getUserPlain(GetUserPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args) {
        return getUser(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args) {
        return getUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [Account-level](https://docs.databricks.com/aws/en/admin/users-groups/groups) or [Workspace-level](https://docs.databricks.com/aws/en/admin/users-groups/workspace-local-groups) groups.
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args) {
        return getViews(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args) {
        return getViewsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalogName`.`schemaName`.`volumeName`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args) {
        return getVolume(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalogName`.`schemaName`.`volumeName`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args) {
        return getVolumePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalogName`.`schemaName`.`volumeName`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalogName`.`schemaName`.`volumeName`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalogName`.`schemaName`.`volumeName`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args) {
        return getVolumes(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args) {
        return getVolumesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption() {
        return getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain() {
        return getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args) {
        return getWorkspaceNetworkOption(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs args) {
        return getWorkspaceNetworkOptionPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single workspace network option.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a network policy by id:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.builder()
     *             .workspaceId("9999999999999999")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetWorkspaceSettingV2Result> getWorkspaceSettingV2() {
        return getWorkspaceSettingV2(GetWorkspaceSettingV2Args.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetWorkspaceSettingV2Result> getWorkspaceSettingV2Plain() {
        return getWorkspaceSettingV2Plain(GetWorkspaceSettingV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetWorkspaceSettingV2Result> getWorkspaceSettingV2(GetWorkspaceSettingV2Args args) {
        return getWorkspaceSettingV2(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetWorkspaceSettingV2Result> getWorkspaceSettingV2Plain(GetWorkspaceSettingV2PlainArgs args) {
        return getWorkspaceSettingV2Plain(args, InvokeOptions.Empty);
    }
    public static Output<GetWorkspaceSettingV2Result> getWorkspaceSettingV2(GetWorkspaceSettingV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceSettingV2:getWorkspaceSettingV2", TypeShape.of(GetWorkspaceSettingV2Result.class), args, Utilities.withVersion(options));
    }
    public static Output<GetWorkspaceSettingV2Result> getWorkspaceSettingV2(GetWorkspaceSettingV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceSettingV2:getWorkspaceSettingV2", TypeShape.of(GetWorkspaceSettingV2Result.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetWorkspaceSettingV2Result> getWorkspaceSettingV2Plain(GetWorkspaceSettingV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getWorkspaceSettingV2:getWorkspaceSettingV2", TypeShape.of(GetWorkspaceSettingV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetZonesResult> getZones() {
        return getZones(GetZonesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain() {
        return getZonesPlain(GetZonesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args) {
        return getZones(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args) {
        return getZonesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
}
