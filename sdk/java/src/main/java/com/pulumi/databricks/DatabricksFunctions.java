// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.TypeShape;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.GetAccountNetworkPolicyArgs;
import com.pulumi.databricks.inputs.GetAccountNetworkPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAlertV2Args;
import com.pulumi.databricks.inputs.GetAlertV2PlainArgs;
import com.pulumi.databricks.inputs.GetAppArgs;
import com.pulumi.databricks.inputs.GetAppPlainArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetBudgetPolicyArgs;
import com.pulumi.databricks.inputs.GetBudgetPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogArgs;
import com.pulumi.databricks.inputs.GetCatalogPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogsArgs;
import com.pulumi.databricks.inputs.GetCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetClusterArgs;
import com.pulumi.databricks.inputs.GetClusterPlainArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetClustersArgs;
import com.pulumi.databricks.inputs.GetClustersPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetDashboardsArgs;
import com.pulumi.databricks.inputs.GetDashboardsPlainArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
import com.pulumi.databricks.inputs.GetDatabaseInstancePlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFileArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePlainArgs;
import com.pulumi.databricks.inputs.GetDirectoryArgs;
import com.pulumi.databricks.inputs.GetDirectoryPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationArgs;
import com.pulumi.databricks.inputs.GetExternalLocationPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsPlainArgs;
import com.pulumi.databricks.inputs.GetFunctionsArgs;
import com.pulumi.databricks.inputs.GetFunctionsPlainArgs;
import com.pulumi.databricks.inputs.GetGroupArgs;
import com.pulumi.databricks.inputs.GetGroupPlainArgs;
import com.pulumi.databricks.inputs.GetInstancePoolArgs;
import com.pulumi.databricks.inputs.GetInstancePoolPlainArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesPlainArgs;
import com.pulumi.databricks.inputs.GetJobArgs;
import com.pulumi.databricks.inputs.GetJobPlainArgs;
import com.pulumi.databricks.inputs.GetJobsArgs;
import com.pulumi.databricks.inputs.GetJobsPlainArgs;
import com.pulumi.databricks.inputs.GetMetastoreArgs;
import com.pulumi.databricks.inputs.GetMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetMetastoresArgs;
import com.pulumi.databricks.inputs.GetMetastoresPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowModelArgs;
import com.pulumi.databricks.inputs.GetMlflowModelPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
import com.pulumi.databricks.inputs.GetMlflowModelsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigPlainArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsPlainArgs;
import com.pulumi.databricks.inputs.GetNodeTypeArgs;
import com.pulumi.databricks.inputs.GetNodeTypePlainArgs;
import com.pulumi.databricks.inputs.GetNotebookArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsPlainArgs;
import com.pulumi.databricks.inputs.GetNotebookPlainArgs;
import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
import com.pulumi.databricks.inputs.GetNotificationDestinationsPlainArgs;
import com.pulumi.databricks.inputs.GetOnlineStoreArgs;
import com.pulumi.databricks.inputs.GetOnlineStorePlainArgs;
import com.pulumi.databricks.inputs.GetPipelinesArgs;
import com.pulumi.databricks.inputs.GetPipelinesPlainArgs;
import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
import com.pulumi.databricks.inputs.GetQualityMonitorV2PlainArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPolicyArgs;
import com.pulumi.databricks.inputs.GetRecipientFederationPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelPlainArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
import com.pulumi.databricks.inputs.GetRegisteredModelVersionsPlainArgs;
import com.pulumi.databricks.inputs.GetSchemaArgs;
import com.pulumi.databricks.inputs.GetSchemaPlainArgs;
import com.pulumi.databricks.inputs.GetSchemasArgs;
import com.pulumi.databricks.inputs.GetSchemasPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsPlainArgs;
import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
import com.pulumi.databricks.inputs.GetServingEndpointsPlainArgs;
import com.pulumi.databricks.inputs.GetShareArgs;
import com.pulumi.databricks.inputs.GetSharePlainArgs;
import com.pulumi.databricks.inputs.GetSharesArgs;
import com.pulumi.databricks.inputs.GetSharesPlainArgs;
import com.pulumi.databricks.inputs.GetSparkVersionArgs;
import com.pulumi.databricks.inputs.GetSparkVersionPlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousePlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetTableArgs;
import com.pulumi.databricks.inputs.GetTablePlainArgs;
import com.pulumi.databricks.inputs.GetTablesArgs;
import com.pulumi.databricks.inputs.GetTablesPlainArgs;
import com.pulumi.databricks.inputs.GetUserArgs;
import com.pulumi.databricks.inputs.GetUserPlainArgs;
import com.pulumi.databricks.inputs.GetViewsArgs;
import com.pulumi.databricks.inputs.GetViewsPlainArgs;
import com.pulumi.databricks.inputs.GetVolumeArgs;
import com.pulumi.databricks.inputs.GetVolumePlainArgs;
import com.pulumi.databricks.inputs.GetVolumesArgs;
import com.pulumi.databricks.inputs.GetVolumesPlainArgs;
import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionArgs;
import com.pulumi.databricks.inputs.GetWorkspaceNetworkOptionPlainArgs;
import com.pulumi.databricks.inputs.GetZonesArgs;
import com.pulumi.databricks.inputs.GetZonesPlainArgs;
import com.pulumi.databricks.outputs.GetAccountNetworkPoliciesResult;
import com.pulumi.databricks.outputs.GetAccountNetworkPolicyResult;
import com.pulumi.databricks.outputs.GetAlertV2Result;
import com.pulumi.databricks.outputs.GetAlertsV2InvokeResult;
import com.pulumi.databricks.outputs.GetAppResult;
import com.pulumi.databricks.outputs.GetAppsResult;
import com.pulumi.databricks.outputs.GetAwsAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsBucketPolicyResult;
import com.pulumi.databricks.outputs.GetAwsCrossAccountPolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogPolicyResult;
import com.pulumi.databricks.outputs.GetBudgetPoliciesResult;
import com.pulumi.databricks.outputs.GetBudgetPolicyResult;
import com.pulumi.databricks.outputs.GetCatalogResult;
import com.pulumi.databricks.outputs.GetCatalogsResult;
import com.pulumi.databricks.outputs.GetClusterPolicyResult;
import com.pulumi.databricks.outputs.GetClusterResult;
import com.pulumi.databricks.outputs.GetClustersResult;
import com.pulumi.databricks.outputs.GetCurrentConfigResult;
import com.pulumi.databricks.outputs.GetCurrentMetastoreResult;
import com.pulumi.databricks.outputs.GetCurrentUserResult;
import com.pulumi.databricks.outputs.GetDashboardsResult;
import com.pulumi.databricks.outputs.GetDatabaseInstanceResult;
import com.pulumi.databricks.outputs.GetDatabaseInstancesResult;
import com.pulumi.databricks.outputs.GetDbfsFilePathsResult;
import com.pulumi.databricks.outputs.GetDbfsFileResult;
import com.pulumi.databricks.outputs.GetDirectoryResult;
import com.pulumi.databricks.outputs.GetExternalLocationResult;
import com.pulumi.databricks.outputs.GetExternalLocationsResult;
import com.pulumi.databricks.outputs.GetFunctionsResult;
import com.pulumi.databricks.outputs.GetGroupResult;
import com.pulumi.databricks.outputs.GetInstancePoolResult;
import com.pulumi.databricks.outputs.GetInstanceProfilesResult;
import com.pulumi.databricks.outputs.GetJobResult;
import com.pulumi.databricks.outputs.GetJobsResult;
import com.pulumi.databricks.outputs.GetMetastoreResult;
import com.pulumi.databricks.outputs.GetMetastoresResult;
import com.pulumi.databricks.outputs.GetMlflowExperimentResult;
import com.pulumi.databricks.outputs.GetMlflowModelResult;
import com.pulumi.databricks.outputs.GetMlflowModelsResult;
import com.pulumi.databricks.outputs.GetMwsCredentialsResult;
import com.pulumi.databricks.outputs.GetMwsNetworkConnectivityConfigResult;
import com.pulumi.databricks.outputs.GetMwsNetworkConnectivityConfigsResult;
import com.pulumi.databricks.outputs.GetMwsWorkspacesResult;
import com.pulumi.databricks.outputs.GetNodeTypeResult;
import com.pulumi.databricks.outputs.GetNotebookPathsResult;
import com.pulumi.databricks.outputs.GetNotebookResult;
import com.pulumi.databricks.outputs.GetNotificationDestinationsResult;
import com.pulumi.databricks.outputs.GetOnlineStoreResult;
import com.pulumi.databricks.outputs.GetOnlineStoresResult;
import com.pulumi.databricks.outputs.GetPipelinesResult;
import com.pulumi.databricks.outputs.GetQualityMonitorV2Result;
import com.pulumi.databricks.outputs.GetQualityMonitorsV2Result;
import com.pulumi.databricks.outputs.GetRecipientFederationPoliciesResult;
import com.pulumi.databricks.outputs.GetRecipientFederationPolicyResult;
import com.pulumi.databricks.outputs.GetRegisteredModelResult;
import com.pulumi.databricks.outputs.GetRegisteredModelVersionsResult;
import com.pulumi.databricks.outputs.GetSchemaResult;
import com.pulumi.databricks.outputs.GetSchemasResult;
import com.pulumi.databricks.outputs.GetServicePrincipalResult;
import com.pulumi.databricks.outputs.GetServicePrincipalsResult;
import com.pulumi.databricks.outputs.GetServingEndpointsResult;
import com.pulumi.databricks.outputs.GetShareResult;
import com.pulumi.databricks.outputs.GetSharesResult;
import com.pulumi.databricks.outputs.GetSparkVersionResult;
import com.pulumi.databricks.outputs.GetSqlWarehouseResult;
import com.pulumi.databricks.outputs.GetSqlWarehousesResult;
import com.pulumi.databricks.outputs.GetStorageCredentialResult;
import com.pulumi.databricks.outputs.GetStorageCredentialsResult;
import com.pulumi.databricks.outputs.GetTableResult;
import com.pulumi.databricks.outputs.GetTablesResult;
import com.pulumi.databricks.outputs.GetUserResult;
import com.pulumi.databricks.outputs.GetViewsResult;
import com.pulumi.databricks.outputs.GetVolumeResult;
import com.pulumi.databricks.outputs.GetVolumesResult;
import com.pulumi.databricks.outputs.GetWorkspaceNetworkOptionResult;
import com.pulumi.databricks.outputs.GetZonesResult;
import com.pulumi.deployment.Deployment;
import com.pulumi.deployment.InvokeOptions;
import com.pulumi.deployment.InvokeOutputOptions;
import com.pulumi.resources.InvokeArgs;
import java.util.concurrent.CompletableFuture;

public final class DatabricksFunctions {
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies() {
        return getAccountNetworkPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain() {
        return getAccountNetworkPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args) {
        return getAccountNetworkPolicies(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain(InvokeArgs args) {
        return getAccountNetworkPoliciesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetAccountNetworkPoliciesResult> getAccountNetworkPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetAccountNetworkPoliciesResult> getAccountNetworkPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountNetworkPolicies:getAccountNetworkPolicies", TypeShape.of(GetAccountNetworkPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy() {
        return getAccountNetworkPolicy(GetAccountNetworkPolicyArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain() {
        return getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args) {
        return getAccountNetworkPolicy(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs args) {
        return getAccountNetworkPolicyPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetAccountNetworkPolicyResult> getAccountNetworkPolicy(GetAccountNetworkPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetAccountNetworkPolicyResult> getAccountNetworkPolicyPlain(GetAccountNetworkPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAccountNetworkPolicy:getAccountNetworkPolicy", TypeShape.of(GetAccountNetworkPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2() {
        return getAlertV2(GetAlertV2Args.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain() {
        return getAlertV2Plain(GetAlertV2PlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args) {
        return getAlertV2(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain(GetAlertV2PlainArgs args) {
        return getAlertV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertV2Result> getAlertV2(GetAlertV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alert v2 data source allows you to retrieve detailed information about a specific alert in Databricks SQL. This data source provides access to all alert properties, including its configuration, evaluation criteria, notification settings, and schedule.
     * 
     * You can use this data source to:
     * - Retrieve alert details for reference in other resources
     * - Check the current state and configuration of an alert
     * - Verify notification settings and subscribers
     * - Examine the schedule configuration
     * 
     * ## Example Usage
     * 
     * ### Retrieve Alert by ID
     * This example retrieves a specific alert by its ID:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertV2Result> getAlertV2Plain(GetAlertV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAlertV2:getAlertV2", TypeShape.of(GetAlertV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2() {
        return getAlertsV2(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain() {
        return getAlertsV2Plain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(InvokeArgs args) {
        return getAlertsV2(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain(InvokeArgs args) {
        return getAlertsV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAlertsV2InvokeResult> getAlertsV2(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * The SQL Alerts v2 data source allows you to retrieve a list of alerts in Databricks SQL that are accessible to the current user. This data source returns alerts ordered by their creation time.
     * 
     * You can use this data source to:
     * - Get a comprehensive list of all alerts in your workspace
     * - Monitor and audit alert configurations across your workspace
     * 
     * ## Example Usage
     * 
     * ### List All Alerts
     * This example retrieves all alerts accessible to the current user:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAlertV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getAlertV2(GetAlertV2Args.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAlertsV2InvokeResult> getAlertsV2Plain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAlertsV2:getAlertsV2", TypeShape.of(GetAlertsV2InvokeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args) {
        return getApp(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppResult> getAppPlain(GetAppPlainArgs args) {
        return getAppPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppResult> getApp(GetAppArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about a Databricks App.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAppArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getApp(GetAppArgs.builder()
     *             .name("my-custom-app")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppResult> getAppPlain(GetAppPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getApp:getApp", TypeShape.of(GetAppResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps() {
        return getApps(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain() {
        return getAppsPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args) {
        return getApps(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain(InvokeArgs args) {
        return getAppsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static Output<GetAppsResult> getApps(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).
     * 
     * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.
     * 
     * This data source allows you to fetch information about all Databricks Apps within a workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).
     * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.
     * 
     */
    public static CompletableFuture<GetAppsResult> getAppsPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getApps:getApps", TypeShape.of(GetAppsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args) {
        return getAwsAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args) {
        return getAwsAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args) {
        return getAwsBucketPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args) {
        return getAwsBucketPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisBucketV2 = new BucketV2("thisBucketV2", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(thisBucketV2.id())
     *             .policy(this_.applyValue(_this_ -> _this_.json()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy() {
        return getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain() {
        return getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args) {
        return getAwsCrossAccountPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args) {
        return getAwsCrossAccountPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args) {
        return getAwsUnityCatalogAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args) {
        return getAwsUnityCatalogAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args) {
        return getAwsUnityCatalogPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args) {
        return getAwsUnityCatalogPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * &gt; This data source has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())
     *             .build());
     * 
     *         var metastoreDataAccessRolePolicyAttachment = new RolePolicyAttachment("metastoreDataAccessRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .role(metastoreDataAccess.name())
     *             .policyArn(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies() {
        return getBudgetPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain() {
        return getBudgetPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args) {
        return getBudgetPolicies(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain(InvokeArgs args) {
        return getBudgetPoliciesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPoliciesResult> getBudgetPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of budget policies.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all budget policies:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPoliciesResult> getBudgetPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getBudgetPolicies:getBudgetPolicies", TypeShape.of(GetBudgetPoliciesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy() {
        return getBudgetPolicy(GetBudgetPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain() {
        return getBudgetPolicyPlain(GetBudgetPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args) {
        return getBudgetPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain(GetBudgetPolicyPlainArgs args) {
        return getBudgetPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetBudgetPolicyResult> getBudgetPolicy(GetBudgetPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single budget policy.
     * 
     * &gt; **Note** This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a budget policy by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetBudgetPolicyResult> getBudgetPolicyPlain(GetBudgetPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getBudgetPolicy:getBudgetPolicy", TypeShape.of(GetBudgetPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args) {
        return getCatalog(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args) {
        return getCatalogPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs() {
        return getCatalogs(GetCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain() {
        return getCatalogsPlain(GetCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args) {
        return getCatalogs(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args) {
        return getCatalogsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allCatalogs", all);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterResult> getCluster() {
        return getCluster(GetClusterArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain() {
        return getClusterPlain(GetClusterPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args) {
        return getCluster(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args) {
        return getClusterPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy() {
        return getClusterPolicy(GetClusterPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain() {
        return getClusterPolicyPlain(GetClusterPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args) {
        return getClusterPolicy(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args) {
        return getClusterPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters() {
        return getClusters(GetClustersArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain() {
        return getClustersPlain(GetClustersPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args) {
        return getClusters(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args) {
        return getClustersPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ### Filtering clusters
     * 
     * Listing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import com.pulumi.databricks.inputs.GetClustersFilterByArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterStates("RUNNING")
     *                 .build())
     *             .build());
     * 
     *         final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .policyId("1234-5678-9012")
     *                 .build())
     *             .build());
     * 
     *         final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .clusterSources("API")
     *                 .build())
     *             .build());
     * 
     *         final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .filterBy(GetClustersFilterByArgs.builder()
     *                 .isPinned(true)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig() {
        return getCurrentConfig(GetCurrentConfigArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain() {
        return getCurrentConfigPlain(GetCurrentConfigPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args) {
        return getCurrentConfig(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args) {
        return getCurrentConfigPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore() {
        return getCurrentMetastore(GetCurrentMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain() {
        return getCurrentMetastorePlain(GetCurrentMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args) {
        return getCurrentMetastore(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args) {
        return getCurrentMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser() {
        return getCurrentUser(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain() {
        return getCurrentUserPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args) {
        return getCurrentUser(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args) {
        return getCurrentUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDashboardsResult> getDashboards() {
        return getDashboards(GetDashboardsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain() {
        return getDashboardsPlain(GetDashboardsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args) {
        return getDashboards(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain(GetDashboardsPlainArgs args) {
        return getDashboardsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDashboardsResult> getDashboards(GetDashboardsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDashboardsResult> getDashboardsPlain(GetDashboardsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDashboards:getDashboards", TypeShape.of(GetDashboardsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args) {
        return getDatabaseInstance(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDatabaseInstanceResult> getDatabaseInstancePlain(GetDatabaseInstancePlainArgs args) {
        return getDatabaseInstancePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstanceResult> getDatabaseInstance(GetDatabaseInstanceArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to get a single Database Instance.
     * 
     * ## Example Usage
     * 
     * Referring to a Database Instance by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDatabaseInstanceArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getDatabaseInstance(GetDatabaseInstanceArgs.builder()
     *             .name("my-database-instance")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDatabaseInstanceResult> getDatabaseInstancePlain(GetDatabaseInstancePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseInstance:getDatabaseInstance", TypeShape.of(GetDatabaseInstanceResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances() {
        return getDatabaseInstances(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain() {
        return getDatabaseInstancesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(InvokeArgs args) {
        return getDatabaseInstances(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain(InvokeArgs args) {
        return getDatabaseInstancesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDatabaseInstancesResult> getDatabaseInstances(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of Database Instances within the workspace.
     * The list can then be accessed via the data object&#39;s `database_instances` field.
     * 
     * ## Example Usage
     * 
     * Getting a list of all Database Instances:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getDatabaseInstances(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allDatabaseInstances", all.databaseInstances());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDatabaseInstancesResult> getDatabaseInstancesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDatabaseInstances:getDatabaseInstances", TypeShape.of(GetDatabaseInstancesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args) {
        return getDbfsFile(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args) {
        return getDbfsFilePlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args) {
        return getDbfsFilePaths(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args) {
        return getDbfsFilePathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args) {
        return getDirectory(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args) {
        return getDirectoryPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args) {
        return getExternalLocation(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args) {
        return getExternalLocationPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations() {
        return getExternalLocations(GetExternalLocationsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain() {
        return getExternalLocationsPlain(GetExternalLocationsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args) {
        return getExternalLocations(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args) {
        return getExternalLocationsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args) {
        return getFunctions(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static CompletableFuture<GetFunctionsResult> getFunctionsPlain(GetFunctionsPlainArgs args) {
        return getFunctionsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static Output<GetFunctionsResult> getFunctions(GetFunctionsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all functions defined in a specific schema (`main.default` in this example):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetFunctionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()
     *             .catalogName("main")
     *             .schemaName("default")
     *             .build());
     * 
     *         ctx.export("allExternalLocations", all.functions());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to get information about a single schema
     * 
     */
    public static CompletableFuture<GetFunctionsResult> getFunctionsPlain(GetFunctionsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getFunctions:getFunctions", TypeShape.of(GetFunctionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args) {
        return getGroup(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args) {
        return getGroupPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args) {
        return getInstancePool(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args) {
        return getInstancePoolPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_instance_pool.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(pool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles() {
        return getInstanceProfiles(GetInstanceProfilesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain() {
        return getInstanceProfilesPlain(GetInstanceProfilesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args) {
        return getInstanceProfiles(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args) {
        return getInstanceProfilesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allInstanceProfiles", all.instanceProfiles());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob() {
        return getJob(GetJobArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain() {
        return getJobPlain(GetJobPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args) {
        return getJob(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args) {
        return getJobPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs() {
        return getJobs(GetJobsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain() {
        return getJobsPlain(GetJobsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args) {
        return getJobs(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args) {
        return getJobsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; By default, this data resource will error in case of jobs with duplicate names. To support duplicate names, set `key = &#34;id&#34;` to map jobs by ID.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .jobNameContains("test")
     *             .build());
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting IDs of databricks.Job mapped by ID, allowing duplicate job names:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()
     *             .key("id")
     *             .build());
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids())) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore() {
        return getMetastore(GetMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain() {
        return getMetastorePlain(GetMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args) {
        return getMetastore(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args) {
        return getMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var metastore = new BucketV2("metastore", BucketV2Args.builder()
     *             .bucket(String.format("%s-metastore", prefix))
     *             .forceDestroy(true)
     *             .build());
     * 
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(metastore.id().applyValue(_id -> String.format("s3://%s/metastore", _id)))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(_this_ -> _this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores() {
        return getMetastores(GetMetastoresArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain() {
        return getMetastoresPlain(GetMetastoresPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args) {
        return getMetastores(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args) {
        return getMetastoresPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * &gt; Data resource will error in case of metastores with duplicate names.
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMetastores", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment() {
        return getMlflowExperiment(GetMlflowExperimentArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain() {
        return getMlflowExperimentPlain(GetMlflowExperimentPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args) {
        return getMlflowExperiment(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args) {
        return getMlflowExperimentPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args) {
        return getMlflowModel(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args) {
        return getMlflowModelPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels() {
        return getMlflowModels(GetMlflowModelsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain() {
        return getMlflowModelsPlain(GetMlflowModelsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args) {
        return getMlflowModels(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain(GetMlflowModelsPlainArgs args) {
        return getMlflowModelsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelsResult> getMlflowModels(GetMlflowModelsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelsResult> getMlflowModelsPlain(GetMlflowModelsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowModels:getMlflowModels", TypeShape.of(GetMlflowModelsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials() {
        return getMwsCredentials(GetMwsCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain() {
        return getMwsCredentialsPlain(GetMwsCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args) {
        return getMwsCredentials(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args) {
        return getMwsCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allMwsCredentials", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args) {
        return getMwsNetworkConnectivityConfig(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfigPlain(GetMwsNetworkConnectivityConfigPlainArgs args) {
        return getMwsNetworkConnectivityConfigPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Fetching information about a network connectivity configuration in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()
     *             .name("ncc")
     *             .build());
     * 
     *         ctx.export("config", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigResult> getMwsNetworkConnectivityConfigPlain(GetMwsNetworkConnectivityConfigPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig", TypeShape.of(GetMwsNetworkConnectivityConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs() {
        return getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain() {
        return getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args) {
        return getMwsNetworkConnectivityConfigs(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs args) {
        return getMwsNetworkConnectivityConfigsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static Output<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsNetworkConnectivityConfig in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * List all network connectivity configurations in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .build());
     * 
     *         ctx.export("all", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * List network connectivity configurations from a specific region in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()
     *             .region("us-east-1")
     *             .build());
     * 
     *         ctx.export("filtered", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.
     * * databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.
     * 
     */
    public static CompletableFuture<GetMwsNetworkConnectivityConfigsResult> getMwsNetworkConnectivityConfigsPlain(GetMwsNetworkConnectivityConfigsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs", TypeShape.of(GetMwsNetworkConnectivityConfigsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces() {
        return getMwsWorkspaces(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain() {
        return getMwsWorkspacesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args) {
        return getMwsWorkspaces(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(InvokeArgs args) {
        return getMwsWorkspacesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; This data source can only be used with an account-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *         ctx.export("allMwsWorkspaces", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType() {
        return getNodeType(GetNodeTypeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain() {
        return getNodeTypePlain(GetNodeTypePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args) {
        return getNodeType(args, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args) {
        return getNodeTypePlain(args, InvokeOptions.Empty);
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args) {
        return getNotebook(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args) {
        return getNotebookPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args) {
        return getNotebookPaths(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args) {
        return getNotebookPathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations() {
        return getNotificationDestinations(GetNotificationDestinationsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain() {
        return getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args) {
        return getNotificationDestinations(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs args) {
        return getNotificationDestinationsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotificationDestinationsResult> getNotificationDestinations(GetNotificationDestinationsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.NotificationDestination;
     * import com.pulumi.databricks.NotificationDestinationArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;
     * import com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         var email = new NotificationDestination("email", NotificationDestinationArgs.builder()
     *             .displayName("Email Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .email(NotificationDestinationConfigEmailArgs.builder()
     *                     .addresses("abc}{@literal @}{@code gmail.com")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         var slack = new NotificationDestination("slack", NotificationDestinationArgs.builder()
     *             .displayName("Slack Destination")
     *             .config(NotificationDestinationConfigArgs.builder()
     *                 .slack(NotificationDestinationConfigSlackArgs.builder()
     *                     .url("https://hooks.slack.com/services/...")
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *         // Lists all notification desitnations
     *         final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .build());
     * 
     *         // List destinations of specific type and name
     *         final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()
     *             .displayNameContains("Destination")
     *             .type("EMAIL")
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotificationDestinationsResult> getNotificationDestinationsPlain(GetNotificationDestinationsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotificationDestinations:getNotificationDestinations", TypeShape.of(GetNotificationDestinationsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args) {
        return getOnlineStore(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoreResult> getOnlineStorePlain(GetOnlineStorePlainArgs args) {
        return getOnlineStorePlain(args, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoreResult> getOnlineStore(GetOnlineStoreArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetOnlineStoreResult> getOnlineStorePlain(GetOnlineStorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getOnlineStore:getOnlineStore", TypeShape.of(GetOnlineStoreResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoresResult> getOnlineStores() {
        return getOnlineStores(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain() {
        return getOnlineStoresPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(InvokeArgs args) {
        return getOnlineStores(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain(InvokeArgs args) {
        return getOnlineStoresPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetOnlineStoresResult> getOnlineStores(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetOnlineStoresResult> getOnlineStoresPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getOnlineStores:getOnlineStores", TypeShape.of(GetOnlineStoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines() {
        return getPipelines(GetPipelinesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain() {
        return getPipelinesPlain(GetPipelinesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args) {
        return getPipelines(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args) {
        return getPipelinesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .build());
     * 
     *         ctx.export("allPipelines", all.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args) {
        return getQualityMonitorV2(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetQualityMonitorV2Result> getQualityMonitorV2Plain(GetQualityMonitorV2PlainArgs args) {
        return getQualityMonitorV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorV2Result> getQualityMonitorV2(GetQualityMonitorV2Args args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch a quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Referring to a quality monitor by uc object type (currently only support `schema`) and object id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import com.pulumi.databricks.inputs.GetQualityMonitorV2Args;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("my_catalog.my_schema")
     *             .build());
     * 
     *         final var thisGetQualityMonitorV2 = DatabricksFunctions.getQualityMonitorV2(GetQualityMonitorV2Args.builder()
     *             .objectType("schema")
     *             .objectId(this_.schemaInfo().schemaId())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetQualityMonitorV2Result> getQualityMonitorV2Plain(GetQualityMonitorV2PlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getQualityMonitorV2:getQualityMonitorV2", TypeShape.of(GetQualityMonitorV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2() {
        return getQualityMonitorsV2(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain() {
        return getQualityMonitorsV2Plain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(InvokeArgs args) {
        return getQualityMonitorsV2(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain(InvokeArgs args) {
        return getQualityMonitorsV2Plain(args, InvokeOptions.Empty);
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetQualityMonitorsV2Result> getQualityMonitorsV2(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source can be used to fetch the list of quality monitors v2.
     * 
     * &gt; **Note** This data source can only be used with an workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting a list of all quality monitors:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getQualityMonitorsV2(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetQualityMonitorsV2Result> getQualityMonitorsV2Plain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getQualityMonitorsV2:getQualityMonitorsV2", TypeShape.of(GetQualityMonitorsV2Result.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies() {
        return getRecipientFederationPolicies(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain() {
        return getRecipientFederationPoliciesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(InvokeArgs args) {
        return getRecipientFederationPolicies(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain(InvokeArgs args) {
        return getRecipientFederationPoliciesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPoliciesResult> getRecipientFederationPolicies(InvokeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetRecipientFederationPoliciesResult> getRecipientFederationPoliciesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRecipientFederationPolicies:getRecipientFederationPolicies", TypeShape.of(GetRecipientFederationPoliciesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy() {
        return getRecipientFederationPolicy(GetRecipientFederationPolicyArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain() {
        return getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args) {
        return getRecipientFederationPolicy(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs args) {
        return getRecipientFederationPolicyPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetRecipientFederationPolicyResult> getRecipientFederationPolicy(GetRecipientFederationPolicyArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetRecipientFederationPolicyResult> getRecipientFederationPolicyPlain(GetRecipientFederationPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRecipientFederationPolicy:getRecipientFederationPolicy", TypeShape.of(GetRecipientFederationPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args) {
        return getRegisteredModel(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelResult> getRegisteredModelPlain(GetRegisteredModelPlainArgs args) {
        return getRegisteredModelPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelResult> getRegisteredModel(GetRegisteredModelArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelResult> getRegisteredModelPlain(GetRegisteredModelPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRegisteredModel:getRegisteredModel", TypeShape.of(GetRegisteredModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args) {
        return getRegisteredModelVersions(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelVersionsResult> getRegisteredModelVersionsPlain(GetRegisteredModelVersionsPlainArgs args) {
        return getRegisteredModelVersionsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static Output<GetRegisteredModelVersionsResult> getRegisteredModelVersions(GetRegisteredModelVersionsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()
     *             .fullName("main.default.my_model")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.
     * * databricks.RegisteredModel resource to manage models within Unity Catalog.
     * * databricks.ModelServing to serve this model on a Databricks serving endpoint.
     * * databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.
     * 
     */
    public static CompletableFuture<GetRegisteredModelVersionsResult> getRegisteredModelVersionsPlain(GetRegisteredModelVersionsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getRegisteredModelVersions:getRegisteredModelVersions", TypeShape.of(GetRegisteredModelVersionsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args) {
        return getSchema(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args) {
        return getSchemaPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args) {
        return getSchemas(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args) {
        return getSchemasPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal() {
        return getServicePrincipal(GetServicePrincipalArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain() {
        return getServicePrincipalPlain(GetServicePrincipalPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args) {
        return getServicePrincipal(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args) {
        return getServicePrincipalPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_service_principal.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(spn.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals() {
        return getServicePrincipals(GetServicePrincipalsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain() {
        return getServicePrincipalsPlain(GetServicePrincipalsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args) {
        return getServicePrincipals(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args) {
        return getServicePrincipalsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints() {
        return getServingEndpoints(GetServingEndpointsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain() {
        return getServingEndpointsPlain(GetServingEndpointsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args) {
        return getServingEndpoints(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain(GetServingEndpointsPlainArgs args) {
        return getServingEndpointsPlain(args, InvokeOptions.Empty);
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static Output<GetServingEndpointsResult> getServingEndpoints(GetServingEndpointsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetServingEndpointsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()
     *             .build());
     * 
     *         for (var i = 0; i < allDatabricksServingEndpoints.endpoints(); i++) {
     *             new Permissions("mlServingUsage-" + i, PermissionsArgs.builder()
     *                 .servingEndpointId(range.value().id())
     *                 .accessControls(                
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(auto.displayName())
     *                         .permissionLevel("CAN_MANAGE")
     *                         .build(),
     *                     PermissionsAccessControlArgs.builder()
     *                         .groupName(eng.displayName())
     *                         .permissionLevel("CAN_QUERY")
     *                         .build())
     *                 .build());
     * 
     *         
     * }
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.
     * 
     */
    public static CompletableFuture<GetServingEndpointsResult> getServingEndpointsPlain(GetServingEndpointsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServingEndpoints:getServingEndpoints", TypeShape.of(GetServingEndpointsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare() {
        return getShare(GetShareArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain() {
        return getSharePlain(GetSharePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args) {
        return getShare(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args) {
        return getSharePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares() {
        return getShares(GetSharesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain() {
        return getSharesPlain(GetSharesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args) {
        return getShares(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args) {
        return getSharesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()
     *             .build());
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion() {
        return getSparkVersion(GetSparkVersionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain() {
        return getSparkVersionPlain(GetSparkVersionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args) {
        return getSparkVersion(args, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args) {
        return getSparkVersionPlain(args, InvokeOptions.Empty);
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * &gt; This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.id())
     *             .nodeTypeId(withGpu.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse() {
        return getSqlWarehouse(GetSqlWarehouseArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain() {
        return getSqlWarehousePlain(GetSqlWarehousePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args) {
        return getSqlWarehouse(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args) {
        return getSqlWarehousePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses() {
        return getSqlWarehouses(GetSqlWarehousesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain() {
        return getSqlWarehousesPlain(GetSqlWarehousesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args) {
        return getSqlWarehouses(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args) {
        return getSqlWarehousesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args) {
        return getStorageCredential(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args) {
        return getStorageCredentialPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials() {
        return getStorageCredentials(GetStorageCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain() {
        return getStorageCredentialsPlain(GetStorageCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args) {
        return getStorageCredentials(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args) {
        return getStorageCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()
     *             .build());
     * 
     *         ctx.export("allStorageCredentials", all.names());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args) {
        return getTable(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args) {
        return getTablePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.name())
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args) {
        return getTables(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args) {
        return getTablesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser() {
        return getUser(GetUserArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain() {
        return getUserPlain(GetUserPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args) {
        return getUser(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args) {
        return getUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about databricks_user.
     * 
     * &gt; This data source can be used with an account or workspace-level provider.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.id())
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args) {
        return getViews(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args) {
        return getViewsPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getViewsResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getViewsResult.ids())) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args) {
        return getVolume(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args) {
        return getVolumePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific volume by its fully qualified name
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumeArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()
     *             .name("catalog.schema.volume")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args) {
        return getVolumes(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args) {
        return getVolumesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Plugin Framework Migration
     * 
     * The volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=&#34;databricks.getVolumes&#34;`.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption() {
        return getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain() {
        return getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args) {
        return getWorkspaceNetworkOption(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs args) {
        return getWorkspaceNetworkOptionPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOption(GetWorkspaceNetworkOptionArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetWorkspaceNetworkOptionResult> getWorkspaceNetworkOptionPlain(GetWorkspaceNetworkOptionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getWorkspaceNetworkOption:getWorkspaceNetworkOption", TypeShape.of(GetWorkspaceNetworkOptionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones() {
        return getZones(GetZonesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain() {
        return getZonesPlain(GetZonesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args) {
        return getZones(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args) {
        return getZonesPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args, InvokeOutputOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * &gt; This data source can only be used with a workspace-level provider!
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
}
