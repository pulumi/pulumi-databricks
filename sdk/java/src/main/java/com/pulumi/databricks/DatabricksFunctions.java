// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.TypeShape;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogsArgs;
import com.pulumi.databricks.inputs.GetCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetClusterArgs;
import com.pulumi.databricks.inputs.GetClusterPlainArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetClustersArgs;
import com.pulumi.databricks.inputs.GetClustersPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFileArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePlainArgs;
import com.pulumi.databricks.inputs.GetDirectoryArgs;
import com.pulumi.databricks.inputs.GetDirectoryPlainArgs;
import com.pulumi.databricks.inputs.GetGroupArgs;
import com.pulumi.databricks.inputs.GetGroupPlainArgs;
import com.pulumi.databricks.inputs.GetInstancePoolArgs;
import com.pulumi.databricks.inputs.GetInstancePoolPlainArgs;
import com.pulumi.databricks.inputs.GetJobArgs;
import com.pulumi.databricks.inputs.GetJobPlainArgs;
import com.pulumi.databricks.inputs.GetJobsArgs;
import com.pulumi.databricks.inputs.GetJobsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesPlainArgs;
import com.pulumi.databricks.inputs.GetNodeTypeArgs;
import com.pulumi.databricks.inputs.GetNodeTypePlainArgs;
import com.pulumi.databricks.inputs.GetNotebookArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsPlainArgs;
import com.pulumi.databricks.inputs.GetNotebookPlainArgs;
import com.pulumi.databricks.inputs.GetPipelinesArgs;
import com.pulumi.databricks.inputs.GetPipelinesPlainArgs;
import com.pulumi.databricks.inputs.GetSchemasArgs;
import com.pulumi.databricks.inputs.GetSchemasPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsPlainArgs;
import com.pulumi.databricks.inputs.GetShareArgs;
import com.pulumi.databricks.inputs.GetSharePlainArgs;
import com.pulumi.databricks.inputs.GetSharesArgs;
import com.pulumi.databricks.inputs.GetSharesPlainArgs;
import com.pulumi.databricks.inputs.GetSparkVersionArgs;
import com.pulumi.databricks.inputs.GetSparkVersionPlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousePlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesPlainArgs;
import com.pulumi.databricks.inputs.GetTablesArgs;
import com.pulumi.databricks.inputs.GetTablesPlainArgs;
import com.pulumi.databricks.inputs.GetUserArgs;
import com.pulumi.databricks.inputs.GetUserPlainArgs;
import com.pulumi.databricks.inputs.GetViewsArgs;
import com.pulumi.databricks.inputs.GetViewsPlainArgs;
import com.pulumi.databricks.outputs.GetAwsAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsBucketPolicyResult;
import com.pulumi.databricks.outputs.GetAwsCrossAccountPolicyResult;
import com.pulumi.databricks.outputs.GetCatalogsResult;
import com.pulumi.databricks.outputs.GetClusterPolicyResult;
import com.pulumi.databricks.outputs.GetClusterResult;
import com.pulumi.databricks.outputs.GetClustersResult;
import com.pulumi.databricks.outputs.GetCurrentUserResult;
import com.pulumi.databricks.outputs.GetDbfsFilePathsResult;
import com.pulumi.databricks.outputs.GetDbfsFileResult;
import com.pulumi.databricks.outputs.GetDirectoryResult;
import com.pulumi.databricks.outputs.GetGroupResult;
import com.pulumi.databricks.outputs.GetInstancePoolResult;
import com.pulumi.databricks.outputs.GetJobResult;
import com.pulumi.databricks.outputs.GetJobsResult;
import com.pulumi.databricks.outputs.GetMwsCredentialsResult;
import com.pulumi.databricks.outputs.GetMwsWorkspacesResult;
import com.pulumi.databricks.outputs.GetNodeTypeResult;
import com.pulumi.databricks.outputs.GetNotebookPathsResult;
import com.pulumi.databricks.outputs.GetNotebookResult;
import com.pulumi.databricks.outputs.GetPipelinesResult;
import com.pulumi.databricks.outputs.GetSchemasResult;
import com.pulumi.databricks.outputs.GetServicePrincipalResult;
import com.pulumi.databricks.outputs.GetServicePrincipalsResult;
import com.pulumi.databricks.outputs.GetShareResult;
import com.pulumi.databricks.outputs.GetSharesResult;
import com.pulumi.databricks.outputs.GetSparkVersionResult;
import com.pulumi.databricks.outputs.GetSqlWarehouseResult;
import com.pulumi.databricks.outputs.GetSqlWarehousesResult;
import com.pulumi.databricks.outputs.GetTablesResult;
import com.pulumi.databricks.outputs.GetUserResult;
import com.pulumi.databricks.outputs.GetViewsResult;
import com.pulumi.databricks.outputs.GetZonesResult;
import com.pulumi.deployment.Deployment;
import com.pulumi.deployment.InvokeOptions;
import com.pulumi.resources.InvokeArgs;
import java.util.concurrent.CompletableFuture;

public final class DatabricksFunctions {
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args) {
        return getAwsAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args) {
        return getAwsAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args) {
        return getAwsBucketPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args) {
        return getAwsBucketPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy() {
        return getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain() {
        return getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args) {
        return getAwsCrossAccountPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args) {
        return getAwsCrossAccountPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs() {
        return getCatalogs(GetCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain() {
        return getCatalogsPlain(GetCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args) {
        return getCatalogs(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args) {
        return getCatalogsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster() {
        return getCluster(GetClusterArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain() {
        return getClusterPlain(GetClusterPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args) {
        return getCluster(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args) {
        return getClusterPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy() {
        return getClusterPolicy(GetClusterPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain() {
        return getClusterPolicyPlain(GetClusterPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args) {
        return getClusterPolicy(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args) {
        return getClusterPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters() {
        return getClusters(GetClustersArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain() {
        return getClustersPlain(GetClustersPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args) {
        return getClusters(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args) {
        return getClustersPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser() {
        return getCurrentUser(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain() {
        return getCurrentUserPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args) {
        return getCurrentUser(args, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args) {
        return getCurrentUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args) {
        return getDbfsFile(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args) {
        return getDbfsFilePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args) {
        return getDbfsFilePaths(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args) {
        return getDbfsFilePathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args) {
        return getDirectory(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args) {
        return getDirectoryPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args) {
        return getGroup(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args) {
        return getGroupPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args) {
        return getInstancePool(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args) {
        return getInstancePoolPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob() {
        return getJob(GetJobArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain() {
        return getJobPlain(GetJobPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args) {
        return getJob(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args) {
        return getJobPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs() {
        return getJobs(GetJobsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain() {
        return getJobsPlain(GetJobsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args) {
        return getJobs(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args) {
        return getJobsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials() {
        return getMwsCredentials(GetMwsCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain() {
        return getMwsCredentialsPlain(GetMwsCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args) {
        return getMwsCredentials(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args) {
        return getMwsCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces() {
        return getMwsWorkspaces(GetMwsWorkspacesArgs.Empty, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain() {
        return getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs.Empty, InvokeOptions.Empty);
    }
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args) {
        return getMwsWorkspaces(args, InvokeOptions.Empty);
    }
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args) {
        return getMwsWorkspacesPlain(args, InvokeOptions.Empty);
    }
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType() {
        return getNodeType(GetNodeTypeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain() {
        return getNodeTypePlain(GetNodeTypePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args) {
        return getNodeType(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args) {
        return getNodeTypePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args) {
        return getNotebook(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args) {
        return getNotebookPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args) {
        return getNotebookPaths(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args) {
        return getNotebookPathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines() {
        return getPipelines(GetPipelinesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain() {
        return getPipelinesPlain(GetPipelinesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args) {
        return getPipelines(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args) {
        return getPipelinesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args) {
        return getSchemas(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args) {
        return getSchemasPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal() {
        return getServicePrincipal(GetServicePrincipalArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain() {
        return getServicePrincipalPlain(GetServicePrincipalPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args) {
        return getServicePrincipal(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args) {
        return getServicePrincipalPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals() {
        return getServicePrincipals(GetServicePrincipalsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain() {
        return getServicePrincipalsPlain(GetServicePrincipalsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args) {
        return getServicePrincipals(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args) {
        return getServicePrincipalsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare() {
        return getShare(GetShareArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain() {
        return getSharePlain(GetSharePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args) {
        return getShare(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args) {
        return getSharePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares() {
        return getShares(GetSharesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain() {
        return getSharesPlain(GetSharesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args) {
        return getShares(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args) {
        return getSharesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion() {
        return getSparkVersion(GetSparkVersionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain() {
        return getSparkVersionPlain(GetSparkVersionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args) {
        return getSparkVersion(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args) {
        return getSparkVersionPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args) {
        return getSqlWarehouse(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args) {
        return getSqlWarehousePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses() {
        return getSqlWarehouses(GetSqlWarehousesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain() {
        return getSqlWarehousesPlain(GetSqlWarehousesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args) {
        return getSqlWarehouses(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args) {
        return getSqlWarehousesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args) {
        return getTables(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args) {
        return getTablesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser() {
        return getUser(GetUserArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain() {
        return getUserPlain(GetUserPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args) {
        return getUser(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args) {
        return getUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args) {
        return getViews(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args) {
        return getViewsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones() {
        return getZones(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain() {
        return getZonesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones(InvokeArgs args) {
        return getZones(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(InvokeArgs args) {
        return getZonesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
}
