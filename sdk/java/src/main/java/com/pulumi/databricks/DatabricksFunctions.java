// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.TypeShape;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogsArgs;
import com.pulumi.databricks.inputs.GetCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetClusterArgs;
import com.pulumi.databricks.inputs.GetClusterPlainArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetClustersArgs;
import com.pulumi.databricks.inputs.GetClustersPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFileArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePlainArgs;
import com.pulumi.databricks.inputs.GetDirectoryArgs;
import com.pulumi.databricks.inputs.GetDirectoryPlainArgs;
import com.pulumi.databricks.inputs.GetGroupArgs;
import com.pulumi.databricks.inputs.GetGroupPlainArgs;
import com.pulumi.databricks.inputs.GetInstancePoolArgs;
import com.pulumi.databricks.inputs.GetInstancePoolPlainArgs;
import com.pulumi.databricks.inputs.GetJobArgs;
import com.pulumi.databricks.inputs.GetJobPlainArgs;
import com.pulumi.databricks.inputs.GetJobsArgs;
import com.pulumi.databricks.inputs.GetJobsPlainArgs;
import com.pulumi.databricks.inputs.GetMetastoreArgs;
import com.pulumi.databricks.inputs.GetMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetMetastoresArgs;
import com.pulumi.databricks.inputs.GetMetastoresPlainArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesPlainArgs;
import com.pulumi.databricks.inputs.GetNodeTypeArgs;
import com.pulumi.databricks.inputs.GetNodeTypePlainArgs;
import com.pulumi.databricks.inputs.GetNotebookArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsPlainArgs;
import com.pulumi.databricks.inputs.GetNotebookPlainArgs;
import com.pulumi.databricks.inputs.GetPipelinesArgs;
import com.pulumi.databricks.inputs.GetPipelinesPlainArgs;
import com.pulumi.databricks.inputs.GetSchemasArgs;
import com.pulumi.databricks.inputs.GetSchemasPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsPlainArgs;
import com.pulumi.databricks.inputs.GetShareArgs;
import com.pulumi.databricks.inputs.GetSharePlainArgs;
import com.pulumi.databricks.inputs.GetSharesArgs;
import com.pulumi.databricks.inputs.GetSharesPlainArgs;
import com.pulumi.databricks.inputs.GetSparkVersionArgs;
import com.pulumi.databricks.inputs.GetSparkVersionPlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousePlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesPlainArgs;
import com.pulumi.databricks.inputs.GetTablesArgs;
import com.pulumi.databricks.inputs.GetTablesPlainArgs;
import com.pulumi.databricks.inputs.GetUserArgs;
import com.pulumi.databricks.inputs.GetUserPlainArgs;
import com.pulumi.databricks.inputs.GetViewsArgs;
import com.pulumi.databricks.inputs.GetViewsPlainArgs;
import com.pulumi.databricks.outputs.GetAwsAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsBucketPolicyResult;
import com.pulumi.databricks.outputs.GetAwsCrossAccountPolicyResult;
import com.pulumi.databricks.outputs.GetCatalogsResult;
import com.pulumi.databricks.outputs.GetClusterPolicyResult;
import com.pulumi.databricks.outputs.GetClusterResult;
import com.pulumi.databricks.outputs.GetClustersResult;
import com.pulumi.databricks.outputs.GetCurrentUserResult;
import com.pulumi.databricks.outputs.GetDbfsFilePathsResult;
import com.pulumi.databricks.outputs.GetDbfsFileResult;
import com.pulumi.databricks.outputs.GetDirectoryResult;
import com.pulumi.databricks.outputs.GetGroupResult;
import com.pulumi.databricks.outputs.GetInstancePoolResult;
import com.pulumi.databricks.outputs.GetJobResult;
import com.pulumi.databricks.outputs.GetJobsResult;
import com.pulumi.databricks.outputs.GetMetastoreResult;
import com.pulumi.databricks.outputs.GetMetastoresResult;
import com.pulumi.databricks.outputs.GetMwsCredentialsResult;
import com.pulumi.databricks.outputs.GetMwsWorkspacesResult;
import com.pulumi.databricks.outputs.GetNodeTypeResult;
import com.pulumi.databricks.outputs.GetNotebookPathsResult;
import com.pulumi.databricks.outputs.GetNotebookResult;
import com.pulumi.databricks.outputs.GetPipelinesResult;
import com.pulumi.databricks.outputs.GetSchemasResult;
import com.pulumi.databricks.outputs.GetServicePrincipalResult;
import com.pulumi.databricks.outputs.GetServicePrincipalsResult;
import com.pulumi.databricks.outputs.GetShareResult;
import com.pulumi.databricks.outputs.GetSharesResult;
import com.pulumi.databricks.outputs.GetSparkVersionResult;
import com.pulumi.databricks.outputs.GetSqlWarehouseResult;
import com.pulumi.databricks.outputs.GetSqlWarehousesResult;
import com.pulumi.databricks.outputs.GetTablesResult;
import com.pulumi.databricks.outputs.GetUserResult;
import com.pulumi.databricks.outputs.GetViewsResult;
import com.pulumi.databricks.outputs.GetZonesResult;
import com.pulumi.deployment.Deployment;
import com.pulumi.deployment.InvokeOptions;
import com.pulumi.resources.InvokeArgs;
import java.util.concurrent.CompletableFuture;

public final class DatabricksFunctions {
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import com.pulumi.resources.CustomResourceOptions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get(&#34;databricksAccountId&#34;);
     *         final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy(&#34;crossAccountPolicy&#34;, PolicyArgs.builder()        
     *             .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -&gt; getAwsCrossAccountPolicyResult.json()))
     *             .build());
     * 
     *         final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccountRole = new Role(&#34;crossAccountRole&#34;, RoleArgs.builder()        
     *             .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -&gt; getAwsAssumeRolePolicyResult.json()))
     *             .description(&#34;Grants Databricks full access to VPC resources&#34;)
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment(&#34;crossAccountRolePolicyAttachment&#34;, RolePolicyAttachmentArgs.builder()        
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccountRole.name())
     *             .build());
     * 
     *         var thisMwsCredentials = new MwsCredentials(&#34;thisMwsCredentials&#34;, MwsCredentialsArgs.builder()        
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format(&#34;%s-creds&#34;, var_.prefix()))
     *             .roleArn(crossAccountRole.arn())
     *             .build(), CustomResourceOptions.builder()
     *                 .provider(databricks.mws())
     *                 .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args) {
        return getAwsAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import com.pulumi.resources.CustomResourceOptions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get(&#34;databricksAccountId&#34;);
     *         final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy(&#34;crossAccountPolicy&#34;, PolicyArgs.builder()        
     *             .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -&gt; getAwsCrossAccountPolicyResult.json()))
     *             .build());
     * 
     *         final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccountRole = new Role(&#34;crossAccountRole&#34;, RoleArgs.builder()        
     *             .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -&gt; getAwsAssumeRolePolicyResult.json()))
     *             .description(&#34;Grants Databricks full access to VPC resources&#34;)
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment(&#34;crossAccountRolePolicyAttachment&#34;, RolePolicyAttachmentArgs.builder()        
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccountRole.name())
     *             .build());
     * 
     *         var thisMwsCredentials = new MwsCredentials(&#34;thisMwsCredentials&#34;, MwsCredentialsArgs.builder()        
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format(&#34;%s-creds&#34;, var_.prefix()))
     *             .roleArn(crossAccountRole.arn())
     *             .build(), CustomResourceOptions.builder()
     *                 .provider(databricks.mws())
     *                 .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args) {
        return getAwsAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import com.pulumi.resources.CustomResourceOptions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get(&#34;databricksAccountId&#34;);
     *         final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy(&#34;crossAccountPolicy&#34;, PolicyArgs.builder()        
     *             .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -&gt; getAwsCrossAccountPolicyResult.json()))
     *             .build());
     * 
     *         final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccountRole = new Role(&#34;crossAccountRole&#34;, RoleArgs.builder()        
     *             .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -&gt; getAwsAssumeRolePolicyResult.json()))
     *             .description(&#34;Grants Databricks full access to VPC resources&#34;)
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment(&#34;crossAccountRolePolicyAttachment&#34;, RolePolicyAttachmentArgs.builder()        
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccountRole.name())
     *             .build());
     * 
     *         var thisMwsCredentials = new MwsCredentials(&#34;thisMwsCredentials&#34;, MwsCredentialsArgs.builder()        
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format(&#34;%s-creds&#34;, var_.prefix()))
     *             .roleArn(crossAccountRole.arn())
     *             .build(), CustomResourceOptions.builder()
     *                 .provider(databricks.mws())
     *                 .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import com.pulumi.resources.CustomResourceOptions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get(&#34;databricksAccountId&#34;);
     *         final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy(&#34;crossAccountPolicy&#34;, PolicyArgs.builder()        
     *             .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -&gt; getAwsCrossAccountPolicyResult.json()))
     *             .build());
     * 
     *         final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccountRole = new Role(&#34;crossAccountRole&#34;, RoleArgs.builder()        
     *             .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -&gt; getAwsAssumeRolePolicyResult.json()))
     *             .description(&#34;Grants Databricks full access to VPC resources&#34;)
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment(&#34;crossAccountRolePolicyAttachment&#34;, RolePolicyAttachmentArgs.builder()        
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccountRole.name())
     *             .build());
     * 
     *         var thisMwsCredentials = new MwsCredentials(&#34;thisMwsCredentials&#34;, MwsCredentialsArgs.builder()        
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format(&#34;%s-creds&#34;, var_.prefix()))
     *             .roleArn(crossAccountRole.arn())
     *             .build(), CustomResourceOptions.builder()
     *                 .provider(databricks.mws())
     *                 .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args) {
        return getAwsBucketPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args) {
        return getAwsBucketPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide.
     * * End to end workspace management guide
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy() {
        return getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain() {
        return getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args) {
        return getAwsCrossAccountPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args) {
        return getAwsCrossAccountPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks E2 with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs() {
        return getCatalogs(GetCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain() {
        return getCatalogsPlain(GetCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args) {
        return getCatalogs(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args) {
        return getCatalogsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all catalogs:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export(&#34;allCatalogs&#34;, all.applyValue(getCatalogsResult -&gt; getCatalogsResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster() {
        return getCluster(GetClusterArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain() {
        return getClusterPlain(GetClusterPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args) {
        return getCluster(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args) {
        return getClusterPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy() {
        return getClusterPolicy(GetClusterPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain() {
        return getClusterPolicyPlain(GetClusterPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args) {
        return getClusterPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args) {
        return getClusterPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name(&#34;Personal Compute&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .policyId(personal.applyValue(getClusterPolicyResult -&gt; getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters() {
        return getClusters(GetClustersArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain() {
        return getClustersPlain(GetClustersPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args) {
        return getClusters(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args) {
        return getClustersPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all clusters on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser() {
        return getCurrentUser(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain() {
        return getCurrentUserPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args) {
        return getCurrentUser(args, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args) {
        return getCurrentUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `id` -  The id of the calling user.
     * * `external_id` - ID of the user in an external identity provider.
     * * `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`
     * * `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.
     * * `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.
     * * `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.
     * * `workspace_url` - URL of the current Databricks workspace.
     * * `acl_principal_id` - identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com` if current user is user, or `servicePrincipals/00000000-0000-0000-0000-000000000000` if current user is service principal.
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args) {
        return getDbfsFile(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args) {
        return getDbfsFilePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .limitFileSize(&#34;true&#34;)
     *             .path(&#34;dbfs:/reports/some.csv&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args) {
        return getDbfsFilePaths(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args) {
        return getDbfsFilePathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path(&#34;dbfs:/user/hive/default.db/table&#34;)
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args) {
        return getDirectory(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args) {
        return getDirectoryPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args) {
        return getGroup(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args) {
        return getGroupPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         var me = new User(&#34;me&#34;, UserArgs.builder()        
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args) {
        return getInstancePool(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args) {
        return getInstancePoolPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name(&#34;All spot&#34;)
     *             .build());
     * 
     *         var myCluster = new Cluster(&#34;myCluster&#34;, ClusterArgs.builder()        
     *             .instancePoolId(data.databricks_instance_pool().pool().id())
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob() {
        return getJob(GetJobArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain() {
        return getJobPlain(GetJobPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args) {
        return getJob(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args) {
        return getJobPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName(&#34;My job&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;jobNumWorkers&#34;, this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs() {
        return getJobs(GetJobsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain() {
        return getJobsPlain(GetJobsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args) {
        return getJobs(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args) {
        return getJobsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -&gt; {
     *             final var resources = new ArrayList&lt;Permissions&gt;();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions(&#34;everyoneCanViewAllJobs-&#34; + range.key(), PermissionsArgs.builder()                
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName(&#34;users&#34;)
     *                         .permissionLevel(&#34;CAN_VIEW&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * 
     * Getting ID of specific databricks.Job by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export(&#34;x&#34;, %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args) {
        return getMetastore(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args) {
        return getMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores() {
        return getMetastores(GetMetastoresArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain() {
        return getMetastoresPlain(GetMetastoresPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args) {
        return getMetastores(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args) {
        return getMetastoresPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export(&#34;allMetastores&#34;, all.applyValue(getMetastoresResult -&gt; getMetastoresResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials() {
        return getMwsCredentials(GetMwsCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain() {
        return getMwsCredentialsPlain(GetMwsCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args) {
        return getMwsCredentials(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args) {
        return getMwsCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export(&#34;allMwsCredentials&#34;, all.applyValue(getMwsCredentialsResult -&gt; getMwsCredentialsResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces() {
        return getMwsWorkspaces(GetMwsWorkspacesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain() {
        return getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args) {
        return getMwsWorkspaces(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args) {
        return getMwsWorkspacesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export(&#34;allMwsWorkspaces&#34;, all.applyValue(getMwsWorkspacesResult -&gt; getMwsWorkspacesResult.ids()));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks E2 Workspaces.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType() {
        return getNodeType(GetNodeTypeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain() {
        return getNodeTypePlain(GetNodeTypePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args) {
        return getNodeType(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args) {
        return getNodeTypePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args) {
        return getNotebook(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args) {
        return getNotebookPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .format(&#34;SOURCE&#34;)
     *             .path(&#34;/Production/Features&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args) {
        return getNotebookPaths(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args) {
        return getNotebookPathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path(&#34;/Production&#34;)
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines() {
        return getPipelines(GetPipelinesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain() {
        return getPipelinesPlain(GetPipelinesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args) {
        return getPipelines(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args) {
        return getPipelinesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export(&#34;allPipelines&#34;, all.applyValue(getPipelinesResult -&gt; getPipelinesResult.ids()));
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;my_pipeline&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;myPipeline&#34;, this_.ids());
     *     }
     * }
     * ```
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName(&#34;%pipeline%&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;wildcardPipelines&#34;, this_.ids());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args) {
        return getSchemas(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args) {
        return getSchemasPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;allSandboxSchemas&#34;, sandbox.applyValue(getSchemasResult -&gt; getSchemasResult));
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal() {
        return getServicePrincipal(GetServicePrincipalArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain() {
        return getServicePrincipalPlain(GetServicePrincipalPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args) {
        return getServicePrincipal(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args) {
        return getServicePrincipalPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId(&#34;11111111-2222-3333-4444-555666777888&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -&gt; getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals() {
        return getServicePrincipals(GetServicePrincipalsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain() {
        return getServicePrincipalsPlain(GetServicePrincipalsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args) {
        return getServicePrincipals(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args) {
        return getServicePrincipalsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare() {
        return getShare(GetShareArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain() {
        return getSharePlain(GetSharePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args) {
        return getShare(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args) {
        return getSharePlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name(&#34;this&#34;)
     *             .build());
     * 
     *         ctx.export(&#34;createdBy&#34;, this_.createdBy());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares() {
        return getShares(GetSharesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain() {
        return getSharesPlain(GetSharesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args) {
        return getShares(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args) {
        return getSharesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export(&#34;shareName&#34;, this_.shares());
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion() {
        return getSparkVersion(GetSparkVersionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain() {
        return getSparkVersionPlain(GetSparkVersionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args) {
        return getSparkVersion(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args) {
        return getSparkVersionPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster(&#34;research&#34;, ClusterArgs.builder()        
     *             .clusterName(&#34;Research Cluster&#34;)
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -&gt; getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -&gt; getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse() {
        return getSqlWarehouse(GetSqlWarehouseArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain() {
        return getSqlWarehousePlain(GetSqlWarehousePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args) {
        return getSqlWarehouse(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args) {
        return getSqlWarehousePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * * Search for a specific SQL Warehouse by name:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name(&#34;Starter Warehouse&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses() {
        return getSqlWarehouses(GetSqlWarehousesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain() {
        return getSqlWarehousesPlain(GetSqlWarehousesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args) {
        return getSqlWarehouses(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args) {
        return getSqlWarehousesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Retrieve all SQL warehouses on this workspace on AWS or GCP:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * ```
     * 
     * Retrieve all clusters with &#34;Shared&#34; in their cluster name on this Azure Databricks workspace:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains(&#34;shared&#34;)
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsTables = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsTables.applyValue(getTablesResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args) {
        return getTables(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsTables = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsTables.applyValue(getTablesResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args) {
        return getTablesPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsTables = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsTables.applyValue(getTablesResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsTables = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsTables.applyValue(getTablesResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser() {
        return getUser(GetUserArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain() {
        return getUserPlain(GetUserPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args) {
        return getUser(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args) {
        return getUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName(&#34;admins&#34;)
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName(&#34;me@example.com&#34;)
     *             .build());
     * 
     *         var myMemberA = new GroupMember(&#34;myMemberA&#34;, GroupMemberArgs.builder()        
     *             .groupId(admins.applyValue(getGroupResult -&gt; getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -&gt; getUserResult.id()))
     *             .build());
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * * databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * * databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * * databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * * databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsViews = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsViews.applyValue(getViewsResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getViewsResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .view(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args) {
        return getViews(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsViews = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsViews.applyValue(getViewsResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getViewsResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .view(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args) {
        return getViewsPlain(args, InvokeOptions.Empty);
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsViews = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsViews.applyValue(getViewsResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getViewsResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .view(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetViewsArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var thingsViews = DatabricksFunctions.getViews(GetViewsArgs.builder()
     *             .catalogName(&#34;sandbox&#34;)
     *             .schemaName(&#34;things&#34;)
     *             .build());
     * 
     *         final var thingsGrants = thingsViews.applyValue(getViewsResult -&gt; {
     *             final var resources = new ArrayList&lt;Grants&gt;();
     *             for (var range : KeyedValue.of(getViewsResult.ids()) {
     *                 var resource = new Grants(&#34;thingsGrants-&#34; + range.key(), GrantsArgs.builder()                
     *                     .view(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal(&#34;sensitive&#34;)
     *                         .privileges(                        
     *                             &#34;SELECT&#34;,
     *                             &#34;MODIFY&#34;)
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * ```
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones() {
        return getZones(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain() {
        return getZonesPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones(InvokeArgs args) {
        return getZones(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(InvokeArgs args) {
        return getZonesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static Output<GetZonesResult> getZones(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * ```java
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * ```
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
}
