// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks
{
    /// <summary>
    /// This resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks.
    /// 
    /// A `databricks.QualityMonitor` is attached to a databricks.SqlTable and can be of type timeseries, snapshot or inference.
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Databricks = Pulumi.Databricks;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var sandbox = new Databricks.Catalog("sandbox", new()
    ///     {
    ///         Name = "sandbox",
    ///         Comment = "this catalog is managed by terraform",
    ///         Properties = 
    ///         {
    ///             { "purpose", "testing" },
    ///         },
    ///     });
    /// 
    ///     var things = new Databricks.Schema("things", new()
    ///     {
    ///         CatalogName = sandbox.Id,
    ///         Name = "things",
    ///         Comment = "this database is managed by terraform",
    ///         Properties = 
    ///         {
    ///             { "kind", "various" },
    ///         },
    ///     });
    /// 
    ///     var myTestTable = new Databricks.SqlTable("myTestTable", new()
    ///     {
    ///         CatalogName = "main",
    ///         SchemaName = things.Name,
    ///         Name = "bar",
    ///         TableType = "MANAGED",
    ///         DataSourceFormat = "DELTA",
    ///         Columns = new[]
    ///         {
    ///             new Databricks.Inputs.SqlTableColumnArgs
    ///             {
    ///                 Name = "timestamp",
    ///                 Type = "int",
    ///             },
    ///         },
    ///     });
    /// 
    ///     var testTimeseriesMonitor = new Databricks.QualityMonitor("testTimeseriesMonitor", new()
    ///     {
    ///         TableName = Output.Tuple(sandbox.Name, things.Name, myTestTable.Name).Apply(values =&gt;
    ///         {
    ///             var sandboxName = values.Item1;
    ///             var thingsName = values.Item2;
    ///             var myTestTableName = values.Item3;
    ///             return $"{sandboxName}.{thingsName}.{myTestTableName}";
    ///         }),
    ///         AssetsDir = myTestTable.Name.Apply(name =&gt; $"/Shared/provider-test/databricks_quality_monitoring/{name}"),
    ///         OutputSchemaName = Output.Tuple(sandbox.Name, things.Name).Apply(values =&gt;
    ///         {
    ///             var sandboxName = values.Item1;
    ///             var thingsName = values.Item2;
    ///             return $"{sandboxName}.{thingsName}";
    ///         }),
    ///         TimeSeries = new Databricks.Inputs.QualityMonitorTimeSeriesArgs
    ///         {
    ///             Granularities = new[]
    ///             {
    ///                 "1 hour",
    ///             },
    ///             TimestampCol = "timestamp",
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Inference Monitor
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Databricks = Pulumi.Databricks;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var testMonitorInference = new Databricks.QualityMonitor("testMonitorInference", new()
    ///     {
    ///         TableName = $"{sandbox.Name}.{things.Name}.{myTestTable.Name}",
    ///         AssetsDir = $"/Shared/provider-test/databricks_quality_monitoring/{myTestTable.Name}",
    ///         OutputSchemaName = $"{sandbox.Name}.{things.Name}",
    ///         InferenceLog = new Databricks.Inputs.QualityMonitorInferenceLogArgs
    ///         {
    ///             Granularities = new[]
    ///             {
    ///                 "1 hour",
    ///             },
    ///             TimestampCol = "timestamp",
    ///             PredictionCol = "prediction",
    ///             ModelIdCol = "model_id",
    ///             ProblemType = "PROBLEM_TYPE_REGRESSION",
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Snapshot Monitor
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Databricks = Pulumi.Databricks;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var testMonitorInference = new Databricks.QualityMonitor("testMonitorInference", new()
    ///     {
    ///         TableName = $"{sandbox.Name}.{things.Name}.{myTestTable.Name}",
    ///         AssetsDir = $"/Shared/provider-test/databricks_quality_monitoring/{myTestTable.Name}",
    ///         OutputSchemaName = $"{sandbox.Name}.{things.Name}",
    ///         Snapshot = null,
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Related Resources
    /// 
    /// The following resources are often used in the same context:
    /// 
    /// * databricks.Catalog
    /// * databricks.Schema
    /// * databricks.SqlTable
    /// </summary>
    [DatabricksResourceType("databricks:index/qualityMonitor:QualityMonitor")]
    public partial class QualityMonitor : global::Pulumi.CustomResource
    {
        /// <summary>
        /// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
        /// </summary>
        [Output("assetsDir")]
        public Output<string> AssetsDir { get; private set; } = null!;

        /// <summary>
        /// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
        /// table.
        /// </summary>
        [Output("baselineTableName")]
        public Output<string?> BaselineTableName { get; private set; } = null!;

        /// <summary>
        /// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
        /// </summary>
        [Output("customMetrics")]
        public Output<ImmutableArray<Outputs.QualityMonitorCustomMetric>> CustomMetrics { get; private set; } = null!;

        /// <summary>
        /// The ID of the generated dashboard.
        /// </summary>
        [Output("dashboardId")]
        public Output<string> DashboardId { get; private set; } = null!;

        /// <summary>
        /// The data classification config for the monitor
        /// </summary>
        [Output("dataClassificationConfig")]
        public Output<Outputs.QualityMonitorDataClassificationConfig?> DataClassificationConfig { get; private set; } = null!;

        /// <summary>
        /// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
        /// </summary>
        [Output("driftMetricsTableName")]
        public Output<string> DriftMetricsTableName { get; private set; } = null!;

        /// <summary>
        /// Configuration for the inference log monitor
        /// </summary>
        [Output("inferenceLog")]
        public Output<Outputs.QualityMonitorInferenceLog?> InferenceLog { get; private set; } = null!;

        [Output("latestMonitorFailureMsg")]
        public Output<string?> LatestMonitorFailureMsg { get; private set; } = null!;

        /// <summary>
        /// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
        /// </summary>
        [Output("monitorVersion")]
        public Output<string> MonitorVersion { get; private set; } = null!;

        /// <summary>
        /// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:
        /// </summary>
        [Output("notifications")]
        public Output<Outputs.QualityMonitorNotifications?> Notifications { get; private set; } = null!;

        /// <summary>
        /// Schema where output metric tables are created
        /// </summary>
        [Output("outputSchemaName")]
        public Output<string> OutputSchemaName { get; private set; } = null!;

        /// <summary>
        /// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
        /// </summary>
        [Output("profileMetricsTableName")]
        public Output<string> ProfileMetricsTableName { get; private set; } = null!;

        /// <summary>
        /// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
        /// </summary>
        [Output("schedule")]
        public Output<Outputs.QualityMonitorSchedule?> Schedule { get; private set; } = null!;

        /// <summary>
        /// Whether to skip creating a default dashboard summarizing data quality metrics.
        /// </summary>
        [Output("skipBuiltinDashboard")]
        public Output<bool?> SkipBuiltinDashboard { get; private set; } = null!;

        /// <summary>
        /// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
        /// </summary>
        [Output("slicingExprs")]
        public Output<ImmutableArray<string>> SlicingExprs { get; private set; } = null!;

        /// <summary>
        /// Configuration for monitoring snapshot tables.
        /// </summary>
        [Output("snapshot")]
        public Output<Outputs.QualityMonitorSnapshot?> Snapshot { get; private set; } = null!;

        /// <summary>
        /// Status of the Monitor
        /// </summary>
        [Output("status")]
        public Output<string> Status { get; private set; } = null!;

        /// <summary>
        /// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
        /// </summary>
        [Output("tableName")]
        public Output<string> TableName { get; private set; } = null!;

        /// <summary>
        /// Configuration for monitoring timeseries tables.
        /// </summary>
        [Output("timeSeries")]
        public Output<Outputs.QualityMonitorTimeSeries?> TimeSeries { get; private set; } = null!;

        /// <summary>
        /// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
        /// </summary>
        [Output("warehouseId")]
        public Output<string?> WarehouseId { get; private set; } = null!;


        /// <summary>
        /// Create a QualityMonitor resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public QualityMonitor(string name, QualityMonitorArgs args, CustomResourceOptions? options = null)
            : base("databricks:index/qualityMonitor:QualityMonitor", name, args ?? new QualityMonitorArgs(), MakeResourceOptions(options, ""))
        {
        }

        private QualityMonitor(string name, Input<string> id, QualityMonitorState? state = null, CustomResourceOptions? options = null)
            : base("databricks:index/qualityMonitor:QualityMonitor", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing QualityMonitor resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static QualityMonitor Get(string name, Input<string> id, QualityMonitorState? state = null, CustomResourceOptions? options = null)
        {
            return new QualityMonitor(name, id, state, options);
        }
    }

    public sealed class QualityMonitorArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
        /// </summary>
        [Input("assetsDir", required: true)]
        public Input<string> AssetsDir { get; set; } = null!;

        /// <summary>
        /// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
        /// table.
        /// </summary>
        [Input("baselineTableName")]
        public Input<string>? BaselineTableName { get; set; }

        [Input("customMetrics")]
        private InputList<Inputs.QualityMonitorCustomMetricArgs>? _customMetrics;

        /// <summary>
        /// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
        /// </summary>
        public InputList<Inputs.QualityMonitorCustomMetricArgs> CustomMetrics
        {
            get => _customMetrics ?? (_customMetrics = new InputList<Inputs.QualityMonitorCustomMetricArgs>());
            set => _customMetrics = value;
        }

        /// <summary>
        /// The data classification config for the monitor
        /// </summary>
        [Input("dataClassificationConfig")]
        public Input<Inputs.QualityMonitorDataClassificationConfigArgs>? DataClassificationConfig { get; set; }

        /// <summary>
        /// Configuration for the inference log monitor
        /// </summary>
        [Input("inferenceLog")]
        public Input<Inputs.QualityMonitorInferenceLogArgs>? InferenceLog { get; set; }

        [Input("latestMonitorFailureMsg")]
        public Input<string>? LatestMonitorFailureMsg { get; set; }

        /// <summary>
        /// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:
        /// </summary>
        [Input("notifications")]
        public Input<Inputs.QualityMonitorNotificationsArgs>? Notifications { get; set; }

        /// <summary>
        /// Schema where output metric tables are created
        /// </summary>
        [Input("outputSchemaName", required: true)]
        public Input<string> OutputSchemaName { get; set; } = null!;

        /// <summary>
        /// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
        /// </summary>
        [Input("schedule")]
        public Input<Inputs.QualityMonitorScheduleArgs>? Schedule { get; set; }

        /// <summary>
        /// Whether to skip creating a default dashboard summarizing data quality metrics.
        /// </summary>
        [Input("skipBuiltinDashboard")]
        public Input<bool>? SkipBuiltinDashboard { get; set; }

        [Input("slicingExprs")]
        private InputList<string>? _slicingExprs;

        /// <summary>
        /// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
        /// </summary>
        public InputList<string> SlicingExprs
        {
            get => _slicingExprs ?? (_slicingExprs = new InputList<string>());
            set => _slicingExprs = value;
        }

        /// <summary>
        /// Configuration for monitoring snapshot tables.
        /// </summary>
        [Input("snapshot")]
        public Input<Inputs.QualityMonitorSnapshotArgs>? Snapshot { get; set; }

        /// <summary>
        /// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
        /// </summary>
        [Input("tableName", required: true)]
        public Input<string> TableName { get; set; } = null!;

        /// <summary>
        /// Configuration for monitoring timeseries tables.
        /// </summary>
        [Input("timeSeries")]
        public Input<Inputs.QualityMonitorTimeSeriesArgs>? TimeSeries { get; set; }

        /// <summary>
        /// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
        /// </summary>
        [Input("warehouseId")]
        public Input<string>? WarehouseId { get; set; }

        public QualityMonitorArgs()
        {
        }
        public static new QualityMonitorArgs Empty => new QualityMonitorArgs();
    }

    public sealed class QualityMonitorState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
        /// </summary>
        [Input("assetsDir")]
        public Input<string>? AssetsDir { get; set; }

        /// <summary>
        /// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
        /// table.
        /// </summary>
        [Input("baselineTableName")]
        public Input<string>? BaselineTableName { get; set; }

        [Input("customMetrics")]
        private InputList<Inputs.QualityMonitorCustomMetricGetArgs>? _customMetrics;

        /// <summary>
        /// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
        /// </summary>
        public InputList<Inputs.QualityMonitorCustomMetricGetArgs> CustomMetrics
        {
            get => _customMetrics ?? (_customMetrics = new InputList<Inputs.QualityMonitorCustomMetricGetArgs>());
            set => _customMetrics = value;
        }

        /// <summary>
        /// The ID of the generated dashboard.
        /// </summary>
        [Input("dashboardId")]
        public Input<string>? DashboardId { get; set; }

        /// <summary>
        /// The data classification config for the monitor
        /// </summary>
        [Input("dataClassificationConfig")]
        public Input<Inputs.QualityMonitorDataClassificationConfigGetArgs>? DataClassificationConfig { get; set; }

        /// <summary>
        /// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
        /// </summary>
        [Input("driftMetricsTableName")]
        public Input<string>? DriftMetricsTableName { get; set; }

        /// <summary>
        /// Configuration for the inference log monitor
        /// </summary>
        [Input("inferenceLog")]
        public Input<Inputs.QualityMonitorInferenceLogGetArgs>? InferenceLog { get; set; }

        [Input("latestMonitorFailureMsg")]
        public Input<string>? LatestMonitorFailureMsg { get; set; }

        /// <summary>
        /// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
        /// </summary>
        [Input("monitorVersion")]
        public Input<string>? MonitorVersion { get; set; }

        /// <summary>
        /// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:
        /// </summary>
        [Input("notifications")]
        public Input<Inputs.QualityMonitorNotificationsGetArgs>? Notifications { get; set; }

        /// <summary>
        /// Schema where output metric tables are created
        /// </summary>
        [Input("outputSchemaName")]
        public Input<string>? OutputSchemaName { get; set; }

        /// <summary>
        /// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
        /// </summary>
        [Input("profileMetricsTableName")]
        public Input<string>? ProfileMetricsTableName { get; set; }

        /// <summary>
        /// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
        /// </summary>
        [Input("schedule")]
        public Input<Inputs.QualityMonitorScheduleGetArgs>? Schedule { get; set; }

        /// <summary>
        /// Whether to skip creating a default dashboard summarizing data quality metrics.
        /// </summary>
        [Input("skipBuiltinDashboard")]
        public Input<bool>? SkipBuiltinDashboard { get; set; }

        [Input("slicingExprs")]
        private InputList<string>? _slicingExprs;

        /// <summary>
        /// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
        /// </summary>
        public InputList<string> SlicingExprs
        {
            get => _slicingExprs ?? (_slicingExprs = new InputList<string>());
            set => _slicingExprs = value;
        }

        /// <summary>
        /// Configuration for monitoring snapshot tables.
        /// </summary>
        [Input("snapshot")]
        public Input<Inputs.QualityMonitorSnapshotGetArgs>? Snapshot { get; set; }

        /// <summary>
        /// Status of the Monitor
        /// </summary>
        [Input("status")]
        public Input<string>? Status { get; set; }

        /// <summary>
        /// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
        /// </summary>
        [Input("tableName")]
        public Input<string>? TableName { get; set; }

        /// <summary>
        /// Configuration for monitoring timeseries tables.
        /// </summary>
        [Input("timeSeries")]
        public Input<Inputs.QualityMonitorTimeSeriesGetArgs>? TimeSeries { get; set; }

        /// <summary>
        /// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
        /// </summary>
        [Input("warehouseId")]
        public Input<string>? WarehouseId { get; set; }

        public QualityMonitorState()
        {
        }
        public static new QualityMonitorState Empty => new QualityMonitorState();
    }
}
