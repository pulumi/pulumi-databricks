// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks.Outputs
{

    [OutputType]
    public sealed class JobTaskForEachTaskTask
    {
        public readonly Outputs.JobTaskForEachTaskTaskCleanRoomsNotebookTask? CleanRoomsNotebookTask;
        public readonly Outputs.JobTaskForEachTaskTaskConditionTask? ConditionTask;
        public readonly Outputs.JobTaskForEachTaskTaskDbtTask? DbtTask;
        /// <summary>
        /// block specifying dependency(-ies) for a given task.
        /// </summary>
        public readonly ImmutableArray<Outputs.JobTaskForEachTaskTaskDependsOn> DependsOns;
        /// <summary>
        /// description for this task.
        /// </summary>
        public readonly string? Description;
        /// <summary>
        /// A flag to disable auto optimization in serverless tasks.
        /// </summary>
        public readonly bool? DisableAutoOptimization;
        /// <summary>
        /// An optional block to specify a set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This block is documented below.
        /// </summary>
        public readonly Outputs.JobTaskForEachTaskTaskEmailNotifications? EmailNotifications;
        /// <summary>
        /// identifier of an `environment` block that is used to specify libraries.  Required for some tasks (`spark_python_task`, `python_wheel_task`, ...) running on serverless compute.
        /// </summary>
        public readonly string? EnvironmentKey;
        /// <summary>
        /// Identifier of the interactive cluster to run job on.  *Note: running tasks on interactive clusters may lead to increased costs!*
        /// </summary>
        public readonly string? ExistingClusterId;
        /// <summary>
        /// block described below that specifies health conditions for a given task.
        /// </summary>
        public readonly Outputs.JobTaskForEachTaskTaskHealth? Health;
        /// <summary>
        /// Identifier of the Job cluster specified in the `job_cluster` block.
        /// </summary>
        public readonly string? JobClusterKey;
        /// <summary>
        /// (Set) An optional list of libraries to be installed on the cluster that will execute the job.
        /// </summary>
        public readonly ImmutableArray<Outputs.JobTaskForEachTaskTaskLibrary> Libraries;
        /// <summary>
        /// (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.
        /// </summary>
        public readonly int? MaxRetries;
        /// <summary>
        /// (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        /// </summary>
        public readonly int? MinRetryIntervalMillis;
        /// <summary>
        /// Task will run on a dedicated cluster.  See databricks.Cluster documentation for specification. *Some parameters, such as `autotermination_minutes`, `is_pinned`, `workload_type` aren't supported!*
        /// </summary>
        public readonly Outputs.JobTaskForEachTaskTaskNewCluster? NewCluster;
        public readonly Outputs.JobTaskForEachTaskTaskNotebookTask? NotebookTask;
        /// <summary>
        /// An optional block controlling the notification settings on the job level documented below.
        /// </summary>
        public readonly Outputs.JobTaskForEachTaskTaskNotificationSettings? NotificationSettings;
        public readonly Outputs.JobTaskForEachTaskTaskPipelineTask? PipelineTask;
        public readonly Outputs.JobTaskForEachTaskTaskPythonWheelTask? PythonWheelTask;
        /// <summary>
        /// (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        /// </summary>
        public readonly bool? RetryOnTimeout;
        /// <summary>
        /// An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of `ALL_SUCCESS`, `AT_LEAST_ONE_SUCCESS`, `NONE_FAILED`, `ALL_DONE`, `AT_LEAST_ONE_FAILED` or `ALL_FAILED`. When omitted, defaults to `ALL_SUCCESS`.
        /// </summary>
        public readonly string? RunIf;
        public readonly Outputs.JobTaskForEachTaskTaskRunJobTask? RunJobTask;
        public readonly Outputs.JobTaskForEachTaskTaskSparkJarTask? SparkJarTask;
        public readonly Outputs.JobTaskForEachTaskTaskSparkPythonTask? SparkPythonTask;
        public readonly Outputs.JobTaskForEachTaskTaskSparkSubmitTask? SparkSubmitTask;
        public readonly Outputs.JobTaskForEachTaskTaskSqlTask? SqlTask;
        /// <summary>
        /// string specifying an unique key for a given task.
        /// * `*_task` - (Required) one of the specific task blocks described below:
        /// </summary>
        public readonly string TaskKey;
        /// <summary>
        /// (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        /// </summary>
        public readonly int? TimeoutSeconds;
        /// <summary>
        /// (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        /// 
        /// &gt; If no `job_cluster_key`, `existing_cluster_id`, or `new_cluster` were specified in task definition, then task will executed using serverless compute.
        /// </summary>
        public readonly Outputs.JobTaskForEachTaskTaskWebhookNotifications? WebhookNotifications;

        [OutputConstructor]
        private JobTaskForEachTaskTask(
            Outputs.JobTaskForEachTaskTaskCleanRoomsNotebookTask? cleanRoomsNotebookTask,

            Outputs.JobTaskForEachTaskTaskConditionTask? conditionTask,

            Outputs.JobTaskForEachTaskTaskDbtTask? dbtTask,

            ImmutableArray<Outputs.JobTaskForEachTaskTaskDependsOn> dependsOns,

            string? description,

            bool? disableAutoOptimization,

            Outputs.JobTaskForEachTaskTaskEmailNotifications? emailNotifications,

            string? environmentKey,

            string? existingClusterId,

            Outputs.JobTaskForEachTaskTaskHealth? health,

            string? jobClusterKey,

            ImmutableArray<Outputs.JobTaskForEachTaskTaskLibrary> libraries,

            int? maxRetries,

            int? minRetryIntervalMillis,

            Outputs.JobTaskForEachTaskTaskNewCluster? newCluster,

            Outputs.JobTaskForEachTaskTaskNotebookTask? notebookTask,

            Outputs.JobTaskForEachTaskTaskNotificationSettings? notificationSettings,

            Outputs.JobTaskForEachTaskTaskPipelineTask? pipelineTask,

            Outputs.JobTaskForEachTaskTaskPythonWheelTask? pythonWheelTask,

            bool? retryOnTimeout,

            string? runIf,

            Outputs.JobTaskForEachTaskTaskRunJobTask? runJobTask,

            Outputs.JobTaskForEachTaskTaskSparkJarTask? sparkJarTask,

            Outputs.JobTaskForEachTaskTaskSparkPythonTask? sparkPythonTask,

            Outputs.JobTaskForEachTaskTaskSparkSubmitTask? sparkSubmitTask,

            Outputs.JobTaskForEachTaskTaskSqlTask? sqlTask,

            string taskKey,

            int? timeoutSeconds,

            Outputs.JobTaskForEachTaskTaskWebhookNotifications? webhookNotifications)
        {
            CleanRoomsNotebookTask = cleanRoomsNotebookTask;
            ConditionTask = conditionTask;
            DbtTask = dbtTask;
            DependsOns = dependsOns;
            Description = description;
            DisableAutoOptimization = disableAutoOptimization;
            EmailNotifications = emailNotifications;
            EnvironmentKey = environmentKey;
            ExistingClusterId = existingClusterId;
            Health = health;
            JobClusterKey = jobClusterKey;
            Libraries = libraries;
            MaxRetries = maxRetries;
            MinRetryIntervalMillis = minRetryIntervalMillis;
            NewCluster = newCluster;
            NotebookTask = notebookTask;
            NotificationSettings = notificationSettings;
            PipelineTask = pipelineTask;
            PythonWheelTask = pythonWheelTask;
            RetryOnTimeout = retryOnTimeout;
            RunIf = runIf;
            RunJobTask = runJobTask;
            SparkJarTask = sparkJarTask;
            SparkPythonTask = sparkPythonTask;
            SparkSubmitTask = sparkSubmitTask;
            SqlTask = sqlTask;
            TaskKey = taskKey;
            TimeoutSeconds = timeoutSeconds;
            WebhookNotifications = webhookNotifications;
        }
    }
}
