// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks.Outputs
{

    [OutputType]
    public sealed class ModelServingProvisionedThroughputAiGateway
    {
        public readonly Outputs.ModelServingProvisionedThroughputAiGatewayFallbackConfig? FallbackConfig;
        /// <summary>
        /// Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:
        /// </summary>
        public readonly Outputs.ModelServingProvisionedThroughputAiGatewayGuardrails? Guardrails;
        /// <summary>
        /// Block describing the configuration of usage tracking. Consists of the following attributes:
        /// </summary>
        public readonly Outputs.ModelServingProvisionedThroughputAiGatewayInferenceTableConfig? InferenceTableConfig;
        /// <summary>
        /// Block describing rate limits for AI gateway. For details see the description of `RateLimits` block above.
        /// </summary>
        public readonly ImmutableArray<Outputs.ModelServingProvisionedThroughputAiGatewayRateLimit> RateLimits;
        /// <summary>
        /// Block with configuration for payload logging using inference tables. For details see the description of `AutoCaptureConfig` block above.
        /// </summary>
        public readonly Outputs.ModelServingProvisionedThroughputAiGatewayUsageTrackingConfig? UsageTrackingConfig;

        [OutputConstructor]
        private ModelServingProvisionedThroughputAiGateway(
            Outputs.ModelServingProvisionedThroughputAiGatewayFallbackConfig? fallbackConfig,

            Outputs.ModelServingProvisionedThroughputAiGatewayGuardrails? guardrails,

            Outputs.ModelServingProvisionedThroughputAiGatewayInferenceTableConfig? inferenceTableConfig,

            ImmutableArray<Outputs.ModelServingProvisionedThroughputAiGatewayRateLimit> rateLimits,

            Outputs.ModelServingProvisionedThroughputAiGatewayUsageTrackingConfig? usageTrackingConfig)
        {
            FallbackConfig = fallbackConfig;
            Guardrails = guardrails;
            InferenceTableConfig = inferenceTableConfig;
            RateLimits = rateLimits;
            UsageTrackingConfig = usageTrackingConfig;
        }
    }
}
