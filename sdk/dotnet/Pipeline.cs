// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks
{
    /// <summary>
    /// Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Databricks = Pulumi.Databricks;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var dltDemo = new Databricks.Notebook("dlt_demo");
    /// 
    ///     var dltDemoRepo = new Databricks.Repo("dlt_demo");
    /// 
    ///     var @this = new Databricks.Pipeline("this", new()
    ///     {
    ///         Name = "Pipeline Name",
    ///         Storage = "/test/first-pipeline",
    ///         Configuration = 
    ///         {
    ///             { "key1", "value1" },
    ///             { "key2", "value2" },
    ///         },
    ///         Clusters = new[]
    ///         {
    ///             new Databricks.Inputs.PipelineClusterArgs
    ///             {
    ///                 Label = "default",
    ///                 NumWorkers = 2,
    ///                 CustomTags = 
    ///                 {
    ///                     { "cluster_type", "default" },
    ///                 },
    ///             },
    ///             new Databricks.Inputs.PipelineClusterArgs
    ///             {
    ///                 Label = "maintenance",
    ///                 NumWorkers = 1,
    ///                 CustomTags = 
    ///                 {
    ///                     { "cluster_type", "maintenance" },
    ///                 },
    ///             },
    ///         },
    ///         Libraries = new[]
    ///         {
    ///             new Databricks.Inputs.PipelineLibraryArgs
    ///             {
    ///                 Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs
    ///                 {
    ///                     Path = dltDemo.Id,
    ///                 },
    ///             },
    ///             new Databricks.Inputs.PipelineLibraryArgs
    ///             {
    ///                 File = new Databricks.Inputs.PipelineLibraryFileArgs
    ///                 {
    ///                     Path = dltDemoRepo.Path.Apply(path =&gt; $"{path}/pipeline.sql"),
    ///                 },
    ///             },
    ///         },
    ///         Continuous = false,
    ///         Notifications = new[]
    ///         {
    ///             new Databricks.Inputs.PipelineNotificationArgs
    ///             {
    ///                 EmailRecipients = new[]
    ///                 {
    ///                     "user@domain.com",
    ///                     "user1@domain.com",
    ///                 },
    ///                 Alerts = new[]
    ///                 {
    ///                     "on-update-failure",
    ///                     "on-update-fatal-failure",
    ///                     "on-update-success",
    ///                     "on-flow-failure",
    ///                 },
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Related Resources
    /// 
    /// The following resources are often used in the same context:
    /// 
    /// * End to end workspace management guide.
    /// * databricks.getPipelines to retrieve [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipeline data.
    /// * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
    /// * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
    /// * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
    /// 
    /// ## Import
    /// 
    /// The resource job can be imported using the id of the pipeline
    /// 
    /// bash
    /// 
    /// ```sh
    /// $ pulumi import databricks:index/pipeline:Pipeline this &lt;pipeline-id&gt;
    /// ```
    /// </summary>
    [DatabricksResourceType("databricks:index/pipeline:Pipeline")]
    public partial class Pipeline : global::Pulumi.CustomResource
    {
        /// <summary>
        /// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
        /// </summary>
        [Output("allowDuplicateNames")]
        public Output<bool?> AllowDuplicateNames { get; private set; } = null!;

        /// <summary>
        /// optional string specifying ID of the budget policy for this DLT pipeline.
        /// </summary>
        [Output("budgetPolicyId")]
        public Output<string?> BudgetPolicyId { get; private set; } = null!;

        /// <summary>
        /// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
        /// </summary>
        [Output("catalog")]
        public Output<string?> Catalog { get; private set; } = null!;

        [Output("cause")]
        public Output<string> Cause { get; private set; } = null!;

        /// <summary>
        /// optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
        /// </summary>
        [Output("channel")]
        public Output<string?> Channel { get; private set; } = null!;

        [Output("clusterId")]
        public Output<string> ClusterId { get; private set; } = null!;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
        /// </summary>
        [Output("clusters")]
        public Output<ImmutableArray<Outputs.PipelineCluster>> Clusters { get; private set; } = null!;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        [Output("configuration")]
        public Output<ImmutableDictionary<string, string>?> Configuration { get; private set; } = null!;

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Output("continuous")]
        public Output<bool?> Continuous { get; private set; } = null!;

        [Output("creatorUserName")]
        public Output<string> CreatorUserName { get; private set; } = null!;

        /// <summary>
        /// Deployment type of this pipeline. Supports following attributes:
        /// </summary>
        [Output("deployment")]
        public Output<Outputs.PipelineDeployment?> Deployment { get; private set; } = null!;

        /// <summary>
        /// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
        /// </summary>
        [Output("development")]
        public Output<bool?> Development { get; private set; } = null!;

        /// <summary>
        /// optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
        /// </summary>
        [Output("edition")]
        public Output<string?> Edition { get; private set; } = null!;

        /// <summary>
        /// an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:
        /// </summary>
        [Output("eventLog")]
        public Output<Outputs.PipelineEventLog?> EventLog { get; private set; } = null!;

        [Output("expectedLastModified")]
        public Output<int?> ExpectedLastModified { get; private set; } = null!;

        /// <summary>
        /// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
        /// </summary>
        [Output("filters")]
        public Output<Outputs.PipelineFilters?> Filters { get; private set; } = null!;

        /// <summary>
        /// The definition of a gateway pipeline to support CDC. Consists of following attributes:
        /// </summary>
        [Output("gatewayDefinition")]
        public Output<Outputs.PipelineGatewayDefinition?> GatewayDefinition { get; private set; } = null!;

        [Output("health")]
        public Output<string> Health { get; private set; } = null!;

        [Output("ingestionDefinition")]
        public Output<Outputs.PipelineIngestionDefinition?> IngestionDefinition { get; private set; } = null!;

        [Output("lastModified")]
        public Output<int> LastModified { get; private set; } = null!;

        [Output("latestUpdates")]
        public Output<ImmutableArray<Outputs.PipelineLatestUpdate>> LatestUpdates { get; private set; } = null!;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` &amp; `file` library types that should have the `path` attribute. *Right now only the `notebook` &amp; `file` types are supported.*
        /// </summary>
        [Output("libraries")]
        public Output<ImmutableArray<Outputs.PipelineLibrary>> Libraries { get; private set; } = null!;

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        [Output("notifications")]
        public Output<ImmutableArray<Outputs.PipelineNotification>> Notifications { get; private set; } = null!;

        /// <summary>
        /// A flag indicating whether to use Photon engine. The default value is `false`.
        /// </summary>
        [Output("photon")]
        public Output<bool?> Photon { get; private set; } = null!;

        [Output("restartWindow")]
        public Output<Outputs.PipelineRestartWindow?> RestartWindow { get; private set; } = null!;

        [Output("runAs")]
        public Output<Outputs.PipelineRunAs> RunAs { get; private set; } = null!;

        [Output("runAsUserName")]
        public Output<string> RunAsUserName { get; private set; } = null!;

        /// <summary>
        /// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
        /// </summary>
        [Output("schema")]
        public Output<string?> Schema { get; private set; } = null!;

        /// <summary>
        /// An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
        /// </summary>
        [Output("serverless")]
        public Output<bool?> Serverless { get; private set; } = null!;

        [Output("state")]
        public Output<string> State { get; private set; } = null!;

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
        /// </summary>
        [Output("storage")]
        public Output<string?> Storage { get; private set; } = null!;

        /// <summary>
        /// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Output("target")]
        public Output<string?> Target { get; private set; } = null!;

        [Output("trigger")]
        public Output<Outputs.PipelineTrigger?> Trigger { get; private set; } = null!;

        /// <summary>
        /// URL of the DLT pipeline on the given workspace.
        /// </summary>
        [Output("url")]
        public Output<string> Url { get; private set; } = null!;


        /// <summary>
        /// Create a Pipeline resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public Pipeline(string name, PipelineArgs? args = null, CustomResourceOptions? options = null)
            : base("databricks:index/pipeline:Pipeline", name, args ?? new PipelineArgs(), MakeResourceOptions(options, ""))
        {
        }

        private Pipeline(string name, Input<string> id, PipelineState? state = null, CustomResourceOptions? options = null)
            : base("databricks:index/pipeline:Pipeline", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing Pipeline resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static Pipeline Get(string name, Input<string> id, PipelineState? state = null, CustomResourceOptions? options = null)
        {
            return new Pipeline(name, id, state, options);
        }
    }

    public sealed class PipelineArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
        /// </summary>
        [Input("allowDuplicateNames")]
        public Input<bool>? AllowDuplicateNames { get; set; }

        /// <summary>
        /// optional string specifying ID of the budget policy for this DLT pipeline.
        /// </summary>
        [Input("budgetPolicyId")]
        public Input<string>? BudgetPolicyId { get; set; }

        /// <summary>
        /// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
        /// </summary>
        [Input("catalog")]
        public Input<string>? Catalog { get; set; }

        [Input("cause")]
        public Input<string>? Cause { get; set; }

        /// <summary>
        /// optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
        /// </summary>
        [Input("channel")]
        public Input<string>? Channel { get; set; }

        [Input("clusterId")]
        public Input<string>? ClusterId { get; set; }

        [Input("clusters")]
        private InputList<Inputs.PipelineClusterArgs>? _clusters;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
        /// </summary>
        public InputList<Inputs.PipelineClusterArgs> Clusters
        {
            get => _clusters ?? (_clusters = new InputList<Inputs.PipelineClusterArgs>());
            set => _clusters = value;
        }

        [Input("configuration")]
        private InputMap<string>? _configuration;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        public InputMap<string> Configuration
        {
            get => _configuration ?? (_configuration = new InputMap<string>());
            set => _configuration = value;
        }

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Input("continuous")]
        public Input<bool>? Continuous { get; set; }

        [Input("creatorUserName")]
        public Input<string>? CreatorUserName { get; set; }

        /// <summary>
        /// Deployment type of this pipeline. Supports following attributes:
        /// </summary>
        [Input("deployment")]
        public Input<Inputs.PipelineDeploymentArgs>? Deployment { get; set; }

        /// <summary>
        /// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
        /// </summary>
        [Input("development")]
        public Input<bool>? Development { get; set; }

        /// <summary>
        /// optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
        /// </summary>
        [Input("edition")]
        public Input<string>? Edition { get; set; }

        /// <summary>
        /// an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:
        /// </summary>
        [Input("eventLog")]
        public Input<Inputs.PipelineEventLogArgs>? EventLog { get; set; }

        [Input("expectedLastModified")]
        public Input<int>? ExpectedLastModified { get; set; }

        /// <summary>
        /// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
        /// </summary>
        [Input("filters")]
        public Input<Inputs.PipelineFiltersArgs>? Filters { get; set; }

        /// <summary>
        /// The definition of a gateway pipeline to support CDC. Consists of following attributes:
        /// </summary>
        [Input("gatewayDefinition")]
        public Input<Inputs.PipelineGatewayDefinitionArgs>? GatewayDefinition { get; set; }

        [Input("health")]
        public Input<string>? Health { get; set; }

        [Input("ingestionDefinition")]
        public Input<Inputs.PipelineIngestionDefinitionArgs>? IngestionDefinition { get; set; }

        [Input("lastModified")]
        public Input<int>? LastModified { get; set; }

        [Input("latestUpdates")]
        private InputList<Inputs.PipelineLatestUpdateArgs>? _latestUpdates;
        public InputList<Inputs.PipelineLatestUpdateArgs> LatestUpdates
        {
            get => _latestUpdates ?? (_latestUpdates = new InputList<Inputs.PipelineLatestUpdateArgs>());
            set => _latestUpdates = value;
        }

        [Input("libraries")]
        private InputList<Inputs.PipelineLibraryArgs>? _libraries;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` &amp; `file` library types that should have the `path` attribute. *Right now only the `notebook` &amp; `file` types are supported.*
        /// </summary>
        public InputList<Inputs.PipelineLibraryArgs> Libraries
        {
            get => _libraries ?? (_libraries = new InputList<Inputs.PipelineLibraryArgs>());
            set => _libraries = value;
        }

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("notifications")]
        private InputList<Inputs.PipelineNotificationArgs>? _notifications;
        public InputList<Inputs.PipelineNotificationArgs> Notifications
        {
            get => _notifications ?? (_notifications = new InputList<Inputs.PipelineNotificationArgs>());
            set => _notifications = value;
        }

        /// <summary>
        /// A flag indicating whether to use Photon engine. The default value is `false`.
        /// </summary>
        [Input("photon")]
        public Input<bool>? Photon { get; set; }

        [Input("restartWindow")]
        public Input<Inputs.PipelineRestartWindowArgs>? RestartWindow { get; set; }

        [Input("runAs")]
        public Input<Inputs.PipelineRunAsArgs>? RunAs { get; set; }

        [Input("runAsUserName")]
        public Input<string>? RunAsUserName { get; set; }

        /// <summary>
        /// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
        /// </summary>
        [Input("schema")]
        public Input<string>? Schema { get; set; }

        /// <summary>
        /// An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
        /// </summary>
        [Input("serverless")]
        public Input<bool>? Serverless { get; set; }

        [Input("state")]
        public Input<string>? State { get; set; }

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
        /// </summary>
        [Input("storage")]
        public Input<string>? Storage { get; set; }

        /// <summary>
        /// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Input("target")]
        public Input<string>? Target { get; set; }

        [Input("trigger")]
        public Input<Inputs.PipelineTriggerArgs>? Trigger { get; set; }

        /// <summary>
        /// URL of the DLT pipeline on the given workspace.
        /// </summary>
        [Input("url")]
        public Input<string>? Url { get; set; }

        public PipelineArgs()
        {
        }
        public static new PipelineArgs Empty => new PipelineArgs();
    }

    public sealed class PipelineState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
        /// </summary>
        [Input("allowDuplicateNames")]
        public Input<bool>? AllowDuplicateNames { get; set; }

        /// <summary>
        /// optional string specifying ID of the budget policy for this DLT pipeline.
        /// </summary>
        [Input("budgetPolicyId")]
        public Input<string>? BudgetPolicyId { get; set; }

        /// <summary>
        /// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
        /// </summary>
        [Input("catalog")]
        public Input<string>? Catalog { get; set; }

        [Input("cause")]
        public Input<string>? Cause { get; set; }

        /// <summary>
        /// optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
        /// </summary>
        [Input("channel")]
        public Input<string>? Channel { get; set; }

        [Input("clusterId")]
        public Input<string>? ClusterId { get; set; }

        [Input("clusters")]
        private InputList<Inputs.PipelineClusterGetArgs>? _clusters;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
        /// </summary>
        public InputList<Inputs.PipelineClusterGetArgs> Clusters
        {
            get => _clusters ?? (_clusters = new InputList<Inputs.PipelineClusterGetArgs>());
            set => _clusters = value;
        }

        [Input("configuration")]
        private InputMap<string>? _configuration;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        public InputMap<string> Configuration
        {
            get => _configuration ?? (_configuration = new InputMap<string>());
            set => _configuration = value;
        }

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Input("continuous")]
        public Input<bool>? Continuous { get; set; }

        [Input("creatorUserName")]
        public Input<string>? CreatorUserName { get; set; }

        /// <summary>
        /// Deployment type of this pipeline. Supports following attributes:
        /// </summary>
        [Input("deployment")]
        public Input<Inputs.PipelineDeploymentGetArgs>? Deployment { get; set; }

        /// <summary>
        /// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
        /// </summary>
        [Input("development")]
        public Input<bool>? Development { get; set; }

        /// <summary>
        /// optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
        /// </summary>
        [Input("edition")]
        public Input<string>? Edition { get; set; }

        /// <summary>
        /// an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:
        /// </summary>
        [Input("eventLog")]
        public Input<Inputs.PipelineEventLogGetArgs>? EventLog { get; set; }

        [Input("expectedLastModified")]
        public Input<int>? ExpectedLastModified { get; set; }

        /// <summary>
        /// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
        /// </summary>
        [Input("filters")]
        public Input<Inputs.PipelineFiltersGetArgs>? Filters { get; set; }

        /// <summary>
        /// The definition of a gateway pipeline to support CDC. Consists of following attributes:
        /// </summary>
        [Input("gatewayDefinition")]
        public Input<Inputs.PipelineGatewayDefinitionGetArgs>? GatewayDefinition { get; set; }

        [Input("health")]
        public Input<string>? Health { get; set; }

        [Input("ingestionDefinition")]
        public Input<Inputs.PipelineIngestionDefinitionGetArgs>? IngestionDefinition { get; set; }

        [Input("lastModified")]
        public Input<int>? LastModified { get; set; }

        [Input("latestUpdates")]
        private InputList<Inputs.PipelineLatestUpdateGetArgs>? _latestUpdates;
        public InputList<Inputs.PipelineLatestUpdateGetArgs> LatestUpdates
        {
            get => _latestUpdates ?? (_latestUpdates = new InputList<Inputs.PipelineLatestUpdateGetArgs>());
            set => _latestUpdates = value;
        }

        [Input("libraries")]
        private InputList<Inputs.PipelineLibraryGetArgs>? _libraries;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` &amp; `file` library types that should have the `path` attribute. *Right now only the `notebook` &amp; `file` types are supported.*
        /// </summary>
        public InputList<Inputs.PipelineLibraryGetArgs> Libraries
        {
            get => _libraries ?? (_libraries = new InputList<Inputs.PipelineLibraryGetArgs>());
            set => _libraries = value;
        }

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        [Input("notifications")]
        private InputList<Inputs.PipelineNotificationGetArgs>? _notifications;
        public InputList<Inputs.PipelineNotificationGetArgs> Notifications
        {
            get => _notifications ?? (_notifications = new InputList<Inputs.PipelineNotificationGetArgs>());
            set => _notifications = value;
        }

        /// <summary>
        /// A flag indicating whether to use Photon engine. The default value is `false`.
        /// </summary>
        [Input("photon")]
        public Input<bool>? Photon { get; set; }

        [Input("restartWindow")]
        public Input<Inputs.PipelineRestartWindowGetArgs>? RestartWindow { get; set; }

        [Input("runAs")]
        public Input<Inputs.PipelineRunAsGetArgs>? RunAs { get; set; }

        [Input("runAsUserName")]
        public Input<string>? RunAsUserName { get; set; }

        /// <summary>
        /// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
        /// </summary>
        [Input("schema")]
        public Input<string>? Schema { get; set; }

        /// <summary>
        /// An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
        /// </summary>
        [Input("serverless")]
        public Input<bool>? Serverless { get; set; }

        [Input("state")]
        public Input<string>? State { get; set; }

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
        /// </summary>
        [Input("storage")]
        public Input<string>? Storage { get; set; }

        /// <summary>
        /// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Input("target")]
        public Input<string>? Target { get; set; }

        [Input("trigger")]
        public Input<Inputs.PipelineTriggerGetArgs>? Trigger { get; set; }

        /// <summary>
        /// URL of the DLT pipeline on the given workspace.
        /// </summary>
        [Input("url")]
        public Input<string>? Url { get; set; }

        public PipelineState()
        {
        }
        public static new PipelineState Empty => new PipelineState();
    }
}
