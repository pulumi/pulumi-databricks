// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks.Inputs
{

    public sealed class JobTaskForEachTaskTaskSparkPythonTaskArgs : global::Pulumi.ResourceArgs
    {
        [Input("parameters")]
        private InputList<string>? _parameters;

        /// <summary>
        /// (List) Command line parameters passed to the Python file.
        /// </summary>
        public InputList<string> Parameters
        {
            get => _parameters ?? (_parameters = new InputList<string>());
            set => _parameters = value;
        }

        /// <summary>
        /// The URI of the Python file to be executed. Cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/`. For files stored in a remote repository, the path must be relative. This field is required.
        /// </summary>
        [Input("pythonFile", required: true)]
        public Input<string> PythonFile { get; set; } = null!;

        /// <summary>
        /// Location type of the Python file. When set to `WORKSPACE` or not specified, the file will be retrieved from the local Databricks workspace or cloud location (if the PythonFile has a URI format). When set to `GIT`, the Python file will be retrieved from a Git repository defined in `GitSource`.
        /// * `WORKSPACE`: The Python file is located in a Databricks workspace or at a cloud filesystem URI.
        /// * `GIT`: The Python file is located in a remote Git repository.
        /// </summary>
        [Input("source")]
        public Input<string>? Source { get; set; }

        public JobTaskForEachTaskTaskSparkPythonTaskArgs()
        {
        }
        public static new JobTaskForEachTaskTaskSparkPythonTaskArgs Empty => new JobTaskForEachTaskTaskSparkPythonTaskArgs();
    }
}
