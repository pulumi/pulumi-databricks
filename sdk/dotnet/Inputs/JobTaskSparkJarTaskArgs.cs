// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks.Inputs
{

    public sealed class JobTaskSparkJarTaskArgs : global::Pulumi.ResourceArgs
    {
        [Input("jarUri")]
        public Input<string>? JarUri { get; set; }

        /// <summary>
        /// The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
        /// </summary>
        [Input("mainClassName")]
        public Input<string>? MainClassName { get; set; }

        [Input("parameters")]
        private InputList<string>? _parameters;

        /// <summary>
        /// (Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.
        /// </summary>
        public InputList<string> Parameters
        {
            get => _parameters ?? (_parameters = new InputList<string>());
            set => _parameters = value;
        }

        public JobTaskSparkJarTaskArgs()
        {
        }
        public static new JobTaskSparkJarTaskArgs Empty => new JobTaskSparkJarTaskArgs();
    }
}
