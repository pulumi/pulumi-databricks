// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks
{
    /// <summary>
    /// [![Private Preview](https://img.shields.io/badge/Release_Stage-Private_Preview-blueviolet)](https://docs.databricks.com/aws/en/release-notes/release-types)
    /// </summary>
    [DatabricksResourceType("databricks:index/featureEngineeringKafkaConfig:FeatureEngineeringKafkaConfig")]
    public partial class FeatureEngineeringKafkaConfig : global::Pulumi.CustomResource
    {
        /// <summary>
        /// Authentication configuration for connection to topics
        /// </summary>
        [Output("authConfig")]
        public Output<Outputs.FeatureEngineeringKafkaConfigAuthConfig> AuthConfig { get; private set; } = null!;

        /// <summary>
        /// A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
        /// In the future, a separate table will be maintained by Databricks for forward filling data.
        /// The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
        /// </summary>
        [Output("backfillSource")]
        public Output<Outputs.FeatureEngineeringKafkaConfigBackfillSource?> BackfillSource { get; private set; } = null!;

        /// <summary>
        /// A comma-separated list of host/port pairs pointing to Kafka cluster
        /// </summary>
        [Output("bootstrapServers")]
        public Output<string> BootstrapServers { get; private set; } = null!;

        /// <summary>
        /// Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
        /// </summary>
        [Output("extraOptions")]
        public Output<ImmutableDictionary<string, string>?> ExtraOptions { get; private set; } = null!;

        /// <summary>
        /// Schema configuration for extracting message keys from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Output("keySchema")]
        public Output<Outputs.FeatureEngineeringKafkaConfigKeySchema?> KeySchema { get; private set; } = null!;

        /// <summary>
        /// (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
        /// Can be distinct from topic name
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// Configure the provider for management through account provider.
        /// </summary>
        [Output("providerConfig")]
        public Output<Outputs.FeatureEngineeringKafkaConfigProviderConfig?> ProviderConfig { get; private set; } = null!;

        /// <summary>
        /// Options to configure which Kafka topics to pull data from
        /// </summary>
        [Output("subscriptionMode")]
        public Output<Outputs.FeatureEngineeringKafkaConfigSubscriptionMode> SubscriptionMode { get; private set; } = null!;

        /// <summary>
        /// Schema configuration for extracting message values from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Output("valueSchema")]
        public Output<Outputs.FeatureEngineeringKafkaConfigValueSchema?> ValueSchema { get; private set; } = null!;


        /// <summary>
        /// Create a FeatureEngineeringKafkaConfig resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public FeatureEngineeringKafkaConfig(string name, FeatureEngineeringKafkaConfigArgs args, CustomResourceOptions? options = null)
            : base("databricks:index/featureEngineeringKafkaConfig:FeatureEngineeringKafkaConfig", name, args ?? new FeatureEngineeringKafkaConfigArgs(), MakeResourceOptions(options, ""))
        {
        }

        private FeatureEngineeringKafkaConfig(string name, Input<string> id, FeatureEngineeringKafkaConfigState? state = null, CustomResourceOptions? options = null)
            : base("databricks:index/featureEngineeringKafkaConfig:FeatureEngineeringKafkaConfig", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing FeatureEngineeringKafkaConfig resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static FeatureEngineeringKafkaConfig Get(string name, Input<string> id, FeatureEngineeringKafkaConfigState? state = null, CustomResourceOptions? options = null)
        {
            return new FeatureEngineeringKafkaConfig(name, id, state, options);
        }
    }

    public sealed class FeatureEngineeringKafkaConfigArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Authentication configuration for connection to topics
        /// </summary>
        [Input("authConfig", required: true)]
        public Input<Inputs.FeatureEngineeringKafkaConfigAuthConfigArgs> AuthConfig { get; set; } = null!;

        /// <summary>
        /// A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
        /// In the future, a separate table will be maintained by Databricks for forward filling data.
        /// The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
        /// </summary>
        [Input("backfillSource")]
        public Input<Inputs.FeatureEngineeringKafkaConfigBackfillSourceArgs>? BackfillSource { get; set; }

        /// <summary>
        /// A comma-separated list of host/port pairs pointing to Kafka cluster
        /// </summary>
        [Input("bootstrapServers", required: true)]
        public Input<string> BootstrapServers { get; set; } = null!;

        [Input("extraOptions")]
        private InputMap<string>? _extraOptions;

        /// <summary>
        /// Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
        /// </summary>
        public InputMap<string> ExtraOptions
        {
            get => _extraOptions ?? (_extraOptions = new InputMap<string>());
            set => _extraOptions = value;
        }

        /// <summary>
        /// Schema configuration for extracting message keys from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Input("keySchema")]
        public Input<Inputs.FeatureEngineeringKafkaConfigKeySchemaArgs>? KeySchema { get; set; }

        /// <summary>
        /// Configure the provider for management through account provider.
        /// </summary>
        [Input("providerConfig")]
        public Input<Inputs.FeatureEngineeringKafkaConfigProviderConfigArgs>? ProviderConfig { get; set; }

        /// <summary>
        /// Options to configure which Kafka topics to pull data from
        /// </summary>
        [Input("subscriptionMode", required: true)]
        public Input<Inputs.FeatureEngineeringKafkaConfigSubscriptionModeArgs> SubscriptionMode { get; set; } = null!;

        /// <summary>
        /// Schema configuration for extracting message values from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Input("valueSchema")]
        public Input<Inputs.FeatureEngineeringKafkaConfigValueSchemaArgs>? ValueSchema { get; set; }

        public FeatureEngineeringKafkaConfigArgs()
        {
        }
        public static new FeatureEngineeringKafkaConfigArgs Empty => new FeatureEngineeringKafkaConfigArgs();
    }

    public sealed class FeatureEngineeringKafkaConfigState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Authentication configuration for connection to topics
        /// </summary>
        [Input("authConfig")]
        public Input<Inputs.FeatureEngineeringKafkaConfigAuthConfigGetArgs>? AuthConfig { get; set; }

        /// <summary>
        /// A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
        /// In the future, a separate table will be maintained by Databricks for forward filling data.
        /// The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
        /// </summary>
        [Input("backfillSource")]
        public Input<Inputs.FeatureEngineeringKafkaConfigBackfillSourceGetArgs>? BackfillSource { get; set; }

        /// <summary>
        /// A comma-separated list of host/port pairs pointing to Kafka cluster
        /// </summary>
        [Input("bootstrapServers")]
        public Input<string>? BootstrapServers { get; set; }

        [Input("extraOptions")]
        private InputMap<string>? _extraOptions;

        /// <summary>
        /// Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
        /// </summary>
        public InputMap<string> ExtraOptions
        {
            get => _extraOptions ?? (_extraOptions = new InputMap<string>());
            set => _extraOptions = value;
        }

        /// <summary>
        /// Schema configuration for extracting message keys from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Input("keySchema")]
        public Input<Inputs.FeatureEngineeringKafkaConfigKeySchemaGetArgs>? KeySchema { get; set; }

        /// <summary>
        /// (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
        /// Can be distinct from topic name
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// Configure the provider for management through account provider.
        /// </summary>
        [Input("providerConfig")]
        public Input<Inputs.FeatureEngineeringKafkaConfigProviderConfigGetArgs>? ProviderConfig { get; set; }

        /// <summary>
        /// Options to configure which Kafka topics to pull data from
        /// </summary>
        [Input("subscriptionMode")]
        public Input<Inputs.FeatureEngineeringKafkaConfigSubscriptionModeGetArgs>? SubscriptionMode { get; set; }

        /// <summary>
        /// Schema configuration for extracting message values from topics. At least one of KeySchema and ValueSchema must be provided
        /// </summary>
        [Input("valueSchema")]
        public Input<Inputs.FeatureEngineeringKafkaConfigValueSchemaGetArgs>? ValueSchema { get; set; }

        public FeatureEngineeringKafkaConfigState()
        {
        }
        public static new FeatureEngineeringKafkaConfigState Empty => new FeatureEngineeringKafkaConfigState();
    }
}
