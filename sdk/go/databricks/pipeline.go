// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package databricks

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi-databricks/sdk/go/databricks/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Use `Pipeline` to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
//
// > This resource can only be used with a workspace-level provider!
//
// ## Example Usage
//
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			ldpDemo, err := databricks.NewNotebook(ctx, "ldp_demo", nil)
//			if err != nil {
//				return err
//			}
//			ldpDemoRepo, err := databricks.NewRepo(ctx, "ldp_demo", nil)
//			if err != nil {
//				return err
//			}
//			_, err = databricks.NewPipeline(ctx, "this", &databricks.PipelineArgs{
//				Name:    pulumi.String("Pipeline Name"),
//				Storage: pulumi.String("/test/first-pipeline"),
//				Configuration: pulumi.StringMap{
//					"key1": pulumi.String("value1"),
//					"key2": pulumi.String("value2"),
//				},
//				Clusters: databricks.PipelineClusterArray{
//					&databricks.PipelineClusterArgs{
//						Label:      pulumi.String("default"),
//						NumWorkers: pulumi.Int(2),
//						CustomTags: pulumi.StringMap{
//							"cluster_type": pulumi.String("default"),
//						},
//					},
//					&databricks.PipelineClusterArgs{
//						Label:      pulumi.String("maintenance"),
//						NumWorkers: pulumi.Int(1),
//						CustomTags: pulumi.StringMap{
//							"cluster_type": pulumi.String("maintenance"),
//						},
//					},
//				},
//				Libraries: databricks.PipelineLibraryArray{
//					&databricks.PipelineLibraryArgs{
//						Notebook: &databricks.PipelineLibraryNotebookArgs{
//							Path: ldpDemo.ID(),
//						},
//					},
//					&databricks.PipelineLibraryArgs{
//						File: &databricks.PipelineLibraryFileArgs{
//							Path: ldpDemoRepo.Path.ApplyT(func(path string) (string, error) {
//								return fmt.Sprintf("%v/pipeline.sql", path), nil
//							}).(pulumi.StringOutput),
//						},
//					},
//					&databricks.PipelineLibraryArgs{
//						Glob: &databricks.PipelineLibraryGlobArgs{
//							Include: ldpDemoRepo.Path.ApplyT(func(path string) (string, error) {
//								return fmt.Sprintf("%v/subfolder/**", path), nil
//							}).(pulumi.StringOutput),
//						},
//					},
//				},
//				Continuous: pulumi.Bool(false),
//				Notifications: databricks.PipelineNotificationArray{
//					&databricks.PipelineNotificationArgs{
//						EmailRecipients: pulumi.StringArray{
//							pulumi.String("user@domain.com"),
//							pulumi.String("user1@domain.com"),
//						},
//						Alerts: pulumi.StringArray{
//							pulumi.String("on-update-failure"),
//							pulumi.String("on-update-fatal-failure"),
//							pulumi.String("on-update-success"),
//							pulumi.String("on-flow-failure"),
//						},
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Related Resources
//
// The following resources are often used in the same context:
//
// * End to end workspace management guide.
// * getPipelines to retrieve [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt) data.
// * Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
// * Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
// * Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
//
// ## Import
//
// # The resource job can be imported using the id of the pipeline
//
// hcl
//
// import {
//
//	to = databricks_pipeline.this
//
//	id = "<pipeline-id>"
//
// }
//
// Alternatively, when using `terraform` version 1.4 or earlier, import using the `pulumi import` command:
//
// bash
//
// ```sh
// $ pulumi import databricks:index/pipeline:Pipeline this <pipeline-id>
// ```
type Pipeline struct {
	pulumi.CustomResourceState

	// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
	AllowDuplicateNames pulumi.BoolPtrOutput `pulumi:"allowDuplicateNames"`
	// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
	BudgetPolicyId pulumi.StringPtrOutput `pulumi:"budgetPolicyId"`
	// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
	Catalog pulumi.StringPtrOutput `pulumi:"catalog"`
	Cause   pulumi.StringOutput    `pulumi:"cause"`
	// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
	Channel   pulumi.StringPtrOutput `pulumi:"channel"`
	ClusterId pulumi.StringOutput    `pulumi:"clusterId"`
	// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
	Clusters PipelineClusterArrayOutput `pulumi:"clusters"`
	// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
	Configuration pulumi.StringMapOutput `pulumi:"configuration"`
	// A flag indicating whether to run the pipeline continuously. The default value is `false`.
	Continuous      pulumi.BoolPtrOutput `pulumi:"continuous"`
	CreatorUserName pulumi.StringOutput  `pulumi:"creatorUserName"`
	// Deployment type of this pipeline. Supports following attributes:
	Deployment PipelineDeploymentPtrOutput `pulumi:"deployment"`
	// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
	Development pulumi.BoolPtrOutput `pulumi:"development"`
	// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
	Edition     pulumi.StringPtrOutput       `pulumi:"edition"`
	Environment PipelineEnvironmentPtrOutput `pulumi:"environment"`
	// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
	EventLog             PipelineEventLogPtrOutput `pulumi:"eventLog"`
	ExpectedLastModified pulumi.IntPtrOutput       `pulumi:"expectedLastModified"`
	// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
	Filters PipelineFiltersPtrOutput `pulumi:"filters"`
	// The definition of a gateway pipeline to support CDC. Consists of following attributes:
	GatewayDefinition   PipelineGatewayDefinitionPtrOutput   `pulumi:"gatewayDefinition"`
	Health              pulumi.StringOutput                  `pulumi:"health"`
	IngestionDefinition PipelineIngestionDefinitionPtrOutput `pulumi:"ingestionDefinition"`
	LastModified        pulumi.IntOutput                     `pulumi:"lastModified"`
	LatestUpdates       PipelineLatestUpdateArrayOutput      `pulumi:"latestUpdates"`
	// blocks - Specifies pipeline code.
	Libraries PipelineLibraryArrayOutput `pulumi:"libraries"`
	// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
	Name          pulumi.StringOutput             `pulumi:"name"`
	Notifications PipelineNotificationArrayOutput `pulumi:"notifications"`
	// A flag indicating whether to use Photon engine. The default value is `false`.
	Photon        pulumi.BoolPtrOutput           `pulumi:"photon"`
	RestartWindow PipelineRestartWindowPtrOutput `pulumi:"restartWindow"`
	// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
	RootPath      pulumi.StringPtrOutput `pulumi:"rootPath"`
	RunAs         PipelineRunAsOutput    `pulumi:"runAs"`
	RunAsUserName pulumi.StringOutput    `pulumi:"runAsUserName"`
	// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
	Schema pulumi.StringPtrOutput `pulumi:"schema"`
	// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
	Serverless pulumi.BoolPtrOutput `pulumi:"serverless"`
	State      pulumi.StringOutput  `pulumi:"state"`
	// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
	Storage pulumi.StringPtrOutput `pulumi:"storage"`
	// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
	Tags pulumi.StringMapOutput `pulumi:"tags"`
	// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
	Target  pulumi.StringPtrOutput   `pulumi:"target"`
	Trigger PipelineTriggerPtrOutput `pulumi:"trigger"`
	// URL of the Lakeflow Declarative Pipeline on the given workspace.
	Url pulumi.StringOutput `pulumi:"url"`
}

// NewPipeline registers a new resource with the given unique name, arguments, and options.
func NewPipeline(ctx *pulumi.Context,
	name string, args *PipelineArgs, opts ...pulumi.ResourceOption) (*Pipeline, error) {
	if args == nil {
		args = &PipelineArgs{}
	}

	opts = internal.PkgResourceDefaultOpts(opts)
	var resource Pipeline
	err := ctx.RegisterResource("databricks:index/pipeline:Pipeline", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetPipeline gets an existing Pipeline resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetPipeline(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *PipelineState, opts ...pulumi.ResourceOption) (*Pipeline, error) {
	var resource Pipeline
	err := ctx.ReadResource("databricks:index/pipeline:Pipeline", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Pipeline resources.
type pipelineState struct {
	// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
	AllowDuplicateNames *bool `pulumi:"allowDuplicateNames"`
	// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
	BudgetPolicyId *string `pulumi:"budgetPolicyId"`
	// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
	Catalog *string `pulumi:"catalog"`
	Cause   *string `pulumi:"cause"`
	// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
	Channel   *string `pulumi:"channel"`
	ClusterId *string `pulumi:"clusterId"`
	// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
	Clusters []PipelineCluster `pulumi:"clusters"`
	// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
	Configuration map[string]string `pulumi:"configuration"`
	// A flag indicating whether to run the pipeline continuously. The default value is `false`.
	Continuous      *bool   `pulumi:"continuous"`
	CreatorUserName *string `pulumi:"creatorUserName"`
	// Deployment type of this pipeline. Supports following attributes:
	Deployment *PipelineDeployment `pulumi:"deployment"`
	// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
	Development *bool `pulumi:"development"`
	// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
	Edition     *string              `pulumi:"edition"`
	Environment *PipelineEnvironment `pulumi:"environment"`
	// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
	EventLog             *PipelineEventLog `pulumi:"eventLog"`
	ExpectedLastModified *int              `pulumi:"expectedLastModified"`
	// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
	Filters *PipelineFilters `pulumi:"filters"`
	// The definition of a gateway pipeline to support CDC. Consists of following attributes:
	GatewayDefinition   *PipelineGatewayDefinition   `pulumi:"gatewayDefinition"`
	Health              *string                      `pulumi:"health"`
	IngestionDefinition *PipelineIngestionDefinition `pulumi:"ingestionDefinition"`
	LastModified        *int                         `pulumi:"lastModified"`
	LatestUpdates       []PipelineLatestUpdate       `pulumi:"latestUpdates"`
	// blocks - Specifies pipeline code.
	Libraries []PipelineLibrary `pulumi:"libraries"`
	// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
	Name          *string                `pulumi:"name"`
	Notifications []PipelineNotification `pulumi:"notifications"`
	// A flag indicating whether to use Photon engine. The default value is `false`.
	Photon        *bool                  `pulumi:"photon"`
	RestartWindow *PipelineRestartWindow `pulumi:"restartWindow"`
	// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
	RootPath      *string        `pulumi:"rootPath"`
	RunAs         *PipelineRunAs `pulumi:"runAs"`
	RunAsUserName *string        `pulumi:"runAsUserName"`
	// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
	Schema *string `pulumi:"schema"`
	// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
	Serverless *bool   `pulumi:"serverless"`
	State      *string `pulumi:"state"`
	// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
	Storage *string `pulumi:"storage"`
	// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
	Tags map[string]string `pulumi:"tags"`
	// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
	Target  *string          `pulumi:"target"`
	Trigger *PipelineTrigger `pulumi:"trigger"`
	// URL of the Lakeflow Declarative Pipeline on the given workspace.
	Url *string `pulumi:"url"`
}

type PipelineState struct {
	// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
	AllowDuplicateNames pulumi.BoolPtrInput
	// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
	BudgetPolicyId pulumi.StringPtrInput
	// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
	Catalog pulumi.StringPtrInput
	Cause   pulumi.StringPtrInput
	// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
	Channel   pulumi.StringPtrInput
	ClusterId pulumi.StringPtrInput
	// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
	Clusters PipelineClusterArrayInput
	// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
	Configuration pulumi.StringMapInput
	// A flag indicating whether to run the pipeline continuously. The default value is `false`.
	Continuous      pulumi.BoolPtrInput
	CreatorUserName pulumi.StringPtrInput
	// Deployment type of this pipeline. Supports following attributes:
	Deployment PipelineDeploymentPtrInput
	// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
	Development pulumi.BoolPtrInput
	// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
	Edition     pulumi.StringPtrInput
	Environment PipelineEnvironmentPtrInput
	// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
	EventLog             PipelineEventLogPtrInput
	ExpectedLastModified pulumi.IntPtrInput
	// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
	Filters PipelineFiltersPtrInput
	// The definition of a gateway pipeline to support CDC. Consists of following attributes:
	GatewayDefinition   PipelineGatewayDefinitionPtrInput
	Health              pulumi.StringPtrInput
	IngestionDefinition PipelineIngestionDefinitionPtrInput
	LastModified        pulumi.IntPtrInput
	LatestUpdates       PipelineLatestUpdateArrayInput
	// blocks - Specifies pipeline code.
	Libraries PipelineLibraryArrayInput
	// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
	Name          pulumi.StringPtrInput
	Notifications PipelineNotificationArrayInput
	// A flag indicating whether to use Photon engine. The default value is `false`.
	Photon        pulumi.BoolPtrInput
	RestartWindow PipelineRestartWindowPtrInput
	// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
	RootPath      pulumi.StringPtrInput
	RunAs         PipelineRunAsPtrInput
	RunAsUserName pulumi.StringPtrInput
	// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
	Schema pulumi.StringPtrInput
	// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
	Serverless pulumi.BoolPtrInput
	State      pulumi.StringPtrInput
	// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
	Storage pulumi.StringPtrInput
	// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
	Tags pulumi.StringMapInput
	// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
	Target  pulumi.StringPtrInput
	Trigger PipelineTriggerPtrInput
	// URL of the Lakeflow Declarative Pipeline on the given workspace.
	Url pulumi.StringPtrInput
}

func (PipelineState) ElementType() reflect.Type {
	return reflect.TypeOf((*pipelineState)(nil)).Elem()
}

type pipelineArgs struct {
	// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
	AllowDuplicateNames *bool `pulumi:"allowDuplicateNames"`
	// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
	BudgetPolicyId *string `pulumi:"budgetPolicyId"`
	// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
	Catalog *string `pulumi:"catalog"`
	Cause   *string `pulumi:"cause"`
	// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
	Channel   *string `pulumi:"channel"`
	ClusterId *string `pulumi:"clusterId"`
	// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
	Clusters []PipelineCluster `pulumi:"clusters"`
	// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
	Configuration map[string]string `pulumi:"configuration"`
	// A flag indicating whether to run the pipeline continuously. The default value is `false`.
	Continuous      *bool   `pulumi:"continuous"`
	CreatorUserName *string `pulumi:"creatorUserName"`
	// Deployment type of this pipeline. Supports following attributes:
	Deployment *PipelineDeployment `pulumi:"deployment"`
	// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
	Development *bool `pulumi:"development"`
	// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
	Edition     *string              `pulumi:"edition"`
	Environment *PipelineEnvironment `pulumi:"environment"`
	// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
	EventLog             *PipelineEventLog `pulumi:"eventLog"`
	ExpectedLastModified *int              `pulumi:"expectedLastModified"`
	// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
	Filters *PipelineFilters `pulumi:"filters"`
	// The definition of a gateway pipeline to support CDC. Consists of following attributes:
	GatewayDefinition   *PipelineGatewayDefinition   `pulumi:"gatewayDefinition"`
	Health              *string                      `pulumi:"health"`
	IngestionDefinition *PipelineIngestionDefinition `pulumi:"ingestionDefinition"`
	LastModified        *int                         `pulumi:"lastModified"`
	LatestUpdates       []PipelineLatestUpdate       `pulumi:"latestUpdates"`
	// blocks - Specifies pipeline code.
	Libraries []PipelineLibrary `pulumi:"libraries"`
	// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
	Name          *string                `pulumi:"name"`
	Notifications []PipelineNotification `pulumi:"notifications"`
	// A flag indicating whether to use Photon engine. The default value is `false`.
	Photon        *bool                  `pulumi:"photon"`
	RestartWindow *PipelineRestartWindow `pulumi:"restartWindow"`
	// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
	RootPath      *string        `pulumi:"rootPath"`
	RunAs         *PipelineRunAs `pulumi:"runAs"`
	RunAsUserName *string        `pulumi:"runAsUserName"`
	// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
	Schema *string `pulumi:"schema"`
	// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
	Serverless *bool   `pulumi:"serverless"`
	State      *string `pulumi:"state"`
	// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
	Storage *string `pulumi:"storage"`
	// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
	Tags map[string]string `pulumi:"tags"`
	// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
	Target  *string          `pulumi:"target"`
	Trigger *PipelineTrigger `pulumi:"trigger"`
	// URL of the Lakeflow Declarative Pipeline on the given workspace.
	Url *string `pulumi:"url"`
}

// The set of arguments for constructing a Pipeline resource.
type PipelineArgs struct {
	// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
	AllowDuplicateNames pulumi.BoolPtrInput
	// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
	BudgetPolicyId pulumi.StringPtrInput
	// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
	Catalog pulumi.StringPtrInput
	Cause   pulumi.StringPtrInput
	// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
	Channel   pulumi.StringPtrInput
	ClusterId pulumi.StringPtrInput
	// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
	Clusters PipelineClusterArrayInput
	// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
	Configuration pulumi.StringMapInput
	// A flag indicating whether to run the pipeline continuously. The default value is `false`.
	Continuous      pulumi.BoolPtrInput
	CreatorUserName pulumi.StringPtrInput
	// Deployment type of this pipeline. Supports following attributes:
	Deployment PipelineDeploymentPtrInput
	// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
	Development pulumi.BoolPtrInput
	// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
	Edition     pulumi.StringPtrInput
	Environment PipelineEnvironmentPtrInput
	// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
	EventLog             PipelineEventLogPtrInput
	ExpectedLastModified pulumi.IntPtrInput
	// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
	Filters PipelineFiltersPtrInput
	// The definition of a gateway pipeline to support CDC. Consists of following attributes:
	GatewayDefinition   PipelineGatewayDefinitionPtrInput
	Health              pulumi.StringPtrInput
	IngestionDefinition PipelineIngestionDefinitionPtrInput
	LastModified        pulumi.IntPtrInput
	LatestUpdates       PipelineLatestUpdateArrayInput
	// blocks - Specifies pipeline code.
	Libraries PipelineLibraryArrayInput
	// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
	Name          pulumi.StringPtrInput
	Notifications PipelineNotificationArrayInput
	// A flag indicating whether to use Photon engine. The default value is `false`.
	Photon        pulumi.BoolPtrInput
	RestartWindow PipelineRestartWindowPtrInput
	// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
	RootPath      pulumi.StringPtrInput
	RunAs         PipelineRunAsPtrInput
	RunAsUserName pulumi.StringPtrInput
	// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
	Schema pulumi.StringPtrInput
	// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
	Serverless pulumi.BoolPtrInput
	State      pulumi.StringPtrInput
	// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
	Storage pulumi.StringPtrInput
	// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
	Tags pulumi.StringMapInput
	// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
	Target  pulumi.StringPtrInput
	Trigger PipelineTriggerPtrInput
	// URL of the Lakeflow Declarative Pipeline on the given workspace.
	Url pulumi.StringPtrInput
}

func (PipelineArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*pipelineArgs)(nil)).Elem()
}

type PipelineInput interface {
	pulumi.Input

	ToPipelineOutput() PipelineOutput
	ToPipelineOutputWithContext(ctx context.Context) PipelineOutput
}

func (*Pipeline) ElementType() reflect.Type {
	return reflect.TypeOf((**Pipeline)(nil)).Elem()
}

func (i *Pipeline) ToPipelineOutput() PipelineOutput {
	return i.ToPipelineOutputWithContext(context.Background())
}

func (i *Pipeline) ToPipelineOutputWithContext(ctx context.Context) PipelineOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PipelineOutput)
}

// PipelineArrayInput is an input type that accepts PipelineArray and PipelineArrayOutput values.
// You can construct a concrete instance of `PipelineArrayInput` via:
//
//	PipelineArray{ PipelineArgs{...} }
type PipelineArrayInput interface {
	pulumi.Input

	ToPipelineArrayOutput() PipelineArrayOutput
	ToPipelineArrayOutputWithContext(context.Context) PipelineArrayOutput
}

type PipelineArray []PipelineInput

func (PipelineArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Pipeline)(nil)).Elem()
}

func (i PipelineArray) ToPipelineArrayOutput() PipelineArrayOutput {
	return i.ToPipelineArrayOutputWithContext(context.Background())
}

func (i PipelineArray) ToPipelineArrayOutputWithContext(ctx context.Context) PipelineArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PipelineArrayOutput)
}

// PipelineMapInput is an input type that accepts PipelineMap and PipelineMapOutput values.
// You can construct a concrete instance of `PipelineMapInput` via:
//
//	PipelineMap{ "key": PipelineArgs{...} }
type PipelineMapInput interface {
	pulumi.Input

	ToPipelineMapOutput() PipelineMapOutput
	ToPipelineMapOutputWithContext(context.Context) PipelineMapOutput
}

type PipelineMap map[string]PipelineInput

func (PipelineMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Pipeline)(nil)).Elem()
}

func (i PipelineMap) ToPipelineMapOutput() PipelineMapOutput {
	return i.ToPipelineMapOutputWithContext(context.Background())
}

func (i PipelineMap) ToPipelineMapOutputWithContext(ctx context.Context) PipelineMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PipelineMapOutput)
}

type PipelineOutput struct{ *pulumi.OutputState }

func (PipelineOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Pipeline)(nil)).Elem()
}

func (o PipelineOutput) ToPipelineOutput() PipelineOutput {
	return o
}

func (o PipelineOutput) ToPipelineOutputWithContext(ctx context.Context) PipelineOutput {
	return o
}

// Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
func (o PipelineOutput) AllowDuplicateNames() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.BoolPtrOutput { return v.AllowDuplicateNames }).(pulumi.BoolPtrOutput)
}

// optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.
func (o PipelineOutput) BudgetPolicyId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.BudgetPolicyId }).(pulumi.StringPtrOutput)
}

// The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
func (o PipelineOutput) Catalog() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Catalog }).(pulumi.StringPtrOutput)
}

func (o PipelineOutput) Cause() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.Cause }).(pulumi.StringOutput)
}

// optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
func (o PipelineOutput) Channel() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Channel }).(pulumi.StringPtrOutput)
}

func (o PipelineOutput) ClusterId() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.ClusterId }).(pulumi.StringOutput)
}

// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/api/workspace/pipelines/create#clusters).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
func (o PipelineOutput) Clusters() PipelineClusterArrayOutput {
	return o.ApplyT(func(v *Pipeline) PipelineClusterArrayOutput { return v.Clusters }).(PipelineClusterArrayOutput)
}

// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
func (o PipelineOutput) Configuration() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringMapOutput { return v.Configuration }).(pulumi.StringMapOutput)
}

// A flag indicating whether to run the pipeline continuously. The default value is `false`.
func (o PipelineOutput) Continuous() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.BoolPtrOutput { return v.Continuous }).(pulumi.BoolPtrOutput)
}

func (o PipelineOutput) CreatorUserName() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.CreatorUserName }).(pulumi.StringOutput)
}

// Deployment type of this pipeline. Supports following attributes:
func (o PipelineOutput) Deployment() PipelineDeploymentPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineDeploymentPtrOutput { return v.Deployment }).(PipelineDeploymentPtrOutput)
}

// A flag indicating whether to run the pipeline in development mode. The default value is `false`.
func (o PipelineOutput) Development() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.BoolPtrOutput { return v.Development }).(pulumi.BoolPtrOutput)
}

// optional name of the [product edition](https://docs.databricks.com/aws/en/dlt/configure-pipeline#choose-a-product-edition). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
func (o PipelineOutput) Edition() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Edition }).(pulumi.StringPtrOutput)
}

func (o PipelineOutput) Environment() PipelineEnvironmentPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineEnvironmentPtrOutput { return v.Environment }).(PipelineEnvironmentPtrOutput)
}

// an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:
func (o PipelineOutput) EventLog() PipelineEventLogPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineEventLogPtrOutput { return v.EventLog }).(PipelineEventLogPtrOutput)
}

func (o PipelineOutput) ExpectedLastModified() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.IntPtrOutput { return v.ExpectedLastModified }).(pulumi.IntPtrOutput)
}

// Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
func (o PipelineOutput) Filters() PipelineFiltersPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineFiltersPtrOutput { return v.Filters }).(PipelineFiltersPtrOutput)
}

// The definition of a gateway pipeline to support CDC. Consists of following attributes:
func (o PipelineOutput) GatewayDefinition() PipelineGatewayDefinitionPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineGatewayDefinitionPtrOutput { return v.GatewayDefinition }).(PipelineGatewayDefinitionPtrOutput)
}

func (o PipelineOutput) Health() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.Health }).(pulumi.StringOutput)
}

func (o PipelineOutput) IngestionDefinition() PipelineIngestionDefinitionPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineIngestionDefinitionPtrOutput { return v.IngestionDefinition }).(PipelineIngestionDefinitionPtrOutput)
}

func (o PipelineOutput) LastModified() pulumi.IntOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.IntOutput { return v.LastModified }).(pulumi.IntOutput)
}

func (o PipelineOutput) LatestUpdates() PipelineLatestUpdateArrayOutput {
	return o.ApplyT(func(v *Pipeline) PipelineLatestUpdateArrayOutput { return v.LatestUpdates }).(PipelineLatestUpdateArrayOutput)
}

// blocks - Specifies pipeline code.
func (o PipelineOutput) Libraries() PipelineLibraryArrayOutput {
	return o.ApplyT(func(v *Pipeline) PipelineLibraryArrayOutput { return v.Libraries }).(PipelineLibraryArrayOutput)
}

// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
func (o PipelineOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

func (o PipelineOutput) Notifications() PipelineNotificationArrayOutput {
	return o.ApplyT(func(v *Pipeline) PipelineNotificationArrayOutput { return v.Notifications }).(PipelineNotificationArrayOutput)
}

// A flag indicating whether to use Photon engine. The default value is `false`.
func (o PipelineOutput) Photon() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.BoolPtrOutput { return v.Photon }).(pulumi.BoolPtrOutput)
}

func (o PipelineOutput) RestartWindow() PipelineRestartWindowPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineRestartWindowPtrOutput { return v.RestartWindow }).(PipelineRestartWindowPtrOutput)
}

// An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to `sys.path` when executing Python sources during pipeline execution.
func (o PipelineOutput) RootPath() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.RootPath }).(pulumi.StringPtrOutput)
}

func (o PipelineOutput) RunAs() PipelineRunAsOutput {
	return o.ApplyT(func(v *Pipeline) PipelineRunAsOutput { return v.RunAs }).(PipelineRunAsOutput)
}

func (o PipelineOutput) RunAsUserName() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.RunAsUserName }).(pulumi.StringOutput)
}

// The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
func (o PipelineOutput) Schema() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Schema }).(pulumi.StringPtrOutput)
}

// An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
func (o PipelineOutput) Serverless() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.BoolPtrOutput { return v.Serverless }).(pulumi.BoolPtrOutput)
}

func (o PipelineOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.State }).(pulumi.StringOutput)
}

// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
func (o PipelineOutput) Storage() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Storage }).(pulumi.StringPtrOutput)
}

// A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.
func (o PipelineOutput) Tags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringMapOutput { return v.Tags }).(pulumi.StringMapOutput)
}

// The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
func (o PipelineOutput) Target() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringPtrOutput { return v.Target }).(pulumi.StringPtrOutput)
}

func (o PipelineOutput) Trigger() PipelineTriggerPtrOutput {
	return o.ApplyT(func(v *Pipeline) PipelineTriggerPtrOutput { return v.Trigger }).(PipelineTriggerPtrOutput)
}

// URL of the Lakeflow Declarative Pipeline on the given workspace.
func (o PipelineOutput) Url() pulumi.StringOutput {
	return o.ApplyT(func(v *Pipeline) pulumi.StringOutput { return v.Url }).(pulumi.StringOutput)
}

type PipelineArrayOutput struct{ *pulumi.OutputState }

func (PipelineArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Pipeline)(nil)).Elem()
}

func (o PipelineArrayOutput) ToPipelineArrayOutput() PipelineArrayOutput {
	return o
}

func (o PipelineArrayOutput) ToPipelineArrayOutputWithContext(ctx context.Context) PipelineArrayOutput {
	return o
}

func (o PipelineArrayOutput) Index(i pulumi.IntInput) PipelineOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *Pipeline {
		return vs[0].([]*Pipeline)[vs[1].(int)]
	}).(PipelineOutput)
}

type PipelineMapOutput struct{ *pulumi.OutputState }

func (PipelineMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Pipeline)(nil)).Elem()
}

func (o PipelineMapOutput) ToPipelineMapOutput() PipelineMapOutput {
	return o
}

func (o PipelineMapOutput) ToPipelineMapOutputWithContext(ctx context.Context) PipelineMapOutput {
	return o
}

func (o PipelineMapOutput) MapIndex(k pulumi.StringInput) PipelineOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *Pipeline {
		return vs[0].(map[string]*Pipeline)[vs[1].(string)]
	}).(PipelineOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*PipelineInput)(nil)).Elem(), &Pipeline{})
	pulumi.RegisterInputType(reflect.TypeOf((*PipelineArrayInput)(nil)).Elem(), PipelineArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*PipelineMapInput)(nil)).Elem(), PipelineMap{})
	pulumi.RegisterOutputType(PipelineOutput{})
	pulumi.RegisterOutputType(PipelineArrayOutput{})
	pulumi.RegisterOutputType(PipelineMapOutput{})
}
