// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package databricks

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-databricks/sdk/go/databricks/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// This resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks.
//
// A `LakehouseMonitor` is attached to a SqlTable and can be of type timeseries, snapshot or inference.
//
// ### Inference Monitor
//
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := databricks.NewLakehouseMonitor(ctx, "testMonitorInference", &databricks.LakehouseMonitorArgs{
//				TableName:        pulumi.String(fmt.Sprintf("%v.%v.%v", sandbox.Name, things.Name, myTestTable.Name)),
//				AssetsDir:        pulumi.String(fmt.Sprintf("/Shared/provider-test/databricks_lakehouse_monitoring/%v", myTestTable.Name)),
//				OutputSchemaName: pulumi.String(fmt.Sprintf("%v.%v", sandbox.Name, things.Name)),
//				InferenceLog: &databricks.LakehouseMonitorInferenceLogArgs{
//					Granularities: pulumi.StringArray{
//						pulumi.String("1 hour"),
//					},
//					TimestampCol:  pulumi.String("timestamp"),
//					PredictionCol: pulumi.String("prediction"),
//					ModelIdCol:    pulumi.String("model_id"),
//					ProblemType:   pulumi.String("PROBLEM_TYPE_REGRESSION"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Snapshot Monitor
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := databricks.NewLakehouseMonitor(ctx, "testMonitorInference", &databricks.LakehouseMonitorArgs{
//				TableName:        pulumi.String(fmt.Sprintf("%v.%v.%v", sandbox.Name, things.Name, myTestTable.Name)),
//				AssetsDir:        pulumi.String(fmt.Sprintf("/Shared/provider-test/databricks_lakehouse_monitoring/%v", myTestTable.Name)),
//				OutputSchemaName: pulumi.String(fmt.Sprintf("%v.%v", sandbox.Name, things.Name)),
//				Snapshot:         nil,
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Related Resources
//
// The following resources are often used in the same context:
//
// * Catalog
// * Schema
// * SqlTable
type LakehouseMonitor struct {
	pulumi.CustomResourceState

	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringOutput `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrOutput `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics LakehouseMonitorCustomMetricArrayOutput `pulumi:"customMetrics"`
	// The ID of the generated dashboard.
	DashboardId pulumi.StringOutput `pulumi:"dashboardId"`
	// The data classification config for the monitor
	DataClassificationConfig LakehouseMonitorDataClassificationConfigPtrOutput `pulumi:"dataClassificationConfig"`
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName pulumi.StringOutput `pulumi:"driftMetricsTableName"`
	// Configuration for the inference log monitor
	InferenceLog            LakehouseMonitorInferenceLogPtrOutput `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg pulumi.StringPtrOutput                `pulumi:"latestMonitorFailureMsg"`
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion pulumi.StringOutput `pulumi:"monitorVersion"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications LakehouseMonitorNotificationsPtrOutput `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringOutput `pulumi:"outputSchemaName"`
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName pulumi.StringOutput `pulumi:"profileMetricsTableName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule LakehouseMonitorSchedulePtrOutput `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrOutput `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayOutput `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot LakehouseMonitorSnapshotPtrOutput `pulumi:"snapshot"`
	// Status of the Monitor
	Status pulumi.StringOutput `pulumi:"status"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringOutput `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries LakehouseMonitorTimeSeriesPtrOutput `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrOutput `pulumi:"warehouseId"`
}

// NewLakehouseMonitor registers a new resource with the given unique name, arguments, and options.
func NewLakehouseMonitor(ctx *pulumi.Context,
	name string, args *LakehouseMonitorArgs, opts ...pulumi.ResourceOption) (*LakehouseMonitor, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.AssetsDir == nil {
		return nil, errors.New("invalid value for required argument 'AssetsDir'")
	}
	if args.OutputSchemaName == nil {
		return nil, errors.New("invalid value for required argument 'OutputSchemaName'")
	}
	if args.TableName == nil {
		return nil, errors.New("invalid value for required argument 'TableName'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource LakehouseMonitor
	err := ctx.RegisterResource("databricks:index/lakehouseMonitor:LakehouseMonitor", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetLakehouseMonitor gets an existing LakehouseMonitor resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetLakehouseMonitor(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *LakehouseMonitorState, opts ...pulumi.ResourceOption) (*LakehouseMonitor, error) {
	var resource LakehouseMonitor
	err := ctx.ReadResource("databricks:index/lakehouseMonitor:LakehouseMonitor", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering LakehouseMonitor resources.
type lakehouseMonitorState struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir *string `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName *string `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics []LakehouseMonitorCustomMetric `pulumi:"customMetrics"`
	// The ID of the generated dashboard.
	DashboardId *string `pulumi:"dashboardId"`
	// The data classification config for the monitor
	DataClassificationConfig *LakehouseMonitorDataClassificationConfig `pulumi:"dataClassificationConfig"`
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName *string `pulumi:"driftMetricsTableName"`
	// Configuration for the inference log monitor
	InferenceLog            *LakehouseMonitorInferenceLog `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg *string                       `pulumi:"latestMonitorFailureMsg"`
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion *string `pulumi:"monitorVersion"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications *LakehouseMonitorNotifications `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName *string `pulumi:"outputSchemaName"`
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName *string `pulumi:"profileMetricsTableName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule *LakehouseMonitorSchedule `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard *bool `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs []string `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot *LakehouseMonitorSnapshot `pulumi:"snapshot"`
	// Status of the Monitor
	Status *string `pulumi:"status"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName *string `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries *LakehouseMonitorTimeSeries `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId *string `pulumi:"warehouseId"`
}

type LakehouseMonitorState struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringPtrInput
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrInput
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics LakehouseMonitorCustomMetricArrayInput
	// The ID of the generated dashboard.
	DashboardId pulumi.StringPtrInput
	// The data classification config for the monitor
	DataClassificationConfig LakehouseMonitorDataClassificationConfigPtrInput
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName pulumi.StringPtrInput
	// Configuration for the inference log monitor
	InferenceLog            LakehouseMonitorInferenceLogPtrInput
	LatestMonitorFailureMsg pulumi.StringPtrInput
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion pulumi.StringPtrInput
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications LakehouseMonitorNotificationsPtrInput
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringPtrInput
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName pulumi.StringPtrInput
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule LakehouseMonitorSchedulePtrInput
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrInput
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayInput
	// Configuration for monitoring snapshot tables.
	Snapshot LakehouseMonitorSnapshotPtrInput
	// Status of the Monitor
	Status pulumi.StringPtrInput
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringPtrInput
	// Configuration for monitoring timeseries tables.
	TimeSeries LakehouseMonitorTimeSeriesPtrInput
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrInput
}

func (LakehouseMonitorState) ElementType() reflect.Type {
	return reflect.TypeOf((*lakehouseMonitorState)(nil)).Elem()
}

type lakehouseMonitorArgs struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir string `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName *string `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics []LakehouseMonitorCustomMetric `pulumi:"customMetrics"`
	// The data classification config for the monitor
	DataClassificationConfig *LakehouseMonitorDataClassificationConfig `pulumi:"dataClassificationConfig"`
	// Configuration for the inference log monitor
	InferenceLog            *LakehouseMonitorInferenceLog `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg *string                       `pulumi:"latestMonitorFailureMsg"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications *LakehouseMonitorNotifications `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName string `pulumi:"outputSchemaName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule *LakehouseMonitorSchedule `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard *bool `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs []string `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot *LakehouseMonitorSnapshot `pulumi:"snapshot"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName string `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries *LakehouseMonitorTimeSeries `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId *string `pulumi:"warehouseId"`
}

// The set of arguments for constructing a LakehouseMonitor resource.
type LakehouseMonitorArgs struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringInput
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrInput
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics LakehouseMonitorCustomMetricArrayInput
	// The data classification config for the monitor
	DataClassificationConfig LakehouseMonitorDataClassificationConfigPtrInput
	// Configuration for the inference log monitor
	InferenceLog            LakehouseMonitorInferenceLogPtrInput
	LatestMonitorFailureMsg pulumi.StringPtrInput
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications LakehouseMonitorNotificationsPtrInput
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringInput
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule LakehouseMonitorSchedulePtrInput
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrInput
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayInput
	// Configuration for monitoring snapshot tables.
	Snapshot LakehouseMonitorSnapshotPtrInput
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringInput
	// Configuration for monitoring timeseries tables.
	TimeSeries LakehouseMonitorTimeSeriesPtrInput
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrInput
}

func (LakehouseMonitorArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*lakehouseMonitorArgs)(nil)).Elem()
}

type LakehouseMonitorInput interface {
	pulumi.Input

	ToLakehouseMonitorOutput() LakehouseMonitorOutput
	ToLakehouseMonitorOutputWithContext(ctx context.Context) LakehouseMonitorOutput
}

func (*LakehouseMonitor) ElementType() reflect.Type {
	return reflect.TypeOf((**LakehouseMonitor)(nil)).Elem()
}

func (i *LakehouseMonitor) ToLakehouseMonitorOutput() LakehouseMonitorOutput {
	return i.ToLakehouseMonitorOutputWithContext(context.Background())
}

func (i *LakehouseMonitor) ToLakehouseMonitorOutputWithContext(ctx context.Context) LakehouseMonitorOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LakehouseMonitorOutput)
}

// LakehouseMonitorArrayInput is an input type that accepts LakehouseMonitorArray and LakehouseMonitorArrayOutput values.
// You can construct a concrete instance of `LakehouseMonitorArrayInput` via:
//
//	LakehouseMonitorArray{ LakehouseMonitorArgs{...} }
type LakehouseMonitorArrayInput interface {
	pulumi.Input

	ToLakehouseMonitorArrayOutput() LakehouseMonitorArrayOutput
	ToLakehouseMonitorArrayOutputWithContext(context.Context) LakehouseMonitorArrayOutput
}

type LakehouseMonitorArray []LakehouseMonitorInput

func (LakehouseMonitorArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*LakehouseMonitor)(nil)).Elem()
}

func (i LakehouseMonitorArray) ToLakehouseMonitorArrayOutput() LakehouseMonitorArrayOutput {
	return i.ToLakehouseMonitorArrayOutputWithContext(context.Background())
}

func (i LakehouseMonitorArray) ToLakehouseMonitorArrayOutputWithContext(ctx context.Context) LakehouseMonitorArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LakehouseMonitorArrayOutput)
}

// LakehouseMonitorMapInput is an input type that accepts LakehouseMonitorMap and LakehouseMonitorMapOutput values.
// You can construct a concrete instance of `LakehouseMonitorMapInput` via:
//
//	LakehouseMonitorMap{ "key": LakehouseMonitorArgs{...} }
type LakehouseMonitorMapInput interface {
	pulumi.Input

	ToLakehouseMonitorMapOutput() LakehouseMonitorMapOutput
	ToLakehouseMonitorMapOutputWithContext(context.Context) LakehouseMonitorMapOutput
}

type LakehouseMonitorMap map[string]LakehouseMonitorInput

func (LakehouseMonitorMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*LakehouseMonitor)(nil)).Elem()
}

func (i LakehouseMonitorMap) ToLakehouseMonitorMapOutput() LakehouseMonitorMapOutput {
	return i.ToLakehouseMonitorMapOutputWithContext(context.Background())
}

func (i LakehouseMonitorMap) ToLakehouseMonitorMapOutputWithContext(ctx context.Context) LakehouseMonitorMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LakehouseMonitorMapOutput)
}

type LakehouseMonitorOutput struct{ *pulumi.OutputState }

func (LakehouseMonitorOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LakehouseMonitor)(nil)).Elem()
}

func (o LakehouseMonitorOutput) ToLakehouseMonitorOutput() LakehouseMonitorOutput {
	return o
}

func (o LakehouseMonitorOutput) ToLakehouseMonitorOutputWithContext(ctx context.Context) LakehouseMonitorOutput {
	return o
}

// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
func (o LakehouseMonitorOutput) AssetsDir() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.AssetsDir }).(pulumi.StringOutput)
}

// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
// table.
func (o LakehouseMonitorOutput) BaselineTableName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringPtrOutput { return v.BaselineTableName }).(pulumi.StringPtrOutput)
}

// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
func (o LakehouseMonitorOutput) CustomMetrics() LakehouseMonitorCustomMetricArrayOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorCustomMetricArrayOutput { return v.CustomMetrics }).(LakehouseMonitorCustomMetricArrayOutput)
}

// The ID of the generated dashboard.
func (o LakehouseMonitorOutput) DashboardId() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.DashboardId }).(pulumi.StringOutput)
}

// The data classification config for the monitor
func (o LakehouseMonitorOutput) DataClassificationConfig() LakehouseMonitorDataClassificationConfigPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorDataClassificationConfigPtrOutput {
		return v.DataClassificationConfig
	}).(LakehouseMonitorDataClassificationConfigPtrOutput)
}

// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
func (o LakehouseMonitorOutput) DriftMetricsTableName() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.DriftMetricsTableName }).(pulumi.StringOutput)
}

// Configuration for the inference log monitor
func (o LakehouseMonitorOutput) InferenceLog() LakehouseMonitorInferenceLogPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorInferenceLogPtrOutput { return v.InferenceLog }).(LakehouseMonitorInferenceLogPtrOutput)
}

func (o LakehouseMonitorOutput) LatestMonitorFailureMsg() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringPtrOutput { return v.LatestMonitorFailureMsg }).(pulumi.StringPtrOutput)
}

// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
func (o LakehouseMonitorOutput) MonitorVersion() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.MonitorVersion }).(pulumi.StringOutput)
}

// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
func (o LakehouseMonitorOutput) Notifications() LakehouseMonitorNotificationsPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorNotificationsPtrOutput { return v.Notifications }).(LakehouseMonitorNotificationsPtrOutput)
}

// Schema where output metric tables are created
func (o LakehouseMonitorOutput) OutputSchemaName() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.OutputSchemaName }).(pulumi.StringOutput)
}

// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
func (o LakehouseMonitorOutput) ProfileMetricsTableName() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.ProfileMetricsTableName }).(pulumi.StringOutput)
}

// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
func (o LakehouseMonitorOutput) Schedule() LakehouseMonitorSchedulePtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorSchedulePtrOutput { return v.Schedule }).(LakehouseMonitorSchedulePtrOutput)
}

// Whether to skip creating a default dashboard summarizing data quality metrics.
func (o LakehouseMonitorOutput) SkipBuiltinDashboard() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.BoolPtrOutput { return v.SkipBuiltinDashboard }).(pulumi.BoolPtrOutput)
}

// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
func (o LakehouseMonitorOutput) SlicingExprs() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringArrayOutput { return v.SlicingExprs }).(pulumi.StringArrayOutput)
}

// Configuration for monitoring snapshot tables.
func (o LakehouseMonitorOutput) Snapshot() LakehouseMonitorSnapshotPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorSnapshotPtrOutput { return v.Snapshot }).(LakehouseMonitorSnapshotPtrOutput)
}

// Status of the Monitor
func (o LakehouseMonitorOutput) Status() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.Status }).(pulumi.StringOutput)
}

// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
func (o LakehouseMonitorOutput) TableName() pulumi.StringOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringOutput { return v.TableName }).(pulumi.StringOutput)
}

// Configuration for monitoring timeseries tables.
func (o LakehouseMonitorOutput) TimeSeries() LakehouseMonitorTimeSeriesPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) LakehouseMonitorTimeSeriesPtrOutput { return v.TimeSeries }).(LakehouseMonitorTimeSeriesPtrOutput)
}

// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
func (o LakehouseMonitorOutput) WarehouseId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LakehouseMonitor) pulumi.StringPtrOutput { return v.WarehouseId }).(pulumi.StringPtrOutput)
}

type LakehouseMonitorArrayOutput struct{ *pulumi.OutputState }

func (LakehouseMonitorArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*LakehouseMonitor)(nil)).Elem()
}

func (o LakehouseMonitorArrayOutput) ToLakehouseMonitorArrayOutput() LakehouseMonitorArrayOutput {
	return o
}

func (o LakehouseMonitorArrayOutput) ToLakehouseMonitorArrayOutputWithContext(ctx context.Context) LakehouseMonitorArrayOutput {
	return o
}

func (o LakehouseMonitorArrayOutput) Index(i pulumi.IntInput) LakehouseMonitorOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *LakehouseMonitor {
		return vs[0].([]*LakehouseMonitor)[vs[1].(int)]
	}).(LakehouseMonitorOutput)
}

type LakehouseMonitorMapOutput struct{ *pulumi.OutputState }

func (LakehouseMonitorMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*LakehouseMonitor)(nil)).Elem()
}

func (o LakehouseMonitorMapOutput) ToLakehouseMonitorMapOutput() LakehouseMonitorMapOutput {
	return o
}

func (o LakehouseMonitorMapOutput) ToLakehouseMonitorMapOutputWithContext(ctx context.Context) LakehouseMonitorMapOutput {
	return o
}

func (o LakehouseMonitorMapOutput) MapIndex(k pulumi.StringInput) LakehouseMonitorOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *LakehouseMonitor {
		return vs[0].(map[string]*LakehouseMonitor)[vs[1].(string)]
	}).(LakehouseMonitorOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*LakehouseMonitorInput)(nil)).Elem(), &LakehouseMonitor{})
	pulumi.RegisterInputType(reflect.TypeOf((*LakehouseMonitorArrayInput)(nil)).Elem(), LakehouseMonitorArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*LakehouseMonitorMapInput)(nil)).Elem(), LakehouseMonitorMap{})
	pulumi.RegisterOutputType(LakehouseMonitorOutput{})
	pulumi.RegisterOutputType(LakehouseMonitorArrayOutput{})
	pulumi.RegisterOutputType(LakehouseMonitorMapOutput{})
}
