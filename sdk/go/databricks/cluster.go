// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package databricks

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-databricks/sdk/go/databricks/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// ## Import
//
// The resource cluster can be imported using cluster id.
//
// bash
//
// ```sh
// $ pulumi import databricks:index/cluster:Cluster this <cluster-id>
// ```
type Cluster struct {
	pulumi.CustomResourceState

	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues pulumi.BoolPtrOutput      `pulumi:"applyPolicyDefaultValues"`
	Autoscale                ClusterAutoscalePtrOutput `pulumi:"autoscale"`
	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
	AutoterminationMinutes pulumi.IntPtrOutput                `pulumi:"autoterminationMinutes"`
	AwsAttributes          ClusterAwsAttributesPtrOutput      `pulumi:"awsAttributes"`
	AzureAttributes        ClusterAzureAttributesPtrOutput    `pulumi:"azureAttributes"`
	ClusterId              pulumi.StringOutput                `pulumi:"clusterId"`
	ClusterLogConf         ClusterClusterLogConfPtrOutput     `pulumi:"clusterLogConf"`
	ClusterMountInfos      ClusterClusterMountInfoArrayOutput `pulumi:"clusterMountInfos"`
	// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName pulumi.StringPtrOutput `pulumi:"clusterName"`
	// should have tag `ResourceClass` set to value `Serverless`
	//
	// For example:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared High-Concurrency"),
	// 			SparkVersion:           pulumi.Any(latestLts.Id),
	// 			NodeTypeId:             pulumi.Any(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
	// 				"spark.databricks.cluster.profile":       pulumi.String("serverless"),
	// 			},
	// 			CustomTags: pulumi.StringMap{
	// 				"ResourceClass": pulumi.String("Serverless"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	CustomTags pulumi.StringMapOutput `pulumi:"customTags"`
	// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
	// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
	// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
	// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
	DataSecurityMode pulumi.StringPtrOutput `pulumi:"dataSecurityMode"`
	// (map) Tags that are added by Databricks by default, regardless of any `customTags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
	DefaultTags pulumi.StringMapOutput      `pulumi:"defaultTags"`
	DockerImage ClusterDockerImagePtrOutput `pulumi:"dockerImage"`
	// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
	DriverInstancePoolId pulumi.StringOutput `pulumi:"driverInstancePoolId"`
	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
	DriverNodeTypeId pulumi.StringOutput `pulumi:"driverNodeTypeId"`
	// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
	EnableElasticDisk pulumi.BoolOutput `pulumi:"enableElasticDisk"`
	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
	EnableLocalDiskEncryption pulumi.BoolOutput             `pulumi:"enableLocalDiskEncryption"`
	GcpAttributes             ClusterGcpAttributesPtrOutput `pulumi:"gcpAttributes"`
	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken pulumi.StringPtrOutput       `pulumi:"idempotencyToken"`
	InitScripts      ClusterInitScriptArrayOutput `pulumi:"initScripts"`
	// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolId pulumi.StringPtrOutput `pulumi:"instancePoolId"`
	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned pulumi.BoolPtrOutput `pulumi:"isPinned"`
	// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
	IsSingleNode pulumi.BoolPtrOutput `pulumi:"isSingleNode"`
	// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
	Kind      pulumi.StringPtrOutput    `pulumi:"kind"`
	Libraries ClusterLibraryArrayOutput `pulumi:"libraries"`
	// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
	//
	// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
	// 			LocalDisk: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
	// 			LongTermSupport: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared Autoscaling"),
	// 			SparkVersion:           pulumi.String(latestLts.Id),
	// 			NodeTypeId:             pulumi.String(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			Autoscale: &databricks.ClusterAutoscaleArgs{
	// 				MinWorkers: pulumi.Int(1),
	// 				MaxWorkers: pulumi.Int(50),
	// 			},
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.io.cache.enabled":          pulumi.String("true"),
	// 				"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
	// 				"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	NoWait pulumi.BoolPtrOutput `pulumi:"noWait"`
	// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
	NodeTypeId pulumi.StringOutput `pulumi:"nodeTypeId"`
	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
	NumWorkers pulumi.IntPtrOutput `pulumi:"numWorkers"`
	// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
	PolicyId pulumi.StringPtrOutput `pulumi:"policyId"`
	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
	RuntimeEngine pulumi.StringPtrOutput `pulumi:"runtimeEngine"`
	// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName pulumi.StringPtrOutput `pulumi:"singleUserName"`
	// should have following items:
	// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
	// * `spark.databricks.cluster.profile` set to `serverless`
	SparkConf pulumi.StringMapOutput `pulumi:"sparkConf"`
	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars pulumi.StringMapOutput `pulumi:"sparkEnvVars"`
	// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion pulumi.StringOutput `pulumi:"sparkVersion"`
	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SshPublicKeys pulumi.StringArrayOutput `pulumi:"sshPublicKeys"`
	// (string) State of the cluster.
	State pulumi.StringOutput `pulumi:"state"`
	Url   pulumi.StringOutput `pulumi:"url"`
	// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
	UseMlRuntime pulumi.BoolPtrOutput         `pulumi:"useMlRuntime"`
	WorkloadType ClusterWorkloadTypePtrOutput `pulumi:"workloadType"`
}

// NewCluster registers a new resource with the given unique name, arguments, and options.
func NewCluster(ctx *pulumi.Context,
	name string, args *ClusterArgs, opts ...pulumi.ResourceOption) (*Cluster, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.SparkVersion == nil {
		return nil, errors.New("invalid value for required argument 'SparkVersion'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource Cluster
	err := ctx.RegisterResource("databricks:index/cluster:Cluster", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetCluster gets an existing Cluster resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetCluster(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *ClusterState, opts ...pulumi.ResourceOption) (*Cluster, error) {
	var resource Cluster
	err := ctx.ReadResource("databricks:index/cluster:Cluster", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Cluster resources.
type clusterState struct {
	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues *bool             `pulumi:"applyPolicyDefaultValues"`
	Autoscale                *ClusterAutoscale `pulumi:"autoscale"`
	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
	AutoterminationMinutes *int                      `pulumi:"autoterminationMinutes"`
	AwsAttributes          *ClusterAwsAttributes     `pulumi:"awsAttributes"`
	AzureAttributes        *ClusterAzureAttributes   `pulumi:"azureAttributes"`
	ClusterId              *string                   `pulumi:"clusterId"`
	ClusterLogConf         *ClusterClusterLogConf    `pulumi:"clusterLogConf"`
	ClusterMountInfos      []ClusterClusterMountInfo `pulumi:"clusterMountInfos"`
	// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName *string `pulumi:"clusterName"`
	// should have tag `ResourceClass` set to value `Serverless`
	//
	// For example:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared High-Concurrency"),
	// 			SparkVersion:           pulumi.Any(latestLts.Id),
	// 			NodeTypeId:             pulumi.Any(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
	// 				"spark.databricks.cluster.profile":       pulumi.String("serverless"),
	// 			},
	// 			CustomTags: pulumi.StringMap{
	// 				"ResourceClass": pulumi.String("Serverless"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	CustomTags map[string]string `pulumi:"customTags"`
	// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
	// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
	// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
	// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
	DataSecurityMode *string `pulumi:"dataSecurityMode"`
	// (map) Tags that are added by Databricks by default, regardless of any `customTags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
	DefaultTags map[string]string   `pulumi:"defaultTags"`
	DockerImage *ClusterDockerImage `pulumi:"dockerImage"`
	// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
	DriverInstancePoolId *string `pulumi:"driverInstancePoolId"`
	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
	DriverNodeTypeId *string `pulumi:"driverNodeTypeId"`
	// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
	EnableElasticDisk *bool `pulumi:"enableElasticDisk"`
	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
	EnableLocalDiskEncryption *bool                 `pulumi:"enableLocalDiskEncryption"`
	GcpAttributes             *ClusterGcpAttributes `pulumi:"gcpAttributes"`
	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken *string             `pulumi:"idempotencyToken"`
	InitScripts      []ClusterInitScript `pulumi:"initScripts"`
	// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolId *string `pulumi:"instancePoolId"`
	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned *bool `pulumi:"isPinned"`
	// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
	IsSingleNode *bool `pulumi:"isSingleNode"`
	// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
	Kind      *string          `pulumi:"kind"`
	Libraries []ClusterLibrary `pulumi:"libraries"`
	// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
	//
	// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
	// 			LocalDisk: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
	// 			LongTermSupport: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared Autoscaling"),
	// 			SparkVersion:           pulumi.String(latestLts.Id),
	// 			NodeTypeId:             pulumi.String(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			Autoscale: &databricks.ClusterAutoscaleArgs{
	// 				MinWorkers: pulumi.Int(1),
	// 				MaxWorkers: pulumi.Int(50),
	// 			},
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.io.cache.enabled":          pulumi.String("true"),
	// 				"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
	// 				"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	NoWait *bool `pulumi:"noWait"`
	// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
	NodeTypeId *string `pulumi:"nodeTypeId"`
	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
	NumWorkers *int `pulumi:"numWorkers"`
	// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
	PolicyId *string `pulumi:"policyId"`
	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
	RuntimeEngine *string `pulumi:"runtimeEngine"`
	// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName *string `pulumi:"singleUserName"`
	// should have following items:
	// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
	// * `spark.databricks.cluster.profile` set to `serverless`
	SparkConf map[string]string `pulumi:"sparkConf"`
	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars map[string]string `pulumi:"sparkEnvVars"`
	// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion *string `pulumi:"sparkVersion"`
	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SshPublicKeys []string `pulumi:"sshPublicKeys"`
	// (string) State of the cluster.
	State *string `pulumi:"state"`
	Url   *string `pulumi:"url"`
	// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
	UseMlRuntime *bool                `pulumi:"useMlRuntime"`
	WorkloadType *ClusterWorkloadType `pulumi:"workloadType"`
}

type ClusterState struct {
	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues pulumi.BoolPtrInput
	Autoscale                ClusterAutoscalePtrInput
	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
	AutoterminationMinutes pulumi.IntPtrInput
	AwsAttributes          ClusterAwsAttributesPtrInput
	AzureAttributes        ClusterAzureAttributesPtrInput
	ClusterId              pulumi.StringPtrInput
	ClusterLogConf         ClusterClusterLogConfPtrInput
	ClusterMountInfos      ClusterClusterMountInfoArrayInput
	// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName pulumi.StringPtrInput
	// should have tag `ResourceClass` set to value `Serverless`
	//
	// For example:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared High-Concurrency"),
	// 			SparkVersion:           pulumi.Any(latestLts.Id),
	// 			NodeTypeId:             pulumi.Any(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
	// 				"spark.databricks.cluster.profile":       pulumi.String("serverless"),
	// 			},
	// 			CustomTags: pulumi.StringMap{
	// 				"ResourceClass": pulumi.String("Serverless"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	CustomTags pulumi.StringMapInput
	// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
	// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
	// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
	// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
	DataSecurityMode pulumi.StringPtrInput
	// (map) Tags that are added by Databricks by default, regardless of any `customTags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
	DefaultTags pulumi.StringMapInput
	DockerImage ClusterDockerImagePtrInput
	// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
	DriverInstancePoolId pulumi.StringPtrInput
	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
	DriverNodeTypeId pulumi.StringPtrInput
	// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
	EnableElasticDisk pulumi.BoolPtrInput
	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
	EnableLocalDiskEncryption pulumi.BoolPtrInput
	GcpAttributes             ClusterGcpAttributesPtrInput
	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken pulumi.StringPtrInput
	InitScripts      ClusterInitScriptArrayInput
	// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolId pulumi.StringPtrInput
	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned pulumi.BoolPtrInput
	// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
	IsSingleNode pulumi.BoolPtrInput
	// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
	Kind      pulumi.StringPtrInput
	Libraries ClusterLibraryArrayInput
	// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
	//
	// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
	// 			LocalDisk: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
	// 			LongTermSupport: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared Autoscaling"),
	// 			SparkVersion:           pulumi.String(latestLts.Id),
	// 			NodeTypeId:             pulumi.String(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			Autoscale: &databricks.ClusterAutoscaleArgs{
	// 				MinWorkers: pulumi.Int(1),
	// 				MaxWorkers: pulumi.Int(50),
	// 			},
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.io.cache.enabled":          pulumi.String("true"),
	// 				"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
	// 				"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	NoWait pulumi.BoolPtrInput
	// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
	NodeTypeId pulumi.StringPtrInput
	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
	NumWorkers pulumi.IntPtrInput
	// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
	PolicyId pulumi.StringPtrInput
	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
	RuntimeEngine pulumi.StringPtrInput
	// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName pulumi.StringPtrInput
	// should have following items:
	// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
	// * `spark.databricks.cluster.profile` set to `serverless`
	SparkConf pulumi.StringMapInput
	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars pulumi.StringMapInput
	// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion pulumi.StringPtrInput
	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SshPublicKeys pulumi.StringArrayInput
	// (string) State of the cluster.
	State pulumi.StringPtrInput
	Url   pulumi.StringPtrInput
	// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
	UseMlRuntime pulumi.BoolPtrInput
	WorkloadType ClusterWorkloadTypePtrInput
}

func (ClusterState) ElementType() reflect.Type {
	return reflect.TypeOf((*clusterState)(nil)).Elem()
}

type clusterArgs struct {
	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues *bool             `pulumi:"applyPolicyDefaultValues"`
	Autoscale                *ClusterAutoscale `pulumi:"autoscale"`
	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
	AutoterminationMinutes *int                      `pulumi:"autoterminationMinutes"`
	AwsAttributes          *ClusterAwsAttributes     `pulumi:"awsAttributes"`
	AzureAttributes        *ClusterAzureAttributes   `pulumi:"azureAttributes"`
	ClusterLogConf         *ClusterClusterLogConf    `pulumi:"clusterLogConf"`
	ClusterMountInfos      []ClusterClusterMountInfo `pulumi:"clusterMountInfos"`
	// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName *string `pulumi:"clusterName"`
	// should have tag `ResourceClass` set to value `Serverless`
	//
	// For example:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared High-Concurrency"),
	// 			SparkVersion:           pulumi.Any(latestLts.Id),
	// 			NodeTypeId:             pulumi.Any(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
	// 				"spark.databricks.cluster.profile":       pulumi.String("serverless"),
	// 			},
	// 			CustomTags: pulumi.StringMap{
	// 				"ResourceClass": pulumi.String("Serverless"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	CustomTags map[string]string `pulumi:"customTags"`
	// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
	// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
	// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
	// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
	DataSecurityMode *string             `pulumi:"dataSecurityMode"`
	DockerImage      *ClusterDockerImage `pulumi:"dockerImage"`
	// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
	DriverInstancePoolId *string `pulumi:"driverInstancePoolId"`
	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
	DriverNodeTypeId *string `pulumi:"driverNodeTypeId"`
	// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
	EnableElasticDisk *bool `pulumi:"enableElasticDisk"`
	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
	EnableLocalDiskEncryption *bool                 `pulumi:"enableLocalDiskEncryption"`
	GcpAttributes             *ClusterGcpAttributes `pulumi:"gcpAttributes"`
	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken *string             `pulumi:"idempotencyToken"`
	InitScripts      []ClusterInitScript `pulumi:"initScripts"`
	// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolId *string `pulumi:"instancePoolId"`
	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned *bool `pulumi:"isPinned"`
	// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
	IsSingleNode *bool `pulumi:"isSingleNode"`
	// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
	Kind      *string          `pulumi:"kind"`
	Libraries []ClusterLibrary `pulumi:"libraries"`
	// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
	//
	// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
	// 			LocalDisk: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
	// 			LongTermSupport: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared Autoscaling"),
	// 			SparkVersion:           pulumi.String(latestLts.Id),
	// 			NodeTypeId:             pulumi.String(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			Autoscale: &databricks.ClusterAutoscaleArgs{
	// 				MinWorkers: pulumi.Int(1),
	// 				MaxWorkers: pulumi.Int(50),
	// 			},
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.io.cache.enabled":          pulumi.String("true"),
	// 				"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
	// 				"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	NoWait *bool `pulumi:"noWait"`
	// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
	NodeTypeId *string `pulumi:"nodeTypeId"`
	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
	NumWorkers *int `pulumi:"numWorkers"`
	// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
	PolicyId *string `pulumi:"policyId"`
	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
	RuntimeEngine *string `pulumi:"runtimeEngine"`
	// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName *string `pulumi:"singleUserName"`
	// should have following items:
	// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
	// * `spark.databricks.cluster.profile` set to `serverless`
	SparkConf map[string]string `pulumi:"sparkConf"`
	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars map[string]string `pulumi:"sparkEnvVars"`
	// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion string `pulumi:"sparkVersion"`
	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SshPublicKeys []string `pulumi:"sshPublicKeys"`
	// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
	UseMlRuntime *bool                `pulumi:"useMlRuntime"`
	WorkloadType *ClusterWorkloadType `pulumi:"workloadType"`
}

// The set of arguments for constructing a Cluster resource.
type ClusterArgs struct {
	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues pulumi.BoolPtrInput
	Autoscale                ClusterAutoscalePtrInput
	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
	AutoterminationMinutes pulumi.IntPtrInput
	AwsAttributes          ClusterAwsAttributesPtrInput
	AzureAttributes        ClusterAzureAttributesPtrInput
	ClusterLogConf         ClusterClusterLogConfPtrInput
	ClusterMountInfos      ClusterClusterMountInfoArrayInput
	// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName pulumi.StringPtrInput
	// should have tag `ResourceClass` set to value `Serverless`
	//
	// For example:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared High-Concurrency"),
	// 			SparkVersion:           pulumi.Any(latestLts.Id),
	// 			NodeTypeId:             pulumi.Any(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
	// 				"spark.databricks.cluster.profile":       pulumi.String("serverless"),
	// 			},
	// 			CustomTags: pulumi.StringMap{
	// 				"ResourceClass": pulumi.String("Serverless"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	CustomTags pulumi.StringMapInput
	// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
	// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
	// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
	// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
	DataSecurityMode pulumi.StringPtrInput
	DockerImage      ClusterDockerImagePtrInput
	// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
	DriverInstancePoolId pulumi.StringPtrInput
	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
	DriverNodeTypeId pulumi.StringPtrInput
	// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
	EnableElasticDisk pulumi.BoolPtrInput
	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
	EnableLocalDiskEncryption pulumi.BoolPtrInput
	GcpAttributes             ClusterGcpAttributesPtrInput
	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken pulumi.StringPtrInput
	InitScripts      ClusterInitScriptArrayInput
	// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolId pulumi.StringPtrInput
	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned pulumi.BoolPtrInput
	// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
	IsSingleNode pulumi.BoolPtrInput
	// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
	Kind      pulumi.StringPtrInput
	Libraries ClusterLibraryArrayInput
	// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
	//
	// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
	//
	// ```go
	// package main
	//
	// import (
	// 	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
	// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	// )
	//
	// func main() {
	// 	pulumi.Run(func(ctx *pulumi.Context) error {
	// 		smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
	// 			LocalDisk: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
	// 			LongTermSupport: pulumi.BoolRef(true),
	// 		}, nil)
	// 		if err != nil {
	// 			return err
	// 		}
	// 		_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
	// 			ClusterName:            pulumi.String("Shared Autoscaling"),
	// 			SparkVersion:           pulumi.String(latestLts.Id),
	// 			NodeTypeId:             pulumi.String(smallest.Id),
	// 			AutoterminationMinutes: pulumi.Int(20),
	// 			Autoscale: &databricks.ClusterAutoscaleArgs{
	// 				MinWorkers: pulumi.Int(1),
	// 				MaxWorkers: pulumi.Int(50),
	// 			},
	// 			SparkConf: pulumi.StringMap{
	// 				"spark.databricks.io.cache.enabled":          pulumi.String("true"),
	// 				"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
	// 				"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
	// 			},
	// 		})
	// 		if err != nil {
	// 			return err
	// 		}
	// 		return nil
	// 	})
	// }
	// ```
	NoWait pulumi.BoolPtrInput
	// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
	NodeTypeId pulumi.StringPtrInput
	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
	NumWorkers pulumi.IntPtrInput
	// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
	PolicyId pulumi.StringPtrInput
	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
	RuntimeEngine pulumi.StringPtrInput
	// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName pulumi.StringPtrInput
	// should have following items:
	// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
	// * `spark.databricks.cluster.profile` set to `serverless`
	SparkConf pulumi.StringMapInput
	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars pulumi.StringMapInput
	// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion pulumi.StringInput
	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SshPublicKeys pulumi.StringArrayInput
	// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
	UseMlRuntime pulumi.BoolPtrInput
	WorkloadType ClusterWorkloadTypePtrInput
}

func (ClusterArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*clusterArgs)(nil)).Elem()
}

type ClusterInput interface {
	pulumi.Input

	ToClusterOutput() ClusterOutput
	ToClusterOutputWithContext(ctx context.Context) ClusterOutput
}

func (*Cluster) ElementType() reflect.Type {
	return reflect.TypeOf((**Cluster)(nil)).Elem()
}

func (i *Cluster) ToClusterOutput() ClusterOutput {
	return i.ToClusterOutputWithContext(context.Background())
}

func (i *Cluster) ToClusterOutputWithContext(ctx context.Context) ClusterOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterOutput)
}

// ClusterArrayInput is an input type that accepts ClusterArray and ClusterArrayOutput values.
// You can construct a concrete instance of `ClusterArrayInput` via:
//
//	ClusterArray{ ClusterArgs{...} }
type ClusterArrayInput interface {
	pulumi.Input

	ToClusterArrayOutput() ClusterArrayOutput
	ToClusterArrayOutputWithContext(context.Context) ClusterArrayOutput
}

type ClusterArray []ClusterInput

func (ClusterArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Cluster)(nil)).Elem()
}

func (i ClusterArray) ToClusterArrayOutput() ClusterArrayOutput {
	return i.ToClusterArrayOutputWithContext(context.Background())
}

func (i ClusterArray) ToClusterArrayOutputWithContext(ctx context.Context) ClusterArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterArrayOutput)
}

// ClusterMapInput is an input type that accepts ClusterMap and ClusterMapOutput values.
// You can construct a concrete instance of `ClusterMapInput` via:
//
//	ClusterMap{ "key": ClusterArgs{...} }
type ClusterMapInput interface {
	pulumi.Input

	ToClusterMapOutput() ClusterMapOutput
	ToClusterMapOutputWithContext(context.Context) ClusterMapOutput
}

type ClusterMap map[string]ClusterInput

func (ClusterMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Cluster)(nil)).Elem()
}

func (i ClusterMap) ToClusterMapOutput() ClusterMapOutput {
	return i.ToClusterMapOutputWithContext(context.Background())
}

func (i ClusterMap) ToClusterMapOutputWithContext(ctx context.Context) ClusterMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterMapOutput)
}

type ClusterOutput struct{ *pulumi.OutputState }

func (ClusterOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Cluster)(nil)).Elem()
}

func (o ClusterOutput) ToClusterOutput() ClusterOutput {
	return o
}

func (o ClusterOutput) ToClusterOutputWithContext(ctx context.Context) ClusterOutput {
	return o
}

// Whether to use policy default values for missing cluster attributes.
func (o ClusterOutput) ApplyPolicyDefaultValues() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolPtrOutput { return v.ApplyPolicyDefaultValues }).(pulumi.BoolPtrOutput)
}

func (o ClusterOutput) Autoscale() ClusterAutoscalePtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterAutoscalePtrOutput { return v.Autoscale }).(ClusterAutoscalePtrOutput)
}

// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
func (o ClusterOutput) AutoterminationMinutes() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.IntPtrOutput { return v.AutoterminationMinutes }).(pulumi.IntPtrOutput)
}

func (o ClusterOutput) AwsAttributes() ClusterAwsAttributesPtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterAwsAttributesPtrOutput { return v.AwsAttributes }).(ClusterAwsAttributesPtrOutput)
}

func (o ClusterOutput) AzureAttributes() ClusterAzureAttributesPtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterAzureAttributesPtrOutput { return v.AzureAttributes }).(ClusterAzureAttributesPtrOutput)
}

func (o ClusterOutput) ClusterId() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.ClusterId }).(pulumi.StringOutput)
}

func (o ClusterOutput) ClusterLogConf() ClusterClusterLogConfPtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterClusterLogConfPtrOutput { return v.ClusterLogConf }).(ClusterClusterLogConfPtrOutput)
}

func (o ClusterOutput) ClusterMountInfos() ClusterClusterMountInfoArrayOutput {
	return o.ApplyT(func(v *Cluster) ClusterClusterMountInfoArrayOutput { return v.ClusterMountInfos }).(ClusterClusterMountInfoArrayOutput)
}

// Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
func (o ClusterOutput) ClusterName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.ClusterName }).(pulumi.StringPtrOutput)
}

// should have tag `ResourceClass` set to value `Serverless`
//
// For example:
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := databricks.NewCluster(ctx, "cluster_with_table_access_control", &databricks.ClusterArgs{
//				ClusterName:            pulumi.String("Shared High-Concurrency"),
//				SparkVersion:           pulumi.Any(latestLts.Id),
//				NodeTypeId:             pulumi.Any(smallest.Id),
//				AutoterminationMinutes: pulumi.Int(20),
//				SparkConf: pulumi.StringMap{
//					"spark.databricks.repl.allowedLanguages": pulumi.String("python,sql"),
//					"spark.databricks.cluster.profile":       pulumi.String("serverless"),
//				},
//				CustomTags: pulumi.StringMap{
//					"ResourceClass": pulumi.String("Serverless"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
func (o ClusterOutput) CustomTags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringMapOutput { return v.CustomTags }).(pulumi.StringMapOutput)
}

// Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
// * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
// * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
// * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
func (o ClusterOutput) DataSecurityMode() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.DataSecurityMode }).(pulumi.StringPtrOutput)
}

// (map) Tags that are added by Databricks by default, regardless of any `customTags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
func (o ClusterOutput) DefaultTags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringMapOutput { return v.DefaultTags }).(pulumi.StringMapOutput)
}

func (o ClusterOutput) DockerImage() ClusterDockerImagePtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterDockerImagePtrOutput { return v.DockerImage }).(ClusterDockerImagePtrOutput)
}

// similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
func (o ClusterOutput) DriverInstancePoolId() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.DriverInstancePoolId }).(pulumi.StringOutput)
}

// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
func (o ClusterOutput) DriverNodeTypeId() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.DriverNodeTypeId }).(pulumi.StringOutput)
}

// If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
func (o ClusterOutput) EnableElasticDisk() pulumi.BoolOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolOutput { return v.EnableElasticDisk }).(pulumi.BoolOutput)
}

// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
func (o ClusterOutput) EnableLocalDiskEncryption() pulumi.BoolOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolOutput { return v.EnableLocalDiskEncryption }).(pulumi.BoolOutput)
}

func (o ClusterOutput) GcpAttributes() ClusterGcpAttributesPtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterGcpAttributesPtrOutput { return v.GcpAttributes }).(ClusterGcpAttributesPtrOutput)
}

// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
func (o ClusterOutput) IdempotencyToken() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.IdempotencyToken }).(pulumi.StringPtrOutput)
}

func (o ClusterOutput) InitScripts() ClusterInitScriptArrayOutput {
	return o.ApplyT(func(v *Cluster) ClusterInitScriptArrayOutput { return v.InitScripts }).(ClusterInitScriptArrayOutput)
}

// To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
func (o ClusterOutput) InstancePoolId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.InstancePoolId }).(pulumi.StringPtrOutput)
}

// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
func (o ClusterOutput) IsPinned() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolPtrOutput { return v.IsPinned }).(pulumi.BoolPtrOutput)
}

// When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
func (o ClusterOutput) IsSingleNode() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolPtrOutput { return v.IsSingleNode }).(pulumi.BoolPtrOutput)
}

// The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
func (o ClusterOutput) Kind() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.Kind }).(pulumi.StringPtrOutput)
}

func (o ClusterOutput) Libraries() ClusterLibraryArrayOutput {
	return o.ApplyT(func(v *Cluster) ClusterLibraryArrayOutput { return v.Libraries }).(ClusterLibraryArrayOutput)
}

// If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
//
// The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			smallest, err := databricks.GetNodeType(ctx, &databricks.GetNodeTypeArgs{
//				LocalDisk: pulumi.BoolRef(true),
//			}, nil)
//			if err != nil {
//				return err
//			}
//			latestLts, err := databricks.GetSparkVersion(ctx, &databricks.GetSparkVersionArgs{
//				LongTermSupport: pulumi.BoolRef(true),
//			}, nil)
//			if err != nil {
//				return err
//			}
//			_, err = databricks.NewCluster(ctx, "shared_autoscaling", &databricks.ClusterArgs{
//				ClusterName:            pulumi.String("Shared Autoscaling"),
//				SparkVersion:           pulumi.String(latestLts.Id),
//				NodeTypeId:             pulumi.String(smallest.Id),
//				AutoterminationMinutes: pulumi.Int(20),
//				Autoscale: &databricks.ClusterAutoscaleArgs{
//					MinWorkers: pulumi.Int(1),
//					MaxWorkers: pulumi.Int(50),
//				},
//				SparkConf: pulumi.StringMap{
//					"spark.databricks.io.cache.enabled":          pulumi.String("true"),
//					"spark.databricks.io.cache.maxDiskUsage":     pulumi.String("50g"),
//					"spark.databricks.io.cache.maxMetaDataCache": pulumi.String("1g"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
func (o ClusterOutput) NoWait() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolPtrOutput { return v.NoWait }).(pulumi.BoolPtrOutput)
}

// Any supported getNodeType id. If `instancePoolId` is specified, this field is not needed.
func (o ClusterOutput) NodeTypeId() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.NodeTypeId }).(pulumi.StringOutput)
}

// Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
func (o ClusterOutput) NumWorkers() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.IntPtrOutput { return v.NumWorkers }).(pulumi.IntPtrOutput)
}

// Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
func (o ClusterOutput) PolicyId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.PolicyId }).(pulumi.StringPtrOutput)
}

// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
func (o ClusterOutput) RuntimeEngine() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.RuntimeEngine }).(pulumi.StringPtrOutput)
}

// The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
func (o ClusterOutput) SingleUserName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringPtrOutput { return v.SingleUserName }).(pulumi.StringPtrOutput)
}

// should have following items:
// * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
// * `spark.databricks.cluster.profile` set to `serverless`
func (o ClusterOutput) SparkConf() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringMapOutput { return v.SparkConf }).(pulumi.StringMapOutput)
}

// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
func (o ClusterOutput) SparkEnvVars() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringMapOutput { return v.SparkEnvVars }).(pulumi.StringMapOutput)
}

// [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
func (o ClusterOutput) SparkVersion() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.SparkVersion }).(pulumi.StringOutput)
}

// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
func (o ClusterOutput) SshPublicKeys() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringArrayOutput { return v.SshPublicKeys }).(pulumi.StringArrayOutput)
}

// (string) State of the cluster.
func (o ClusterOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.State }).(pulumi.StringOutput)
}

func (o ClusterOutput) Url() pulumi.StringOutput {
	return o.ApplyT(func(v *Cluster) pulumi.StringOutput { return v.Url }).(pulumi.StringOutput)
}

// Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
func (o ClusterOutput) UseMlRuntime() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *Cluster) pulumi.BoolPtrOutput { return v.UseMlRuntime }).(pulumi.BoolPtrOutput)
}

func (o ClusterOutput) WorkloadType() ClusterWorkloadTypePtrOutput {
	return o.ApplyT(func(v *Cluster) ClusterWorkloadTypePtrOutput { return v.WorkloadType }).(ClusterWorkloadTypePtrOutput)
}

type ClusterArrayOutput struct{ *pulumi.OutputState }

func (ClusterArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Cluster)(nil)).Elem()
}

func (o ClusterArrayOutput) ToClusterArrayOutput() ClusterArrayOutput {
	return o
}

func (o ClusterArrayOutput) ToClusterArrayOutputWithContext(ctx context.Context) ClusterArrayOutput {
	return o
}

func (o ClusterArrayOutput) Index(i pulumi.IntInput) ClusterOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *Cluster {
		return vs[0].([]*Cluster)[vs[1].(int)]
	}).(ClusterOutput)
}

type ClusterMapOutput struct{ *pulumi.OutputState }

func (ClusterMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Cluster)(nil)).Elem()
}

func (o ClusterMapOutput) ToClusterMapOutput() ClusterMapOutput {
	return o
}

func (o ClusterMapOutput) ToClusterMapOutputWithContext(ctx context.Context) ClusterMapOutput {
	return o
}

func (o ClusterMapOutput) MapIndex(k pulumi.StringInput) ClusterOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *Cluster {
		return vs[0].(map[string]*Cluster)[vs[1].(string)]
	}).(ClusterOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterInput)(nil)).Elem(), &Cluster{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterArrayInput)(nil)).Elem(), ClusterArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterMapInput)(nil)).Elem(), ClusterMap{})
	pulumi.RegisterOutputType(ClusterOutput{})
	pulumi.RegisterOutputType(ClusterArrayOutput{})
	pulumi.RegisterOutputType(ClusterMapOutput{})
}
