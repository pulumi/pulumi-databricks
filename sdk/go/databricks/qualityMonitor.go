// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package databricks

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-databricks/sdk/go/databricks/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// This resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks.
//
// A `QualityMonitor` is attached to a SqlTable and can be of type timeseries, snapshot or inference.
//
// ### Inference Monitor
//
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := databricks.NewQualityMonitor(ctx, "testMonitorInference", &databricks.QualityMonitorArgs{
//				TableName:        pulumi.String(fmt.Sprintf("%v.%v.%v", sandbox.Name, things.Name, myTestTable.Name)),
//				AssetsDir:        pulumi.String(fmt.Sprintf("/Shared/provider-test/databricks_quality_monitoring/%v", myTestTable.Name)),
//				OutputSchemaName: pulumi.String(fmt.Sprintf("%v.%v", sandbox.Name, things.Name)),
//				InferenceLog: &databricks.QualityMonitorInferenceLogArgs{
//					Granularities: pulumi.StringArray{
//						pulumi.String("1 hour"),
//					},
//					TimestampCol:  pulumi.String("timestamp"),
//					PredictionCol: pulumi.String("prediction"),
//					ModelIdCol:    pulumi.String("model_id"),
//					ProblemType:   pulumi.String("PROBLEM_TYPE_REGRESSION"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Snapshot Monitor
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-databricks/sdk/go/databricks"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := databricks.NewQualityMonitor(ctx, "testMonitorInference", &databricks.QualityMonitorArgs{
//				TableName:        pulumi.String(fmt.Sprintf("%v.%v.%v", sandbox.Name, things.Name, myTestTable.Name)),
//				AssetsDir:        pulumi.String(fmt.Sprintf("/Shared/provider-test/databricks_quality_monitoring/%v", myTestTable.Name)),
//				OutputSchemaName: pulumi.String(fmt.Sprintf("%v.%v", sandbox.Name, things.Name)),
//				Snapshot:         nil,
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Related Resources
//
// The following resources are often used in the same context:
//
// * Catalog
// * Schema
// * SqlTable
type QualityMonitor struct {
	pulumi.CustomResourceState

	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringOutput `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrOutput `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics QualityMonitorCustomMetricArrayOutput `pulumi:"customMetrics"`
	// The ID of the generated dashboard.
	DashboardId pulumi.StringOutput `pulumi:"dashboardId"`
	// The data classification config for the monitor
	DataClassificationConfig QualityMonitorDataClassificationConfigPtrOutput `pulumi:"dataClassificationConfig"`
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName pulumi.StringOutput `pulumi:"driftMetricsTableName"`
	// Configuration for the inference log monitor
	InferenceLog            QualityMonitorInferenceLogPtrOutput `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg pulumi.StringPtrOutput              `pulumi:"latestMonitorFailureMsg"`
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion pulumi.StringOutput `pulumi:"monitorVersion"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications QualityMonitorNotificationsPtrOutput `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringOutput `pulumi:"outputSchemaName"`
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName pulumi.StringOutput `pulumi:"profileMetricsTableName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule QualityMonitorSchedulePtrOutput `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrOutput `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayOutput `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot QualityMonitorSnapshotPtrOutput `pulumi:"snapshot"`
	// Status of the Monitor
	Status pulumi.StringOutput `pulumi:"status"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringOutput `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries QualityMonitorTimeSeriesPtrOutput `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrOutput `pulumi:"warehouseId"`
}

// NewQualityMonitor registers a new resource with the given unique name, arguments, and options.
func NewQualityMonitor(ctx *pulumi.Context,
	name string, args *QualityMonitorArgs, opts ...pulumi.ResourceOption) (*QualityMonitor, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.AssetsDir == nil {
		return nil, errors.New("invalid value for required argument 'AssetsDir'")
	}
	if args.OutputSchemaName == nil {
		return nil, errors.New("invalid value for required argument 'OutputSchemaName'")
	}
	if args.TableName == nil {
		return nil, errors.New("invalid value for required argument 'TableName'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource QualityMonitor
	err := ctx.RegisterResource("databricks:index/qualityMonitor:QualityMonitor", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetQualityMonitor gets an existing QualityMonitor resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetQualityMonitor(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *QualityMonitorState, opts ...pulumi.ResourceOption) (*QualityMonitor, error) {
	var resource QualityMonitor
	err := ctx.ReadResource("databricks:index/qualityMonitor:QualityMonitor", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering QualityMonitor resources.
type qualityMonitorState struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir *string `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName *string `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics []QualityMonitorCustomMetric `pulumi:"customMetrics"`
	// The ID of the generated dashboard.
	DashboardId *string `pulumi:"dashboardId"`
	// The data classification config for the monitor
	DataClassificationConfig *QualityMonitorDataClassificationConfig `pulumi:"dataClassificationConfig"`
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName *string `pulumi:"driftMetricsTableName"`
	// Configuration for the inference log monitor
	InferenceLog            *QualityMonitorInferenceLog `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg *string                     `pulumi:"latestMonitorFailureMsg"`
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion *string `pulumi:"monitorVersion"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications *QualityMonitorNotifications `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName *string `pulumi:"outputSchemaName"`
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName *string `pulumi:"profileMetricsTableName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule *QualityMonitorSchedule `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard *bool `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs []string `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot *QualityMonitorSnapshot `pulumi:"snapshot"`
	// Status of the Monitor
	Status *string `pulumi:"status"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName *string `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries *QualityMonitorTimeSeries `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId *string `pulumi:"warehouseId"`
}

type QualityMonitorState struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringPtrInput
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrInput
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics QualityMonitorCustomMetricArrayInput
	// The ID of the generated dashboard.
	DashboardId pulumi.StringPtrInput
	// The data classification config for the monitor
	DataClassificationConfig QualityMonitorDataClassificationConfigPtrInput
	// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	DriftMetricsTableName pulumi.StringPtrInput
	// Configuration for the inference log monitor
	InferenceLog            QualityMonitorInferenceLogPtrInput
	LatestMonitorFailureMsg pulumi.StringPtrInput
	// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
	MonitorVersion pulumi.StringPtrInput
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications QualityMonitorNotificationsPtrInput
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringPtrInput
	// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
	ProfileMetricsTableName pulumi.StringPtrInput
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule QualityMonitorSchedulePtrInput
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrInput
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayInput
	// Configuration for monitoring snapshot tables.
	Snapshot QualityMonitorSnapshotPtrInput
	// Status of the Monitor
	Status pulumi.StringPtrInput
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringPtrInput
	// Configuration for monitoring timeseries tables.
	TimeSeries QualityMonitorTimeSeriesPtrInput
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrInput
}

func (QualityMonitorState) ElementType() reflect.Type {
	return reflect.TypeOf((*qualityMonitorState)(nil)).Elem()
}

type qualityMonitorArgs struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir string `pulumi:"assetsDir"`
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName *string `pulumi:"baselineTableName"`
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics []QualityMonitorCustomMetric `pulumi:"customMetrics"`
	// The data classification config for the monitor
	DataClassificationConfig *QualityMonitorDataClassificationConfig `pulumi:"dataClassificationConfig"`
	// Configuration for the inference log monitor
	InferenceLog            *QualityMonitorInferenceLog `pulumi:"inferenceLog"`
	LatestMonitorFailureMsg *string                     `pulumi:"latestMonitorFailureMsg"`
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications *QualityMonitorNotifications `pulumi:"notifications"`
	// Schema where output metric tables are created
	OutputSchemaName string `pulumi:"outputSchemaName"`
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule *QualityMonitorSchedule `pulumi:"schedule"`
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard *bool `pulumi:"skipBuiltinDashboard"`
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs []string `pulumi:"slicingExprs"`
	// Configuration for monitoring snapshot tables.
	Snapshot *QualityMonitorSnapshot `pulumi:"snapshot"`
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName string `pulumi:"tableName"`
	// Configuration for monitoring timeseries tables.
	TimeSeries *QualityMonitorTimeSeries `pulumi:"timeSeries"`
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId *string `pulumi:"warehouseId"`
}

// The set of arguments for constructing a QualityMonitor resource.
type QualityMonitorArgs struct {
	// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
	AssetsDir pulumi.StringInput
	// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
	// table.
	BaselineTableName pulumi.StringPtrInput
	// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
	CustomMetrics QualityMonitorCustomMetricArrayInput
	// The data classification config for the monitor
	DataClassificationConfig QualityMonitorDataClassificationConfigPtrInput
	// Configuration for the inference log monitor
	InferenceLog            QualityMonitorInferenceLogPtrInput
	LatestMonitorFailureMsg pulumi.StringPtrInput
	// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
	Notifications QualityMonitorNotificationsPtrInput
	// Schema where output metric tables are created
	OutputSchemaName pulumi.StringInput
	// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
	Schedule QualityMonitorSchedulePtrInput
	// Whether to skip creating a default dashboard summarizing data quality metrics.
	SkipBuiltinDashboard pulumi.BoolPtrInput
	// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
	SlicingExprs pulumi.StringArrayInput
	// Configuration for monitoring snapshot tables.
	Snapshot QualityMonitorSnapshotPtrInput
	// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
	TableName pulumi.StringInput
	// Configuration for monitoring timeseries tables.
	TimeSeries QualityMonitorTimeSeriesPtrInput
	// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
	WarehouseId pulumi.StringPtrInput
}

func (QualityMonitorArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*qualityMonitorArgs)(nil)).Elem()
}

type QualityMonitorInput interface {
	pulumi.Input

	ToQualityMonitorOutput() QualityMonitorOutput
	ToQualityMonitorOutputWithContext(ctx context.Context) QualityMonitorOutput
}

func (*QualityMonitor) ElementType() reflect.Type {
	return reflect.TypeOf((**QualityMonitor)(nil)).Elem()
}

func (i *QualityMonitor) ToQualityMonitorOutput() QualityMonitorOutput {
	return i.ToQualityMonitorOutputWithContext(context.Background())
}

func (i *QualityMonitor) ToQualityMonitorOutputWithContext(ctx context.Context) QualityMonitorOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QualityMonitorOutput)
}

// QualityMonitorArrayInput is an input type that accepts QualityMonitorArray and QualityMonitorArrayOutput values.
// You can construct a concrete instance of `QualityMonitorArrayInput` via:
//
//	QualityMonitorArray{ QualityMonitorArgs{...} }
type QualityMonitorArrayInput interface {
	pulumi.Input

	ToQualityMonitorArrayOutput() QualityMonitorArrayOutput
	ToQualityMonitorArrayOutputWithContext(context.Context) QualityMonitorArrayOutput
}

type QualityMonitorArray []QualityMonitorInput

func (QualityMonitorArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*QualityMonitor)(nil)).Elem()
}

func (i QualityMonitorArray) ToQualityMonitorArrayOutput() QualityMonitorArrayOutput {
	return i.ToQualityMonitorArrayOutputWithContext(context.Background())
}

func (i QualityMonitorArray) ToQualityMonitorArrayOutputWithContext(ctx context.Context) QualityMonitorArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QualityMonitorArrayOutput)
}

// QualityMonitorMapInput is an input type that accepts QualityMonitorMap and QualityMonitorMapOutput values.
// You can construct a concrete instance of `QualityMonitorMapInput` via:
//
//	QualityMonitorMap{ "key": QualityMonitorArgs{...} }
type QualityMonitorMapInput interface {
	pulumi.Input

	ToQualityMonitorMapOutput() QualityMonitorMapOutput
	ToQualityMonitorMapOutputWithContext(context.Context) QualityMonitorMapOutput
}

type QualityMonitorMap map[string]QualityMonitorInput

func (QualityMonitorMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*QualityMonitor)(nil)).Elem()
}

func (i QualityMonitorMap) ToQualityMonitorMapOutput() QualityMonitorMapOutput {
	return i.ToQualityMonitorMapOutputWithContext(context.Background())
}

func (i QualityMonitorMap) ToQualityMonitorMapOutputWithContext(ctx context.Context) QualityMonitorMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QualityMonitorMapOutput)
}

type QualityMonitorOutput struct{ *pulumi.OutputState }

func (QualityMonitorOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**QualityMonitor)(nil)).Elem()
}

func (o QualityMonitorOutput) ToQualityMonitorOutput() QualityMonitorOutput {
	return o
}

func (o QualityMonitorOutput) ToQualityMonitorOutputWithContext(ctx context.Context) QualityMonitorOutput {
	return o
}

// The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)
func (o QualityMonitorOutput) AssetsDir() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.AssetsDir }).(pulumi.StringOutput)
}

// Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
// table.
func (o QualityMonitorOutput) BaselineTableName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringPtrOutput { return v.BaselineTableName }).(pulumi.StringPtrOutput)
}

// Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).
func (o QualityMonitorOutput) CustomMetrics() QualityMonitorCustomMetricArrayOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorCustomMetricArrayOutput { return v.CustomMetrics }).(QualityMonitorCustomMetricArrayOutput)
}

// The ID of the generated dashboard.
func (o QualityMonitorOutput) DashboardId() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.DashboardId }).(pulumi.StringOutput)
}

// The data classification config for the monitor
func (o QualityMonitorOutput) DataClassificationConfig() QualityMonitorDataClassificationConfigPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorDataClassificationConfigPtrOutput {
		return v.DataClassificationConfig
	}).(QualityMonitorDataClassificationConfigPtrOutput)
}

// The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
func (o QualityMonitorOutput) DriftMetricsTableName() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.DriftMetricsTableName }).(pulumi.StringOutput)
}

// Configuration for the inference log monitor
func (o QualityMonitorOutput) InferenceLog() QualityMonitorInferenceLogPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorInferenceLogPtrOutput { return v.InferenceLog }).(QualityMonitorInferenceLogPtrOutput)
}

func (o QualityMonitorOutput) LatestMonitorFailureMsg() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringPtrOutput { return v.LatestMonitorFailureMsg }).(pulumi.StringPtrOutput)
}

// The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted
func (o QualityMonitorOutput) MonitorVersion() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.MonitorVersion }).(pulumi.StringOutput)
}

// The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `emailAddresses` containing a list of emails to notify:
func (o QualityMonitorOutput) Notifications() QualityMonitorNotificationsPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorNotificationsPtrOutput { return v.Notifications }).(QualityMonitorNotificationsPtrOutput)
}

// Schema where output metric tables are created
func (o QualityMonitorOutput) OutputSchemaName() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.OutputSchemaName }).(pulumi.StringOutput)
}

// The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.
func (o QualityMonitorOutput) ProfileMetricsTableName() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.ProfileMetricsTableName }).(pulumi.StringOutput)
}

// The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:
func (o QualityMonitorOutput) Schedule() QualityMonitorSchedulePtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorSchedulePtrOutput { return v.Schedule }).(QualityMonitorSchedulePtrOutput)
}

// Whether to skip creating a default dashboard summarizing data quality metrics.
func (o QualityMonitorOutput) SkipBuiltinDashboard() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.BoolPtrOutput { return v.SkipBuiltinDashboard }).(pulumi.BoolPtrOutput)
}

// List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.
func (o QualityMonitorOutput) SlicingExprs() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringArrayOutput { return v.SlicingExprs }).(pulumi.StringArrayOutput)
}

// Configuration for monitoring snapshot tables.
func (o QualityMonitorOutput) Snapshot() QualityMonitorSnapshotPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorSnapshotPtrOutput { return v.Snapshot }).(QualityMonitorSnapshotPtrOutput)
}

// Status of the Monitor
func (o QualityMonitorOutput) Status() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.Status }).(pulumi.StringOutput)
}

// The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}
func (o QualityMonitorOutput) TableName() pulumi.StringOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringOutput { return v.TableName }).(pulumi.StringOutput)
}

// Configuration for monitoring timeseries tables.
func (o QualityMonitorOutput) TimeSeries() QualityMonitorTimeSeriesPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) QualityMonitorTimeSeriesPtrOutput { return v.TimeSeries }).(QualityMonitorTimeSeriesPtrOutput)
}

// Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.
func (o QualityMonitorOutput) WarehouseId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *QualityMonitor) pulumi.StringPtrOutput { return v.WarehouseId }).(pulumi.StringPtrOutput)
}

type QualityMonitorArrayOutput struct{ *pulumi.OutputState }

func (QualityMonitorArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*QualityMonitor)(nil)).Elem()
}

func (o QualityMonitorArrayOutput) ToQualityMonitorArrayOutput() QualityMonitorArrayOutput {
	return o
}

func (o QualityMonitorArrayOutput) ToQualityMonitorArrayOutputWithContext(ctx context.Context) QualityMonitorArrayOutput {
	return o
}

func (o QualityMonitorArrayOutput) Index(i pulumi.IntInput) QualityMonitorOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *QualityMonitor {
		return vs[0].([]*QualityMonitor)[vs[1].(int)]
	}).(QualityMonitorOutput)
}

type QualityMonitorMapOutput struct{ *pulumi.OutputState }

func (QualityMonitorMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*QualityMonitor)(nil)).Elem()
}

func (o QualityMonitorMapOutput) ToQualityMonitorMapOutput() QualityMonitorMapOutput {
	return o
}

func (o QualityMonitorMapOutput) ToQualityMonitorMapOutputWithContext(ctx context.Context) QualityMonitorMapOutput {
	return o
}

func (o QualityMonitorMapOutput) MapIndex(k pulumi.StringInput) QualityMonitorOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *QualityMonitor {
		return vs[0].(map[string]*QualityMonitor)[vs[1].(string)]
	}).(QualityMonitorOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*QualityMonitorInput)(nil)).Elem(), &QualityMonitor{})
	pulumi.RegisterInputType(reflect.TypeOf((*QualityMonitorArrayInput)(nil)).Elem(), QualityMonitorArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*QualityMonitorMapInput)(nil)).Elem(), QualityMonitorMap{})
	pulumi.RegisterOutputType(QualityMonitorOutput{})
	pulumi.RegisterOutputType(QualityMonitorArrayOutput{})
	pulumi.RegisterOutputType(QualityMonitorMapOutput{})
}
